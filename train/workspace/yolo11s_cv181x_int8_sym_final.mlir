#loc = loc(unknown)
#loc1 = loc("images")
module @yolo11s attributes {module.FLOPs = 3787969080 : i64, module.addr_mode = "basic", module.asymmetric = false, module.chip = "cv181x", module.cores = 1 : i64, module.devices = 1 : i64, module.high_precision = false, module.inputs = ["images"], module.mode = "INT8", module.outputs = ["/model.23/dfl/conv/Conv_output_0_Conv_f32", "/model.23/Sigmoid_output_0_Sigmoid_f32"], module.platform = "ONNX", module.q_group_size = 0 : i64, module.state = "TPU_ADDRESSED", module.top_run_mode = "STATIC", module.weight_file = "yolo11s_tpu_addressed_cv181x_int8_sym_weight.npz"} {
  module @yolo11s attributes {module.coeff_addr = 1099511627776 : i64, module.coeff_size = 9565440 : i64, module.device_id = 0 : i64, module.neuron_size = 1003520 : i64, module.private_size = 0 : i64, module.step = 0 : i64} {
    func.func @main(%arg0: tensor<1x3x224x320xf32> loc(unknown)) -> (tensor<1x1x4x1470xf32, 5497558138880 : i64>, tensor<1x4x1470xf32, 4398046511104 : i64>) {
      %0 = "top.Input"(%arg0) {channel_format = "nchw", do_preprocess = true, keep_aspect_ratio = true, keep_ratio_mode = "letterbox", mean = [0.000000e+00, 0.000000e+00, 0.000000e+00], pad_type = "center", pad_value = 0 : i64, pixel_format = "rgb", resize_dims = [224, 320], scale = [0.0039215688593685627, 0.0039215688593685627, 0.0039215688593685627]} : (tensor<1x3x224x320xf32>) -> tensor<1x3x224x320x!quant.uniform<i8:f32, 0.0078740157480314959>, 3298534883328 : i64> loc(#loc1)
      %1:2 = call @subfunc_0(%0) : (tensor<1x3x224x320x!quant.uniform<i8:f32, 0.0078740157480314959>, 3298534883328 : i64>) -> (tensor<1x1x4x1470xf32, 5497558138880 : i64>, tensor<1x4x1470xf32, 4398046511104 : i64>) loc(#loc)
      return %1#0, %1#1 : tensor<1x1x4x1470xf32, 5497558138880 : i64>, tensor<1x4x1470xf32, 4398046511104 : i64> loc(#loc)
    } loc(#loc)
    func.func @subfunc_0(%arg0: tensor<1x3x224x320x!quant.uniform<i8:f32, 0.0078740157480314959>, 3298534883328 : i64> loc("images")) -> (tensor<1x1x4x1470xf32, 5497558138880 : i64>, tensor<1x4x1470xf32, 4398046511104 : i64>) attributes {id = 0 : i64, mode = #tpu<run_mode TPU_STATIC>, next_index = array<i32: -1>} {
      %0 = "top.None"() : () -> none loc(#loc)
      %1 = "top.Weight"() : () -> tensor<1x32x1x9xi8, 1099513340768 : i64> loc(#loc2)
      %2 = "top.Weight"() : () -> tensor<1x32x9x4xsi8, 1099514023600 : i64> loc(#loc3)
      %3 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099514495792 : i64> loc(#loc4)
      %4 = "top.Weight"() : () -> tensor<1x64x1x9xi8, 1099514339696 : i64> loc(#loc5)
      %5 = "top.Weight"() : () -> tensor<1x64x9x32xsi8, 1099515664368 : i64> loc(#loc6)
      %6 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099516211568 : i64> loc(#loc7)
      %7 = "top.Weight"() : () -> tensor<1x64x1x9xi8, 1099515088688 : i64> loc(#loc8)
      %8 = "top.Weight"() : () -> tensor<1x64x1x64xsi8, 1099515743024 : i64> loc(#loc9)
      %9 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099513228304 : i64> loc(#loc10)
      %10 = "tpu.Group"(%arg0) ({
        %395 = "tpu.Load"(%arg0) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 11200, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [3], d_idx = [0], d_slice = [1], h_idx = [0, 29, 61, 93, 125, 157, 189], h_slice = [32, 35, 35, 35, 35, 35, 35], w_idx = [0], w_slice = [320], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 4 : i64} : (tensor<1x3x224x320x!quant.uniform<i8:f32, 0.0078740157480314959>, 3298534883328 : i64>) -> tensor<1x3x224x320x!quant.uniform<i8:f32, 0.0078740157480314959>> loc(#loc12)
        %396 = "tpu.Load"(%2) {do_bcast = false, ginfo = #tpu.lg<out_addr = 11712, out_size = 144, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [4], id = 1, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x32x9x4xsi8, 1099514023600 : i64>) -> tensor<1x32x9x4xsi8> loc(#loc13)
        %397 = "tpu.Load"(%1) {do_bcast = false, ginfo = #tpu.lg<out_addr = 11856, out_size = 36, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 2, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x32x1x9xi8, 1099513340768 : i64>) -> tensor<1x32x1x9xi8> loc(#loc14)
        %398 = "tpu.Load"(%3) {do_bcast = true, ginfo = #tpu.lg<out_addr = 32000, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 3, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099514495792 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc15)
        %399 = "tpu.Conv2D"(%395, %396, %397) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 10880, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0, 15, 31, 47, 63, 79, 95], h_slice = [16, 17, 17, 17, 17, 17, 17], w_idx = [0], w_slice = [160], id = 4, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [2, 2], support_compress = true, use_3ic_optimize = 4 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x3x224x320x!quant.uniform<i8:f32, 0.0078740157480314959>>, tensor<1x32x9x4xsi8>, tensor<1x32x1x9xi8>) -> tensor<1x32x112x160x!quant.uniform<i8:f32, 0.49818741417322837>> loc(#loc16)
        %400 = "tpu.Load"(%5) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [32], id = 5, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x9x32xsi8, 1099515664368 : i64>) -> tensor<1x64x9x32xsi8> loc(#loc17)
        %401 = "tpu.Load"(%4) {do_bcast = false, ginfo = #tpu.lg<out_addr = 11904, out_size = 72, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 6, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x9xi8, 1099514339696 : i64>) -> tensor<1x64x1x9xi8> loc(#loc18)
        %402 = "tpu.Lut"(%399, %398) {ginfo = #tpu.lg<out_addr = 0, out_size = 10880, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0, 15, 31, 47, 63, 79, 95], h_slice = [16, 17, 17, 17, 17, 17, 17], w_idx = [0], w_slice = [160], id = 7, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x32x112x160x!quant.uniform<i8:f32, 0.49818741417322837>>, tensor<1x1x1x256xsi8>) -> tensor<1x32x112x160x!quant.uniform<i8:f32, 0.46105073937007873>> loc(#loc19)
        %403 = "tpu.Load"(%6) {do_bcast = true, ginfo = #tpu.lg<out_addr = 23168, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 8, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099516211568 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc20)
        %404 = "tpu.Conv2D"(%402, %400, %401) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 5120, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 8, 16, 24, 32, 40, 48], h_slice = [8, 8, 8, 8, 8, 8, 8], w_idx = [0], w_slice = [80], id = 9, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [2, 2], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x32x112x160x!quant.uniform<i8:f32, 0.46105073937007873>>, tensor<1x64x9x32xsi8>, tensor<1x64x1x9xi8>) -> tensor<1x64x56x80x!quant.uniform<i8:f32, 0.6094166228346457>> loc(#loc21)
        %405 = "tpu.Load"(%8) {do_bcast = false, ginfo = #tpu.lg<out_addr = 11200, out_size = 512, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [64], id = 10, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x64xsi8, 1099515743024 : i64>) -> tensor<1x64x1x64xsi8> loc(#loc22)
        %406 = "tpu.Load"(%7) {do_bcast = false, ginfo = #tpu.lg<out_addr = 32256, out_size = 72, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 11, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x9xi8, 1099515088688 : i64>) -> tensor<1x64x1x9xi8> loc(#loc23)
        %407 = "tpu.Lut"(%404, %403) {ginfo = #tpu.lg<out_addr = 0, out_size = 5120, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 8, 16, 24, 32, 40, 48], h_slice = [8, 8, 8, 8, 8, 8, 8], w_idx = [0], w_slice = [80], id = 12, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x64x56x80x!quant.uniform<i8:f32, 0.6094166228346457>>, tensor<1x1x1x256xsi8>) -> tensor<1x64x56x80x!quant.uniform<i8:f32, 0.47554662362204719>> loc(#loc24)
        %408 = "tpu.Load"(%9) {do_bcast = true, ginfo = #tpu.lg<out_addr = 23424, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 13, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099513228304 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc25)
        %409 = "tpu.Conv2D"(%407, %405, %406) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 5120, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 8, 16, 24, 32, 40, 48], h_slice = [8, 8, 8, 8, 8, 8, 8], w_idx = [0], w_slice = [80], id = 14, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x56x80x!quant.uniform<i8:f32, 0.47554662362204719>>, tensor<1x64x1x64xsi8>, tensor<1x64x1x9xi8>) -> tensor<1x64x56x80x!quant.uniform<i8:f32, 0.2627664401574803>> loc(#loc26)
        %410 = "tpu.Lut"(%409, %408) {ginfo = #tpu.lg<out_addr = 26880, out_size = 5120, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 8, 16, 24, 32, 40, 48], h_slice = [8, 8, 8, 8, 8, 8, 8], w_idx = [0], w_slice = [80], id = 15, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x64x56x80x!quant.uniform<i8:f32, 0.2627664401574803>>, tensor<1x1x1x256xsi8>) -> tensor<1x64x56x80x!quant.uniform<i8:f32, 0.14772613385826772>> loc(#loc11)
        %411 = "tpu.Store"(%410, %0) {ginfo = #tpu.lg<out_addr = 26880, out_size = 5120, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 8, 16, 24, 32, 40, 48], h_slice = [8, 8, 8, 8, 8, 8, 8], w_idx = [0], w_slice = [80], id = 16, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x64x56x80x!quant.uniform<i8:f32, 0.14772613385826772>>, none) -> tensor<1x64x56x80x!quant.uniform<i8:f32, 0.14772613385826772>, 0 : i64> loc(#loc11)
        "tpu.Yield"(%411) : (tensor<1x64x56x80x!quant.uniform<i8:f32, 0.14772613385826772>, 0 : i64>) -> () loc(#loc11)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 4, 3, 16, -2, 7, 5, 6, -3, 9, 8, -4, 12, 10, 11, -5, 14, 13, -6, 15, 0, 1, 2], group_type = 0 : i64, hsecs = 7 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x3x224x320x!quant.uniform<i8:f32, 0.0078740157480314959>, 3298534883328 : i64>) -> tensor<1x64x56x80x!quant.uniform<i8:f32, 0.14772613385826772>, 0 : i64> loc(#loc11)
      %11 = "tpu.Slice"(%10, %0, %0, %0, %0) {axes = [], ends = [9223372036854775807, 32, 9223372036854775807, 9223372036854775807], hasparamConvert_axes = [1], offset = [0, 0, 0, 0], steps = [1, 1, 1, 1]} : (tensor<1x64x56x80x!quant.uniform<i8:f32, 0.14772613385826772>, 0 : i64>, none, none, none, none) -> tensor<1x32x56x80x!quant.uniform<i8:f32, 0.14772613385826772>, 0 : i64> loc(#loc27)
      %12 = "tpu.Slice"(%10, %0, %0, %0, %0) {axes = [], ends = [9223372036854775807, 64, 9223372036854775807, 9223372036854775807], hasparamConvert_axes = [1], offset = [0, 32, 0, 0], steps = [1, 1, 1, 1]} : (tensor<1x64x56x80x!quant.uniform<i8:f32, 0.14772613385826772>, 0 : i64>, none, none, none, none) -> tensor<1x32x56x80x!quant.uniform<i8:f32, 0.14772613385826772>, 143360 : i64> loc(#loc28)
      %13 = "top.Weight"() : () -> tensor<1x16x1x9xi8, 1099514017504 : i64> loc(#loc29)
      %14 = "top.Weight"() : () -> tensor<1x16x9x32xsi8, 1099512504592 : i64> loc(#loc30)
      %15 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099515123184 : i64> loc(#loc31)
      %16 = "tpu.Group"(%12) ({
        %395 = "tpu.Load"(%12) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 9280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0, 27], h_slice = [29, 29], w_idx = [0], w_slice = [80], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x32x56x80x!quant.uniform<i8:f32, 0.14772613385826772>, 143360 : i64>) -> tensor<1x32x56x80x!quant.uniform<i8:f32, 0.14772613385826772>> loc(#loc33)
        %396 = "tpu.Load"(%14) {do_bcast = false, ginfo = #tpu.lg<out_addr = 28672, out_size = 576, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [16], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [32], id = 1, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x16x9x32xsi8, 1099512504592 : i64>) -> tensor<1x16x9x32xsi8> loc(#loc34)
        %397 = "tpu.Load"(%13) {do_bcast = false, ginfo = #tpu.lg<out_addr = 9536, out_size = 18, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [16], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 2, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x16x1x9xi8, 1099514017504 : i64>) -> tensor<1x16x1x9xi8> loc(#loc35)
        %398 = "tpu.Load"(%15) {do_bcast = true, ginfo = #tpu.lg<out_addr = 9280, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 3, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099515123184 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc36)
        %399 = "tpu.Conv2D"(%395, %396, %397) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 4480, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [16], d_idx = [0], d_slice = [1], h_idx = [0, 28], h_slice = [28, 28], w_idx = [0], w_slice = [80], id = 4, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x32x56x80x!quant.uniform<i8:f32, 0.14772613385826772>>, tensor<1x16x9x32xsi8>, tensor<1x16x1x9xi8>) -> tensor<1x16x56x80x!quant.uniform<i8:f32, 0.074481342519685037>> loc(#loc37)
        %400 = "tpu.Lut"(%399, %398) {ginfo = #tpu.lg<out_addr = 20480, out_size = 4480, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [16], d_idx = [0], d_slice = [1], h_idx = [0, 28], h_slice = [28, 28], w_idx = [0], w_slice = [80], id = 5, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x16x56x80x!quant.uniform<i8:f32, 0.074481342519685037>>, tensor<1x1x1x256xsi8>) -> tensor<1x16x56x80x!quant.uniform<i8:f32, 0.069902049606299213>> loc(#loc32)
        %401 = "tpu.Store"(%400, %0) {ginfo = #tpu.lg<out_addr = 20480, out_size = 4480, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [16], d_idx = [0], d_slice = [1], h_idx = [0, 28], h_slice = [28, 28], w_idx = [0], w_slice = [80], id = 6, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x16x56x80x!quant.uniform<i8:f32, 0.069902049606299213>>, none) -> tensor<1x16x56x80x!quant.uniform<i8:f32, 0.069902049606299213>, 286720 : i64> loc(#loc32)
        "tpu.Yield"(%401) : (tensor<1x16x56x80x!quant.uniform<i8:f32, 0.069902049606299213>, 286720 : i64>) -> () loc(#loc32)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 4, 3, 6, -2, 5, 0, 1, 2], group_type = 0 : i64, hsecs = 2 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x32x56x80x!quant.uniform<i8:f32, 0.14772613385826772>, 143360 : i64>) -> tensor<1x16x56x80x!quant.uniform<i8:f32, 0.069902049606299213>, 286720 : i64> loc(#loc32)
      %17 = "top.Weight"() : () -> tensor<1x32x1x9xi8, 1099514014336 : i64> loc(#loc38)
      %18 = "top.Weight"() : () -> tensor<1x32x9x16xsi8, 1099513852160 : i64> loc(#loc39)
      %19 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099514020208 : i64> loc(#loc40)
      %20 = "tpu.Group"(%16) ({
        %395 = "tpu.Load"(%16) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 4640, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [16], d_idx = [0], d_slice = [1], h_idx = [0, 27], h_slice = [29, 29], w_idx = [0], w_slice = [80], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x16x56x80x!quant.uniform<i8:f32, 0.069902049606299213>, 286720 : i64>) -> tensor<1x16x56x80x!quant.uniform<i8:f32, 0.069902049606299213>> loc(#loc42)
        %396 = "tpu.Load"(%18) {do_bcast = false, ginfo = #tpu.lg<out_addr = 8960, out_size = 576, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [16], id = 1, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x32x9x16xsi8, 1099513852160 : i64>) -> tensor<1x32x9x16xsi8> loc(#loc43)
        %397 = "tpu.Load"(%17) {do_bcast = false, ginfo = #tpu.lg<out_addr = 9792, out_size = 36, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 2, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x32x1x9xi8, 1099514014336 : i64>) -> tensor<1x32x1x9xi8> loc(#loc44)
        %398 = "tpu.Load"(%19) {do_bcast = true, ginfo = #tpu.lg<out_addr = 9536, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 3, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099514020208 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc45)
        %399 = "tpu.Conv2D"(%395, %396, %397) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 0, out_size = 8960, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0, 28], h_slice = [28, 28], w_idx = [0], w_slice = [80], id = 4, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x16x56x80x!quant.uniform<i8:f32, 0.069902049606299213>>, tensor<1x32x9x16xsi8>, tensor<1x32x1x9xi8>) -> tensor<1x32x56x80x!quant.uniform<i8:f32, 0.11104472913385827>> loc(#loc46)
        %400 = "tpu.Lut"(%399, %398) {ginfo = #tpu.lg<out_addr = 12288, out_size = 8960, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0, 28], h_slice = [28, 28], w_idx = [0], w_slice = [80], id = 5, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x32x56x80x!quant.uniform<i8:f32, 0.11104472913385827>>, tensor<1x1x1x256xsi8>) -> tensor<1x32x56x80x!quant.uniform<i8:f32, 0.097719342519685046>> loc(#loc41)
        %401 = "tpu.Store"(%400, %0) {ginfo = #tpu.lg<out_addr = 12288, out_size = 8960, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0, 28], h_slice = [28, 28], w_idx = [0], w_slice = [80], id = 6, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x32x56x80x!quant.uniform<i8:f32, 0.097719342519685046>>, none) -> tensor<1x32x56x80x!quant.uniform<i8:f32, 0.097719342519685046>, 860160 : i64> loc(#loc41)
        "tpu.Yield"(%401) : (tensor<1x32x56x80x!quant.uniform<i8:f32, 0.097719342519685046>, 860160 : i64>) -> () loc(#loc41)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 4, 3, 6, -2, 5, 0, 1, 2], group_type = 0 : i64, hsecs = 2 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x16x56x80x!quant.uniform<i8:f32, 0.069902049606299213>, 286720 : i64>) -> tensor<1x32x56x80x!quant.uniform<i8:f32, 0.097719342519685046>, 860160 : i64> loc(#loc41)
      %21 = "top.Weight"() : () -> tensor<1x128x1x9xi8, 1099514034672 : i64> loc(#loc47)
      %22 = "top.Weight"() : () -> tensor<1x128x1x96xsi8, 1099513231152 : i64> loc(#loc48)
      %23 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099514017648 : i64> loc(#loc49)
      %24 = "tpu.Group"(%12, %20, %11) ({
        %395 = "tpu.Load"(%12) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0, 8, 16, 24, 32, 40, 48], h_slice = [8, 8, 8, 8, 8, 8, 8], w_idx = [0], w_slice = [80], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x32x56x80x!quant.uniform<i8:f32, 0.14772613385826772>, 143360 : i64>) -> tensor<1x32x56x80x!quant.uniform<i8:f32, 0.14772613385826772>> loc(#loc33)
        %396 = "tpu.Load"(%20) {do_bcast = false, ginfo = #tpu.lg<out_addr = 28672, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0, 8, 16, 24, 32, 40, 48], h_slice = [8, 8, 8, 8, 8, 8, 8], w_idx = [0], w_slice = [80], id = 1, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x32x56x80x!quant.uniform<i8:f32, 0.097719342519685046>, 860160 : i64>) -> tensor<1x32x56x80x!quant.uniform<i8:f32, 0.097719342519685046>> loc(#loc51)
        %397 = "tpu.Load"(%11) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0, 8, 16, 24, 32, 40, 48], h_slice = [8, 8, 8, 8, 8, 8, 8], w_idx = [0], w_slice = [80], id = 2, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x32x56x80x!quant.uniform<i8:f32, 0.14772613385826772>, 0 : i64>) -> tensor<1x32x56x80x!quant.uniform<i8:f32, 0.14772613385826772>> loc(#loc52)
        %398 = "tpu.Add"(%395, %396) {do_relu = false, ginfo = #tpu.lg<out_addr = 4096, out_size = 2560, buffer_addr = 8192, buffer_size = 2560, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0, 8, 16, 24, 32, 40, 48], h_slice = [8, 8, 8, 8, 8, 8, 8], w_idx = [0], w_slice = [80], id = 3, stage = 1, slice_idx = 0, group_type = 0>, multipliers = [70, 46], relu_limit = -1.000000e+00 : f64, rshifts = [6]} : (tensor<1x32x56x80x!quant.uniform<i8:f32, 0.14772613385826772>>, tensor<1x32x56x80x!quant.uniform<i8:f32, 0.097719342519685046>>) -> tensor<1x32x56x80x!quant.uniform<i8:f32, 0.13364397952755905>> loc(#loc53)
        %399 = "tpu.Load"(%22) {do_bcast = false, ginfo = #tpu.lg<out_addr = 22528, out_size = 1536, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [96], id = 4, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x96xsi8, 1099513231152 : i64>) -> tensor<1x128x1x96xsi8> loc(#loc54)
        %400 = "tpu.Load"(%21) {do_bcast = false, ginfo = #tpu.lg<out_addr = 31232, out_size = 144, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 5, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x9xi8, 1099514034672 : i64>) -> tensor<1x128x1x9xi8> loc(#loc55)
        %401 = "tpu.Concat"(%397, %395, %398) {axis = 1 : si32, do_relu = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 7680, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [96], d_idx = [0], d_slice = [1], h_idx = [0, 8, 16, 24, 32, 40, 48], h_slice = [8, 8, 8, 8, 8, 8, 8], w_idx = [0], w_slice = [80], id = 6, stage = 1, slice_idx = 0, group_type = 0>, multipliers = [1, 1, 115], only_merge = false, relu_limit = -1.000000e+00 : f64, rshifts = [0, 0, 7]} : (tensor<1x32x56x80x!quant.uniform<i8:f32, 0.14772613385826772>>, tensor<1x32x56x80x!quant.uniform<i8:f32, 0.14772613385826772>>, tensor<1x32x56x80x!quant.uniform<i8:f32, 0.13364397952755905>>) -> tensor<1x96x56x80x!quant.uniform<i8:f32, 0.14772613385826772>> loc(#loc56)
        %402 = "tpu.Load"(%23) {do_bcast = true, ginfo = #tpu.lg<out_addr = 10752, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 7, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099514017648 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc57)
        %403 = "tpu.Conv2D"(%401, %399, %400) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 0, out_size = 10240, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 8, 16, 24, 32, 40, 48], h_slice = [8, 8, 8, 8, 8, 8, 8], w_idx = [0], w_slice = [80], id = 8, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x96x56x80x!quant.uniform<i8:f32, 0.14772613385826772>>, tensor<1x128x1x96xsi8>, tensor<1x128x1x9xi8>) -> tensor<1x128x56x80x!quant.uniform<i8:f32, 0.13573605984251969>> loc(#loc58)
        %404 = "tpu.Lut"(%403, %402) {ginfo = #tpu.lg<out_addr = 12288, out_size = 10240, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 8, 16, 24, 32, 40, 48], h_slice = [8, 8, 8, 8, 8, 8, 8], w_idx = [0], w_slice = [80], id = 9, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x128x56x80x!quant.uniform<i8:f32, 0.13573605984251969>>, tensor<1x1x1x256xsi8>) -> tensor<1x128x56x80x!quant.uniform<i8:f32, 0.090521100787401581>> loc(#loc50)
        %405 = "tpu.Store"(%404, %0) {ginfo = #tpu.lg<out_addr = 12288, out_size = 10240, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 8, 16, 24, 32, 40, 48], h_slice = [8, 8, 8, 8, 8, 8, 8], w_idx = [0], w_slice = [80], id = 10, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x128x56x80x!quant.uniform<i8:f32, 0.090521100787401581>>, none) -> tensor<1x128x56x80x!quant.uniform<i8:f32, 0.090521100787401581>, 286720 : i64> loc(#loc50)
        "tpu.Yield"(%405) : (tensor<1x128x56x80x!quant.uniform<i8:f32, 0.090521100787401581>, 286720 : i64>) -> () loc(#loc50)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 3, 2, 10, -2, 6, 4, 5, -3, 8, 7, -4, 9, 0, 1], group_type = 0 : i64, hsecs = 7 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [-2, 2], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x32x56x80x!quant.uniform<i8:f32, 0.14772613385826772>, 143360 : i64>, tensor<1x32x56x80x!quant.uniform<i8:f32, 0.097719342519685046>, 860160 : i64>, tensor<1x32x56x80x!quant.uniform<i8:f32, 0.14772613385826772>, 0 : i64>) -> tensor<1x128x56x80x!quant.uniform<i8:f32, 0.090521100787401581>, 286720 : i64> loc(#loc50)
      %25 = "top.Weight"() : () -> tensor<1x128x1x9xi8, 1099518104352 : i64> loc(#loc59)
      %26 = "top.Weight"() : () -> tensor<1x128x9x128xsi8, 1099515747376 : i64> loc(#loc60)
      %27 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099517466352 : i64> loc(#loc61)
      %28 = "top.Weight"() : () -> tensor<1x128x1x9xi8, 1099513881600 : i64> loc(#loc62)
      %29 = "top.Weight"() : () -> tensor<1x128x1x128xsi8, 1099512488208 : i64> loc(#loc63)
      %30 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099513340512 : i64> loc(#loc64)
      %31 = "tpu.Group"(%24) ({
        %395 = "tpu.Load"(%26) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 18432, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [128], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x9x128xsi8, 1099515747376 : i64>) -> tensor<1x128x9x128xsi8> loc(#loc66)
        %396 = "tpu.Load"(%24) {do_bcast = false, ginfo = #tpu.lg<out_addr = 20480, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 3, 7, 11, 15, 19, 23, 27, 31, 35, 39, 43, 47, 51], h_slice = [4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5], w_idx = [0], w_slice = [80], id = 1, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x56x80x!quant.uniform<i8:f32, 0.090521100787401581>, 286720 : i64>) -> tensor<1x128x56x80x!quant.uniform<i8:f32, 0.090521100787401581>> loc(#loc67)
        %397 = "tpu.Load"(%25) {do_bcast = false, ginfo = #tpu.lg<out_addr = 30208, out_size = 144, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 2, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x9xi8, 1099518104352 : i64>) -> tensor<1x128x1x9xi8> loc(#loc68)
        %398 = "tpu.Load"(%27) {do_bcast = true, ginfo = #tpu.lg<out_addr = 19968, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 3, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099517466352 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc69)
        %399 = "tpu.Conv2D"(%396, %395, %397) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 18688, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26], h_slice = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], w_idx = [0], w_slice = [40], id = 4, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [2, 2], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x56x80x!quant.uniform<i8:f32, 0.090521100787401581>>, tensor<1x128x9x128xsi8>, tensor<1x128x1x9xi8>) -> tensor<1x128x28x40x!quant.uniform<i8:f32, 0.062709842519685047>> loc(#loc70)
        %400 = "tpu.Load"(%29) {do_bcast = false, ginfo = #tpu.lg<out_addr = 26880, out_size = 2048, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [128], id = 5, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x128xsi8, 1099512488208 : i64>) -> tensor<1x128x1x128xsi8> loc(#loc71)
        %401 = "tpu.Load"(%28) {do_bcast = false, ginfo = #tpu.lg<out_addr = 20224, out_size = 144, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 6, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x9xi8, 1099513881600 : i64>) -> tensor<1x128x1x9xi8> loc(#loc72)
        %402 = "tpu.Lut"(%399, %398) {ginfo = #tpu.lg<out_addr = 20480, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26], h_slice = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], w_idx = [0], w_slice = [40], id = 7, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x128x28x40x!quant.uniform<i8:f32, 0.062709842519685047>>, tensor<1x1x1x256xsi8>) -> tensor<1x128x28x40x!quant.uniform<i8:f32, 0.054424705511811021>> loc(#loc73)
        %403 = "tpu.Load"(%30) {do_bcast = true, ginfo = #tpu.lg<out_addr = 18432, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 8, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099513340512 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc74)
        %404 = "tpu.Conv2D"(%402, %400, %401) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 18688, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26], h_slice = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], w_idx = [0], w_slice = [40], id = 9, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x28x40x!quant.uniform<i8:f32, 0.054424705511811021>>, tensor<1x128x1x128xsi8>, tensor<1x128x1x9xi8>) -> tensor<1x128x28x40x!quant.uniform<i8:f32, 0.062148030708661414>> loc(#loc75)
        %405 = "tpu.Lut"(%404, %403) {ginfo = #tpu.lg<out_addr = 28928, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26], h_slice = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], w_idx = [0], w_slice = [40], id = 10, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x128x28x40x!quant.uniform<i8:f32, 0.062148030708661414>>, tensor<1x1x1x256xsi8>) -> tensor<1x128x28x40x!quant.uniform<i8:f32, 0.054269591338582682>> loc(#loc65)
        %406 = "tpu.Store"(%405, %0) {ginfo = #tpu.lg<out_addr = 28928, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26], h_slice = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], w_idx = [0], w_slice = [40], id = 11, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x128x28x40x!quant.uniform<i8:f32, 0.054269591338582682>>, none) -> tensor<1x128x28x40x!quant.uniform<i8:f32, 0.054269591338582682>, 0 : i64> loc(#loc65)
        "tpu.Yield"(%406) : (tensor<1x128x28x40x!quant.uniform<i8:f32, 0.054269591338582682>, 0 : i64>) -> () loc(#loc65)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 4, 3, 11, -2, 7, 5, 6, -3, 9, 8, 0, -4, 10, 1, 2], group_type = 0 : i64, hsecs = 14 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [2], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x128x56x80x!quant.uniform<i8:f32, 0.090521100787401581>, 286720 : i64>) -> tensor<1x128x28x40x!quant.uniform<i8:f32, 0.054269591338582682>, 0 : i64> loc(#loc65)
      %32 = "tpu.Slice"(%31, %0, %0, %0, %0) {axes = [], ends = [9223372036854775807, 64, 9223372036854775807, 9223372036854775807], hasparamConvert_axes = [1], offset = [0, 0, 0, 0], steps = [1, 1, 1, 1]} : (tensor<1x128x28x40x!quant.uniform<i8:f32, 0.054269591338582682>, 0 : i64>, none, none, none, none) -> tensor<1x64x28x40x!quant.uniform<i8:f32, 0.054269591338582682>, 0 : i64> loc(#loc76)
      %33 = "tpu.Slice"(%31, %0, %0, %0, %0) {axes = [], ends = [9223372036854775807, 128, 9223372036854775807, 9223372036854775807], hasparamConvert_axes = [1], offset = [0, 64, 0, 0], steps = [1, 1, 1, 1]} : (tensor<1x128x28x40x!quant.uniform<i8:f32, 0.054269591338582682>, 0 : i64>, none, none, none, none) -> tensor<1x64x28x40x!quant.uniform<i8:f32, 0.054269591338582682>, 71680 : i64> loc(#loc77)
      %34 = "top.Weight"() {do_compress = true} : () -> tensor<1x32x1x9xi8, 1099513228560 : i64> loc(#loc78)
      %35 = "top.Weight"() {do_compress = true} : () -> tensor<1x32x9x64xsi8, 1099512809488 : i64> loc(#loc79)
      %36 = "tpu.Conv2D"(%33, %35, %34) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.054269591338582682>, 71680 : i64>, tensor<1x32x9x64xsi8, 1099512809488 : i64>, tensor<1x32x1x9xi8, 1099513228560 : i64>) -> tensor<1x32x28x40x!quant.uniform<i8:f32, 0.043688266929133864>, 143360 : i64> loc(#loc80)
      %37 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099512829328 : i64> loc(#loc81)
      %38 = "top.Weight"() : () -> tensor<1x64x1x9xi8, 1099514024752 : i64> loc(#loc82)
      %39 = "top.Weight"() : () -> tensor<1x64x9x32xsi8, 1099513321824 : i64> loc(#loc83)
      %40 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099514013824 : i64> loc(#loc84)
      %41 = "tpu.Group"(%36, %33) ({
        %395 = "tpu.Load"(%36) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 2432, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0, 13], h_slice = [15, 15], w_idx = [0], w_slice = [40], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x32x28x40x!quant.uniform<i8:f32, 0.043688266929133864>, 143360 : i64>) -> tensor<1x32x28x40x!quant.uniform<i8:f32, 0.043688266929133864>> loc(#loc86)
        %396 = "tpu.Load"(%37) {do_bcast = true, ginfo = #tpu.lg<out_addr = 6784, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 1, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099512829328 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc87)
        %397 = "tpu.Load"(%39) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [32], id = 2, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x9x32xsi8, 1099513321824 : i64>) -> tensor<1x64x9x32xsi8> loc(#loc88)
        %398 = "tpu.Load"(%38) {do_bcast = false, ginfo = #tpu.lg<out_addr = 20864, out_size = 72, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 3, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x9xi8, 1099514024752 : i64>) -> tensor<1x64x1x9xi8> loc(#loc89)
        %399 = "tpu.Lut"(%395, %396) {ginfo = #tpu.lg<out_addr = 4096, out_size = 2432, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0, 13], h_slice = [15, 15], w_idx = [0], w_slice = [40], id = 4, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x32x28x40x!quant.uniform<i8:f32, 0.043688266929133864>>, tensor<1x1x1x256xsi8>) -> tensor<1x32x28x40x!quant.uniform<i8:f32, 0.038853287401574801>> loc(#loc90)
        %400 = "tpu.Load"(%40) {do_bcast = true, ginfo = #tpu.lg<out_addr = 31488, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 5, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099514013824 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc91)
        %401 = "tpu.Conv2D"(%399, %397, %398) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 8192, out_size = 4480, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 14], h_slice = [14, 14], w_idx = [0], w_slice = [40], id = 6, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x32x28x40x!quant.uniform<i8:f32, 0.038853287401574801>>, tensor<1x64x9x32xsi8>, tensor<1x64x1x9xi8>) -> tensor<1x64x28x40x!quant.uniform<i8:f32, 0.065839774803149606>> loc(#loc92)
        %402 = "tpu.Load"(%33) {do_bcast = false, ginfo = #tpu.lg<out_addr = 2304, out_size = 4480, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 14], h_slice = [14, 14], w_idx = [0], w_slice = [40], id = 7, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.054269591338582682>, 71680 : i64>) -> tensor<1x64x28x40x!quant.uniform<i8:f32, 0.054269591338582682>> loc(#loc93)
        %403 = "tpu.Lut"(%401, %400) {ginfo = #tpu.lg<out_addr = 16384, out_size = 4480, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 14], h_slice = [14, 14], w_idx = [0], w_slice = [40], id = 8, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.065839774803149606>>, tensor<1x1x1x256xsi8>) -> tensor<1x64x28x40x!quant.uniform<i8:f32, 0.054567293700787403>> loc(#loc94)
        %404 = "tpu.Add"(%402, %403) {do_relu = false, ginfo = #tpu.lg<out_addr = 8192, out_size = 4480, buffer_addr = 27008, buffer_size = 4480, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 14], h_slice = [14, 14], w_idx = [0], w_slice = [40], id = 9, stage = 1, slice_idx = 0, group_type = 0>, multipliers = [123, 123], relu_limit = -1.000000e+00 : f64, rshifts = [7]} : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.054269591338582682>>, tensor<1x64x28x40x!quant.uniform<i8:f32, 0.054567293700787403>>) -> tensor<1x64x28x40x!quant.uniform<i8:f32, 0.05634539527559055>> loc(#loc85)
        %405 = "tpu.Store"(%404, %0) {ginfo = #tpu.lg<out_addr = 8192, out_size = 4480, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 14], h_slice = [14, 14], w_idx = [0], w_slice = [40], id = 10, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.05634539527559055>>, none) -> tensor<1x64x28x40x!quant.uniform<i8:f32, 0.05634539527559055>, 788480 : i64> loc(#loc85)
        "tpu.Yield"(%405) : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.05634539527559055>, 788480 : i64>) -> () loc(#loc85)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 4, 2, 3, 10, -2, 6, 5, -3, 8, 7, -4, 9, 0, 1], group_type = 0 : i64, hsecs = 2 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x32x28x40x!quant.uniform<i8:f32, 0.043688266929133864>, 143360 : i64>, tensor<1x64x28x40x!quant.uniform<i8:f32, 0.054269591338582682>, 71680 : i64>) -> tensor<1x64x28x40x!quant.uniform<i8:f32, 0.05634539527559055>, 788480 : i64> loc(#loc85)
      %42 = "top.Weight"() : () -> tensor<1x256x1x9xi8, 1099513314624 : i64> loc(#loc95)
      %43 = "top.Weight"() : () -> tensor<1x256x1x192xsi8, 1099514446640 : i64> loc(#loc96)
      %44 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099514014080 : i64> loc(#loc97)
      %45 = "tpu.Group"(%32, %33, %41) ({
        %395 = "tpu.Load"(%32) {do_bcast = false, ginfo = #tpu.lg<out_addr = 28672, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 4, 8, 12, 16, 20, 24], h_slice = [4, 4, 4, 4, 4, 4, 4], w_idx = [0], w_slice = [40], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.054269591338582682>, 0 : i64>) -> tensor<1x64x28x40x!quant.uniform<i8:f32, 0.054269591338582682>> loc(#loc99)
        %396 = "tpu.Load"(%33) {do_bcast = false, ginfo = #tpu.lg<out_addr = 6144, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 4, 8, 12, 16, 20, 24], h_slice = [4, 4, 4, 4, 4, 4, 4], w_idx = [0], w_slice = [40], id = 1, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.054269591338582682>, 71680 : i64>) -> tensor<1x64x28x40x!quant.uniform<i8:f32, 0.054269591338582682>> loc(#loc93)
        %397 = "tpu.Load"(%41) {do_bcast = false, ginfo = #tpu.lg<out_addr = 13312, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 4, 8, 12, 16, 20, 24], h_slice = [4, 4, 4, 4, 4, 4, 4], w_idx = [0], w_slice = [40], id = 2, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.05634539527559055>, 788480 : i64>) -> tensor<1x64x28x40x!quant.uniform<i8:f32, 0.05634539527559055>> loc(#loc100)
        %398 = "tpu.Load"(%43) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 6144, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [192], id = 3, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x1x192xsi8, 1099514446640 : i64>) -> tensor<1x256x1x192xsi8> loc(#loc101)
        %399 = "tpu.Load"(%42) {do_bcast = false, ginfo = #tpu.lg<out_addr = 7424, out_size = 288, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 4, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x1x9xi8, 1099513314624 : i64>) -> tensor<1x256x1x9xi8> loc(#loc102)
        %400 = "tpu.Concat"(%395, %396, %397) {axis = 1 : si32, do_relu = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 3840, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [192], d_idx = [0], d_slice = [1], h_idx = [0, 4, 8, 12, 16, 20, 24], h_slice = [4, 4, 4, 4, 4, 4, 4], w_idx = [0], w_slice = [40], id = 5, stage = 1, slice_idx = 0, group_type = 0>, multipliers = [126, 126, 65], only_merge = false, relu_limit = -1.000000e+00 : f64, rshifts = [7, 7, 6]} : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.054269591338582682>>, tensor<1x64x28x40x!quant.uniform<i8:f32, 0.054269591338582682>>, tensor<1x64x28x40x!quant.uniform<i8:f32, 0.05634539527559055>>) -> tensor<1x192x28x40x!quant.uniform<i8:f32, 0.055055193700787403>> loc(#loc103)
        %401 = "tpu.Load"(%44) {do_bcast = true, ginfo = #tpu.lg<out_addr = 7712, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 6, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099514014080 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc104)
        %402 = "tpu.Conv2D"(%400, %398, %399) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 5120, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0, 4, 8, 12, 16, 20, 24], h_slice = [4, 4, 4, 4, 4, 4, 4], w_idx = [0], w_slice = [40], id = 7, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x192x28x40x!quant.uniform<i8:f32, 0.055055193700787403>>, tensor<1x256x1x192xsi8>, tensor<1x256x1x9xi8>) -> tensor<1x256x28x40x!quant.uniform<i8:f32, 0.066453821259842533>> loc(#loc105)
        %403 = "tpu.Lut"(%402, %401) {ginfo = #tpu.lg<out_addr = 8192, out_size = 5120, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0, 4, 8, 12, 16, 20, 24], h_slice = [4, 4, 4, 4, 4, 4, 4], w_idx = [0], w_slice = [40], id = 8, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x256x28x40x!quant.uniform<i8:f32, 0.066453821259842533>>, tensor<1x1x1x256xsi8>) -> tensor<1x256x28x40x!quant.uniform<i8:f32, 0.045217366141732281>> loc(#loc98)
        %404 = "tpu.Store"(%403, %0) {ginfo = #tpu.lg<out_addr = 8192, out_size = 5120, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0, 4, 8, 12, 16, 20, 24], h_slice = [4, 4, 4, 4, 4, 4, 4], w_idx = [0], w_slice = [40], id = 9, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x256x28x40x!quant.uniform<i8:f32, 0.045217366141732281>>, none) -> tensor<1x256x28x40x!quant.uniform<i8:f32, 0.045217366141732281>, 501760 : i64> loc(#loc98)
        "tpu.Yield"(%404) : (tensor<1x256x28x40x!quant.uniform<i8:f32, 0.045217366141732281>, 501760 : i64>) -> () loc(#loc98)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 5, 3, 4, -2, 7, 6, 9, -3, 8, 0, 1, 2], group_type = 0 : i64, hsecs = 7 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.054269591338582682>, 0 : i64>, tensor<1x64x28x40x!quant.uniform<i8:f32, 0.054269591338582682>, 71680 : i64>, tensor<1x64x28x40x!quant.uniform<i8:f32, 0.05634539527559055>, 788480 : i64>) -> tensor<1x256x28x40x!quant.uniform<i8:f32, 0.045217366141732281>, 501760 : i64> loc(#loc98)
      %46 = "top.Weight"() {do_compress = true} : () -> tensor<1x256x1x9xi8, 1099513228848 : i64> loc(#loc106)
      %47 = "top.Weight"() {do_compress = true} : () -> tensor<1x256x9x256xsi8, 1099514496048 : i64> loc(#loc107)
      %48 = "tpu.Conv2D"(%45, %47, %46) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [2, 2], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x256x28x40x!quant.uniform<i8:f32, 0.045217366141732281>, 501760 : i64>, tensor<1x256x9x256xsi8, 1099514496048 : i64>, tensor<1x256x1x9xi8, 1099513228848 : i64>) -> tensor<1x256x14x20x!quant.uniform<i8:f32, 0.08080925511811024>, 0 : i64> loc(#loc108)
      %49 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099515088432 : i64> loc(#loc109)
      %50 = "top.Weight"() : () -> tensor<1x256x1x9xi8, 1099517463536 : i64> loc(#loc110)
      %51 = "top.Weight"() : () -> tensor<1x256x1x256xsi8, 1099513249072 : i64> loc(#loc111)
      %52 = "tpu.Group"(%48) ({
        %395 = "tpu.Load"(%48) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0, 7], h_slice = [7, 7], w_idx = [0], w_slice = [20], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x14x20x!quant.uniform<i8:f32, 0.08080925511811024>, 0 : i64>) -> tensor<1x256x14x20x!quant.uniform<i8:f32, 0.08080925511811024>> loc(#loc113)
        %396 = "tpu.Load"(%49) {do_bcast = true, ginfo = #tpu.lg<out_addr = 13088, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 1, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099515088432 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc114)
        %397 = "tpu.Load"(%51) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 8192, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 2, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x1x256xsi8, 1099513249072 : i64>) -> tensor<1x256x1x256xsi8> loc(#loc115)
        %398 = "tpu.Load"(%50) {do_bcast = false, ginfo = #tpu.lg<out_addr = 12800, out_size = 288, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 3, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x1x9xi8, 1099517463536 : i64>) -> tensor<1x256x1x9xi8> loc(#loc116)
        %399 = "tpu.Lut"(%395, %396) {ginfo = #tpu.lg<out_addr = 8192, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0, 7], h_slice = [7, 7], w_idx = [0], w_slice = [20], id = 4, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x256x14x20x!quant.uniform<i8:f32, 0.08080925511811024>>, tensor<1x1x1x256xsi8>) -> tensor<1x256x14x20x!quant.uniform<i8:f32, 0.054117548818897639>> loc(#loc117)
        %400 = "tpu.Conv2D"(%399, %397, %398) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0, 7], h_slice = [7, 7], w_idx = [0], w_slice = [20], id = 5, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x256x14x20x!quant.uniform<i8:f32, 0.054117548818897639>>, tensor<1x256x1x256xsi8>, tensor<1x256x1x9xi8>) -> tensor<1x256x14x20x!quant.uniform<i8:f32, 0.093799103149606308>> loc(#loc112)
        %401 = "tpu.Store"(%400, %0) {ginfo = #tpu.lg<out_addr = 16384, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0, 7], h_slice = [7, 7], w_idx = [0], w_slice = [20], id = 6, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x256x14x20x!quant.uniform<i8:f32, 0.093799103149606308>>, none) -> tensor<1x256x14x20x!quant.uniform<i8:f32, 0.093799103149606308>, 71680 : i64> loc(#loc112)
        "tpu.Yield"(%401) : (tensor<1x256x14x20x!quant.uniform<i8:f32, 0.093799103149606308>, 71680 : i64>) -> () loc(#loc112)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 4, 2, 3, 6, -2, 5, 0, 1], group_type = 0 : i64, hsecs = 2 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x256x14x20x!quant.uniform<i8:f32, 0.08080925511811024>, 0 : i64>) -> tensor<1x256x14x20x!quant.uniform<i8:f32, 0.093799103149606308>, 71680 : i64> loc(#loc112)
      %53 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099515085872 : i64> loc(#loc118)
      %54 = "tpu.Lut"(%52, %53) : (tensor<1x256x14x20x!quant.uniform<i8:f32, 0.093799103149606308>, 71680 : i64>, tensor<1x1x1x256xsi8, 1099515085872 : i64>) -> tensor<1x256x14x20x!quant.uniform<i8:f32, 0.088632026771653549>, 0 : i64> loc(#loc119)
      %55 = "tpu.Slice"(%54, %0, %0, %0, %0) {axes = [], ends = [9223372036854775807, 128, 9223372036854775807, 9223372036854775807], hasparamConvert_axes = [1], offset = [0, 0, 0, 0], steps = [1, 1, 1, 1]} : (tensor<1x256x14x20x!quant.uniform<i8:f32, 0.088632026771653549>, 0 : i64>, none, none, none, none) -> tensor<1x128x14x20x!quant.uniform<i8:f32, 0.088632026771653549>, 0 : i64> loc(#loc120)
      %56 = "tpu.Slice"(%54, %0, %0, %0, %0) {axes = [], ends = [9223372036854775807, 256, 9223372036854775807, 9223372036854775807], hasparamConvert_axes = [1], offset = [0, 128, 0, 0], steps = [1, 1, 1, 1]} : (tensor<1x256x14x20x!quant.uniform<i8:f32, 0.088632026771653549>, 0 : i64>, none, none, none, none) -> tensor<1x128x14x20x!quant.uniform<i8:f32, 0.088632026771653549>, 35840 : i64> loc(#loc121)
      %57 = "top.Weight"() : () -> tensor<1x64x1x9xi8, 1099519812560 : i64> loc(#loc122)
      %58 = "top.Weight"() : () -> tensor<1x64x1x128xsi8, 1099514026480 : i64> loc(#loc123)
      %59 = "top.Weight"() : () -> tensor<1x64x1x9xi8, 1099514023024 : i64> loc(#loc124)
      %60 = "top.Weight"() : () -> tensor<1x64x1x128xsi8, 1099517466608 : i64> loc(#loc125)
      %61 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099512114832 : i64> loc(#loc126)
      %62 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099515664112 : i64> loc(#loc127)
      %63 = "top.Weight"() : () -> tensor<1x64x1x9xi8, 1099514016928 : i64> loc(#loc128)
      %64 = "top.Weight"() : () -> tensor<1x64x9x64xsi8, 1099519256528 : i64> loc(#loc129)
      %65 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099515685616 : i64> loc(#loc130)
      %66 = "top.Weight"() : () -> tensor<1x64x1x9xi8, 1099515089840 : i64> loc(#loc131)
      %67 = "top.Weight"() : () -> tensor<1x64x9x64xsi8, 1099514036080 : i64> loc(#loc132)
      %68 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099515703984 : i64> loc(#loc133)
      %69 = "top.Weight"() : () -> tensor<1x64x1x9xi8, 1099516210992 : i64> loc(#loc134)
      %70 = "top.Weight"() : () -> tensor<1x64x9x64xsi8, 1099515704240 : i64> loc(#loc135)
      %71 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099515685360 : i64> loc(#loc136)
      %72 = "top.Weight"() : () -> tensor<1x64x1x9xi8, 1099519096976 : i64> loc(#loc137)
      %73 = "top.Weight"() : () -> tensor<1x64x9x64xsi8, 1099519381328 : i64> loc(#loc138)
      %74 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099513340256 : i64> loc(#loc139)
      %75 = "top.Weight"() : () -> tensor<1x128x1x9xi8, 1099514014624 : i64> loc(#loc140)
      %76 = "top.Weight"() : () -> tensor<1x128x1x128xsi8, 1099515685872 : i64> loc(#loc141)
      %77 = "tpu.Group"(%56) ({
        %395 = "tpu.Load"(%56) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [14], w_idx = [0], w_slice = [20], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.088632026771653549>, 35840 : i64>) -> tensor<1x128x14x20x!quant.uniform<i8:f32, 0.088632026771653549>> loc(#loc143)
        %396 = "tpu.Load"(%58) {do_bcast = false, ginfo = #tpu.lg<out_addr = 28672, out_size = 1024, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [128], id = 1, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x128xsi8, 1099514026480 : i64>) -> tensor<1x64x1x128xsi8> loc(#loc144)
        %397 = "tpu.Load"(%57) {do_bcast = false, ginfo = #tpu.lg<out_addr = 4752, out_size = 72, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 2, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x9xi8, 1099519812560 : i64>) -> tensor<1x64x1x9xi8> loc(#loc145)
        %398 = "tpu.Load"(%60) {do_bcast = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 1024, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [128], id = 3, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x128xsi8, 1099517466608 : i64>) -> tensor<1x64x1x128xsi8> loc(#loc146)
        %399 = "tpu.Load"(%59) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 72, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 4, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x9xi8, 1099514023024 : i64>) -> tensor<1x64x1x9xi8> loc(#loc147)
        %400 = "tpu.Conv2D"(%395, %396, %397) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 8192, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [14], w_idx = [0], w_slice = [20], id = 5, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.088632026771653549>>, tensor<1x64x1x128xsi8>, tensor<1x64x1x9xi8>) -> tensor<1x64x14x20x!quant.uniform<i8:f32, 0.054024407086614171>> loc(#loc148)
        %401 = "tpu.Load"(%61) {do_bcast = true, ginfo = #tpu.lg<out_addr = 20480, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 6, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099512114832 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc149)
        %402 = "tpu.Conv2D"(%395, %398, %399) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [14], w_idx = [0], w_slice = [20], id = 7, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.088632026771653549>>, tensor<1x64x1x128xsi8>, tensor<1x64x1x9xi8>) -> tensor<1x64x14x20x!quant.uniform<i8:f32, 0.11307370157480315>> loc(#loc150)
        %403 = "tpu.Load"(%62) {do_bcast = true, ginfo = #tpu.lg<out_addr = 24576, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 8, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099515664112 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc151)
        %404 = "tpu.Lut"(%400, %401) {ginfo = #tpu.lg<out_addr = 2304, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [14], w_idx = [0], w_slice = [20], id = 9, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.054024407086614171>>, tensor<1x1x1x256xsi8>) -> tensor<1x64x14x20x!quant.uniform<i8:f32, 0.045667181102362203>> loc(#loc152)
        %405 = "tpu.Load"(%64) {do_bcast = false, ginfo = #tpu.lg<out_addr = 8192, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [64], id = 10, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x9x64xsi8, 1099519256528 : i64>) -> tensor<1x64x9x64xsi8> loc(#loc153)
        %406 = "tpu.Load"(%63) {do_bcast = false, ginfo = #tpu.lg<out_addr = 28672, out_size = 72, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 11, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x9xi8, 1099514016928 : i64>) -> tensor<1x64x1x9xi8> loc(#loc154)
        %407 = "tpu.Lut"(%402, %403) {ginfo = #tpu.lg<out_addr = 0, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [14], w_idx = [0], w_slice = [20], id = 12, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.11307370157480315>>, tensor<1x1x1x256xsi8>) -> tensor<1x64x14x20x!quant.uniform<i8:f32, 0.10927710787401575>> loc(#loc155)
        %408 = "tpu.Load"(%65) {do_bcast = true, ginfo = #tpu.lg<out_addr = 20480, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 13, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099515685616 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc156)
        %409 = "tpu.Conv2D"(%404, %405, %406) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [14], w_idx = [0], w_slice = [20], id = 14, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.045667181102362203>>, tensor<1x64x9x64xsi8>, tensor<1x64x1x9xi8>) -> tensor<1x64x14x20x!quant.uniform<i8:f32, 0.087325163779527551>> loc(#loc157)
        %410 = "tpu.Load"(%67) {do_bcast = false, ginfo = #tpu.lg<out_addr = 4608, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [64], id = 15, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x9x64xsi8, 1099514036080 : i64>) -> tensor<1x64x9x64xsi8> loc(#loc158)
        %411 = "tpu.Load"(%66) {do_bcast = false, ginfo = #tpu.lg<out_addr = 28672, out_size = 72, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 16, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x9xi8, 1099515089840 : i64>) -> tensor<1x64x1x9xi8> loc(#loc159)
        %412 = "tpu.Lut"(%409, %408) {ginfo = #tpu.lg<out_addr = 12288, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [14], w_idx = [0], w_slice = [20], id = 17, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.087325163779527551>>, tensor<1x1x1x256xsi8>) -> tensor<1x64x14x20x!quant.uniform<i8:f32, 0.053206806299212601>> loc(#loc160)
        %413 = "tpu.Load"(%68) {do_bcast = true, ginfo = #tpu.lg<out_addr = 24576, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 18, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099515703984 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc161)
        %414 = "tpu.Conv2D"(%412, %410, %411) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [14], w_idx = [0], w_slice = [20], id = 19, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.053206806299212601>>, tensor<1x64x9x64xsi8>, tensor<1x64x1x9xi8>) -> tensor<1x64x14x20x!quant.uniform<i8:f32, 0.079110603937007862>> loc(#loc162)
        %415 = "tpu.Lut"(%414, %413) {ginfo = #tpu.lg<out_addr = 20480, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [14], w_idx = [0], w_slice = [20], id = 20, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.079110603937007862>>, tensor<1x1x1x256xsi8>) -> tensor<1x64x14x20x!quant.uniform<i8:f32, 0.054553804724409447>> loc(#loc163)
        %416 = "tpu.Load"(%70) {do_bcast = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [64], id = 21, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x9x64xsi8, 1099515704240 : i64>) -> tensor<1x64x9x64xsi8> loc(#loc164)
        %417 = "tpu.Load"(%69) {do_bcast = false, ginfo = #tpu.lg<out_addr = 28672, out_size = 72, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 22, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x9xi8, 1099516210992 : i64>) -> tensor<1x64x1x9xi8> loc(#loc165)
        %418 = "tpu.Add"(%404, %415) {do_relu = false, ginfo = #tpu.lg<out_addr = 8192, out_size = 2304, buffer_addr = 24576, buffer_size = 2304, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [14], w_idx = [0], w_slice = [20], id = 23, stage = 1, slice_idx = 0, group_type = 0>, multipliers = [66, 79], relu_limit = -1.000000e+00 : f64, rshifts = [7]} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.045667181102362203>>, tensor<1x64x14x20x!quant.uniform<i8:f32, 0.054553804724409447>>) -> tensor<1x64x14x20x!quant.uniform<i8:f32, 0.087464529921259845>> loc(#loc166)
        %419 = "tpu.Load"(%71) {do_bcast = true, ginfo = #tpu.lg<out_addr = 24576, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 24, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099515685360 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc167)
        %420 = "tpu.Conv2D"(%418, %416, %417) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 20480, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [14], w_idx = [0], w_slice = [20], id = 25, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.087464529921259845>>, tensor<1x64x9x64xsi8>, tensor<1x64x1x9xi8>) -> tensor<1x64x14x20x!quant.uniform<i8:f32, 0.07786492440944881>> loc(#loc168)
        %421 = "tpu.Load"(%73) {do_bcast = false, ginfo = #tpu.lg<out_addr = 2304, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [64], id = 26, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x9x64xsi8, 1099519381328 : i64>) -> tensor<1x64x9x64xsi8> loc(#loc169)
        %422 = "tpu.Load"(%72) {do_bcast = false, ginfo = #tpu.lg<out_addr = 28672, out_size = 72, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 27, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x9xi8, 1099519096976 : i64>) -> tensor<1x64x1x9xi8> loc(#loc170)
        %423 = "tpu.Lut"(%420, %419) {ginfo = #tpu.lg<out_addr = 10496, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [14], w_idx = [0], w_slice = [20], id = 28, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.07786492440944881>>, tensor<1x1x1x256xsi8>) -> tensor<1x64x14x20x!quant.uniform<i8:f32, 0.066325911811023625>> loc(#loc171)
        %424 = "tpu.Load"(%74) {do_bcast = true, ginfo = #tpu.lg<out_addr = 20480, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 29, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099513340256 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc172)
        %425 = "tpu.Conv2D"(%423, %421, %422) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [14], w_idx = [0], w_slice = [20], id = 30, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.066325911811023625>>, tensor<1x64x9x64xsi8>, tensor<1x64x1x9xi8>) -> tensor<1x64x14x20x!quant.uniform<i8:f32, 0.11823927480314961>> loc(#loc173)
        %426 = "tpu.Lut"(%425, %424) {ginfo = #tpu.lg<out_addr = 2304, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [14], w_idx = [0], w_slice = [20], id = 31, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.11823927480314961>>, tensor<1x1x1x256xsi8>) -> tensor<1x64x14x20x!quant.uniform<i8:f32, 0.14313128582677165>> loc(#loc174)
        %427 = "tpu.Add"(%418, %426) {do_relu = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 2304, buffer_addr = 12288, buffer_size = 2304, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [14], w_idx = [0], w_slice = [20], id = 32, stage = 1, slice_idx = 0, group_type = 0>, multipliers = [51, 83], relu_limit = -1.000000e+00 : f64, rshifts = [6]} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.087464529921259845>>, tensor<1x64x14x20x!quant.uniform<i8:f32, 0.14313128582677165>>) -> tensor<1x64x14x20x!quant.uniform<i8:f32, 0.10927710787401575>> loc(#loc175)
        %428 = "tpu.Load"(%76) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 2048, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [128], id = 33, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x128xsi8, 1099515685872 : i64>) -> tensor<1x128x1x128xsi8> loc(#loc176)
        %429 = "tpu.Load"(%75) {do_bcast = false, ginfo = #tpu.lg<out_addr = 4608, out_size = 144, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 34, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x9xi8, 1099514014624 : i64>) -> tensor<1x128x1x9xi8> loc(#loc177)
        %430 = "tpu.Concat"(%427, %407) {axis = 1 : si32, do_relu = false, ginfo = #tpu.lg<out_addr = 8192, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [14], w_idx = [0], w_slice = [20], id = 35, stage = 1, slice_idx = 0, group_type = 0>, multipliers = [1, 1], only_merge = true, relu_limit = -1.000000e+00 : f64, rshifts = [0, 0]} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.10927710787401575>>, tensor<1x64x14x20x!quant.uniform<i8:f32, 0.10927710787401575>>) -> tensor<1x128x14x20x!quant.uniform<i8:f32, 0.10927710787401575>> loc(#loc178)
        %431 = "tpu.Conv2D"(%430, %428, %429) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [14], w_idx = [0], w_slice = [20], id = 36, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.10927710787401575>>, tensor<1x128x1x128xsi8>, tensor<1x128x1x9xi8>) -> tensor<1x128x14x20x!quant.uniform<i8:f32, 0.11413456299212599>> loc(#loc142)
        %432 = "tpu.Store"(%431, %0) {ginfo = #tpu.lg<out_addr = 16384, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [14], w_idx = [0], w_slice = [20], id = 37, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.11413456299212599>>, none) -> tensor<1x128x14x20x!quant.uniform<i8:f32, 0.11413456299212599>, 71680 : i64> loc(#loc142)
        "tpu.Yield"(%432) : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.11413456299212599>, 71680 : i64>) -> () loc(#loc142)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 5, 3, 4, 37, -2, 7, 6, -3, 9, 8, -4, 12, 10, 11, -5, 14, 13, -6, 17, 15, 16, -7, 19, 18, -8, 20, -9, 23, 21, 22, -10, 25, 24, -11, 28, 26, 27, -12, 30, 29, -13, 31, -14, 32, -15, 35, 33, 34, -16, 36, 0, 1, 2], group_type = 0 : i64, hsecs = 1 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.088632026771653549>, 35840 : i64>) -> tensor<1x128x14x20x!quant.uniform<i8:f32, 0.11413456299212599>, 71680 : i64> loc(#loc142)
      %78 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099512832656 : i64> loc(#loc179)
      %79 = "top.Weight"() : () -> tensor<1x256x1x9xi8, 1099512829840 : i64> loc(#loc180)
      %80 = "top.Weight"() : () -> tensor<1x256x1x384xsi8, 1099514343728 : i64> loc(#loc181)
      %81 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099512829584 : i64> loc(#loc182)
      %82 = "tpu.Group"(%77, %55, %56) ({
        %395 = "tpu.Load"(%77) {do_bcast = false, ginfo = #tpu.lg<out_addr = 16896, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 7], h_slice = [7, 7], w_idx = [0], w_slice = [20], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.11413456299212599>, 71680 : i64>) -> tensor<1x128x14x20x!quant.uniform<i8:f32, 0.11413456299212599>> loc(#loc184)
        %396 = "tpu.Load"(%78) {do_bcast = true, ginfo = #tpu.lg<out_addr = 19488, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 1, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099512832656 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc185)
        %397 = "tpu.Load"(%55) {do_bcast = false, ginfo = #tpu.lg<out_addr = 20480, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 7], h_slice = [7, 7], w_idx = [0], w_slice = [20], id = 2, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.088632026771653549>, 0 : i64>) -> tensor<1x128x14x20x!quant.uniform<i8:f32, 0.088632026771653549>> loc(#loc186)
        %398 = "tpu.Load"(%56) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 7], h_slice = [7, 7], w_idx = [0], w_slice = [20], id = 3, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.088632026771653549>, 35840 : i64>) -> tensor<1x128x14x20x!quant.uniform<i8:f32, 0.088632026771653549>> loc(#loc143)
        %399 = "tpu.Load"(%79) {do_bcast = false, ginfo = #tpu.lg<out_addr = 19200, out_size = 288, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 4, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x1x9xi8, 1099512829840 : i64>) -> tensor<1x256x1x9xi8> loc(#loc187)
        %400 = "tpu.Lut"(%395, %396) {ginfo = #tpu.lg<out_addr = 28672, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 7], h_slice = [7, 7], w_idx = [0], w_slice = [20], id = 5, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.11413456299212599>>, tensor<1x1x1x256xsi8>) -> tensor<1x128x14x20x!quant.uniform<i8:f32, 0.069217777165354336>> loc(#loc188)
        %401 = "tpu.Load"(%80) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 12288, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [384], id = 6, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x1x384xsi8, 1099514343728 : i64>) -> tensor<1x256x1x384xsi8> loc(#loc189)
        %402 = "tpu.Concat"(%397, %398, %400) {axis = 1 : si32, do_relu = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 6912, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [384], d_idx = [0], d_slice = [1], h_idx = [0, 7], h_slice = [7, 7], w_idx = [0], w_slice = [20], id = 7, stage = 1, slice_idx = 0, group_type = 0>, multipliers = [1, 1, 99], only_merge = false, relu_limit = -1.000000e+00 : f64, rshifts = [0, 0, 7]} : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.088632026771653549>>, tensor<1x128x14x20x!quant.uniform<i8:f32, 0.088632026771653549>>, tensor<1x128x14x20x!quant.uniform<i8:f32, 0.069217777165354336>>) -> tensor<1x384x14x20x!quant.uniform<i8:f32, 0.088632026771653549>> loc(#loc190)
        %403 = "tpu.Load"(%81) {do_bcast = true, ginfo = #tpu.lg<out_addr = 30976, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 8, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099512829584 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc191)
        %404 = "tpu.Conv2D"(%402, %401, %399) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 20480, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0, 7], h_slice = [7, 7], w_idx = [0], w_slice = [20], id = 9, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x384x14x20x!quant.uniform<i8:f32, 0.088632026771653549>>, tensor<1x256x1x384xsi8>, tensor<1x256x1x9xi8>) -> tensor<1x256x14x20x!quant.uniform<i8:f32, 0.11384002047244095>> loc(#loc192)
        %405 = "tpu.Lut"(%404, %403) {ginfo = #tpu.lg<out_addr = 12288, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0, 7], h_slice = [7, 7], w_idx = [0], w_slice = [20], id = 10, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x256x14x20x!quant.uniform<i8:f32, 0.11384002047244095>>, tensor<1x1x1x256xsi8>) -> tensor<1x256x14x20x!quant.uniform<i8:f32, 0.089566330708661421>> loc(#loc183)
        %406 = "tpu.Store"(%405, %0) {ginfo = #tpu.lg<out_addr = 12288, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0, 7], h_slice = [7, 7], w_idx = [0], w_slice = [20], id = 11, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x256x14x20x!quant.uniform<i8:f32, 0.089566330708661421>>, none) -> tensor<1x256x14x20x!quant.uniform<i8:f32, 0.089566330708661421>, 430080 : i64> loc(#loc183)
        "tpu.Yield"(%406) : (tensor<1x256x14x20x!quant.uniform<i8:f32, 0.089566330708661421>, 430080 : i64>) -> () loc(#loc183)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 5, 2, 3, 11, 4, -2, 7, 6, -3, 9, 8, -4, 10, 0, 1], group_type = 0 : i64, hsecs = 2 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.11413456299212599>, 71680 : i64>, tensor<1x128x14x20x!quant.uniform<i8:f32, 0.088632026771653549>, 0 : i64>, tensor<1x128x14x20x!quant.uniform<i8:f32, 0.088632026771653549>, 35840 : i64>) -> tensor<1x256x14x20x!quant.uniform<i8:f32, 0.089566330708661421>, 430080 : i64> loc(#loc183)
      %83 = "top.Weight"() {do_compress = true} : () -> tensor<1x512x1x9xi8, 1099514442032 : i64> loc(#loc193)
      %84 = "top.Weight"() {do_compress = true} : () -> tensor<1x512x9x256xsi8, 1099516281072 : i64> loc(#loc194)
      %85 = "tpu.Conv2D"(%82, %84, %83) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [2, 2], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x256x14x20x!quant.uniform<i8:f32, 0.089566330708661421>, 430080 : i64>, tensor<1x512x9x256xsi8, 1099516281072 : i64>, tensor<1x512x1x9xi8, 1099514442032 : i64>) -> tensor<1x512x7x10x!quant.uniform<i8:f32, 0.087329143307086604>, 0 : i64> loc(#loc195)
      %86 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099512827920 : i64> loc(#loc196)
      %87 = "tpu.Lut"(%85, %86) : (tensor<1x512x7x10x!quant.uniform<i8:f32, 0.087329143307086604>, 0 : i64>, tensor<1x1x1x256xsi8, 1099512827920 : i64>) -> tensor<1x512x7x10x!quant.uniform<i8:f32, 0.067260511811023618>, 35840 : i64> loc(#loc197)
      %88 = "top.Weight"() {do_compress = true} : () -> tensor<1x512x1x9xi8, 1099512110224 : i64> loc(#loc198)
      %89 = "top.Weight"() {do_compress = true} : () -> tensor<1x512x1x512xsi8, 1099512542224 : i64> loc(#loc199)
      %90 = "tpu.Conv2D"(%87, %89, %88) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x512x7x10x!quant.uniform<i8:f32, 0.067260511811023618>, 35840 : i64>, tensor<1x512x1x512xsi8, 1099512542224 : i64>, tensor<1x512x1x9xi8, 1099512110224 : i64>) -> tensor<1x512x7x10x!quant.uniform<i8:f32, 0.14502109291338583>, 0 : i64> loc(#loc200)
      %91 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099512541968 : i64> loc(#loc201)
      %92 = "tpu.Lut"(%90, %91) : (tensor<1x512x7x10x!quant.uniform<i8:f32, 0.14502109291338583>, 0 : i64>, tensor<1x1x1x256xsi8, 1099512541968 : i64>) -> tensor<1x512x7x10x!quant.uniform<i8:f32, 0.12726251023622048>, 53760 : i64> loc(#loc202)
      %93 = "tpu.Slice"(%92, %0, %0, %0, %0) {axes = [], ends = [9223372036854775807, 256, 9223372036854775807, 9223372036854775807], hasparamConvert_axes = [1], offset = [0, 0, 0, 0], steps = [1, 1, 1, 1]} : (tensor<1x512x7x10x!quant.uniform<i8:f32, 0.12726251023622048>, 53760 : i64>, none, none, none, none) -> tensor<1x256x7x10x!quant.uniform<i8:f32, 0.12726251023622048>, 53760 : i64> loc(#loc203)
      %94 = "tpu.Slice"(%92, %0, %0, %0, %0) {axes = [], ends = [9223372036854775807, 512, 9223372036854775807, 9223372036854775807], hasparamConvert_axes = [1], offset = [0, 256, 0, 0], steps = [1, 1, 1, 1]} : (tensor<1x512x7x10x!quant.uniform<i8:f32, 0.12726251023622048>, 53760 : i64>, none, none, none, none) -> tensor<1x256x7x10x!quant.uniform<i8:f32, 0.12726251023622048>, 71680 : i64> loc(#loc204)
      %95 = "top.Weight"() : () -> tensor<1x128x1x9xi8, 1099512108816 : i64> loc(#loc205)
      %96 = "top.Weight"() : () -> tensor<1x128x1x256xsi8, 1099512509200 : i64> loc(#loc206)
      %97 = "top.Weight"() : () -> tensor<1x128x1x9xi8, 1099518413728 : i64> loc(#loc207)
      %98 = "top.Weight"() : () -> tensor<1x128x1x256xsi8, 1099520260560 : i64> loc(#loc208)
      %99 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099512804624 : i64> loc(#loc209)
      %100 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099514020464 : i64> loc(#loc210)
      %101 = "top.Weight"() : () -> tensor<1x128x1x9xi8, 1099514015776 : i64> loc(#loc211)
      %102 = "top.Weight"() : () -> tensor<1x128x9x128xsi8, 1099518198432 : i64> loc(#loc212)
      %103:3 = "tpu.Group"(%94) ({
        %395 = "tpu.Load"(%94) {do_bcast = false, ginfo = #tpu.lg<out_addr = 28672, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x7x10x!quant.uniform<i8:f32, 0.12726251023622048>, 71680 : i64>) -> tensor<1x256x7x10x!quant.uniform<i8:f32, 0.12726251023622048>> loc(#loc216)
        %396 = "tpu.Load"(%96) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 4096, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 1, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x256xsi8, 1099512509200 : i64>) -> tensor<1x128x1x256xsi8> loc(#loc217)
        %397 = "tpu.Load"(%95) {do_bcast = false, ginfo = #tpu.lg<out_addr = 19712, out_size = 144, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 2, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x9xi8, 1099512108816 : i64>) -> tensor<1x128x1x9xi8> loc(#loc218)
        %398 = "tpu.Load"(%98) {do_bcast = false, ginfo = #tpu.lg<out_addr = 20480, out_size = 4096, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 3, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x256xsi8, 1099520260560 : i64>) -> tensor<1x128x1x256xsi8> loc(#loc219)
        %399 = "tpu.Load"(%97) {do_bcast = false, ginfo = #tpu.lg<out_addr = 19968, out_size = 144, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 4, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x9xi8, 1099518413728 : i64>) -> tensor<1x128x1x9xi8> loc(#loc220)
        %400 = "tpu.Conv2D"(%395, %396, %397) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 18432, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 5, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x256x7x10x!quant.uniform<i8:f32, 0.12726251023622048>>, tensor<1x128x1x256xsi8>, tensor<1x128x1x9xi8>) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.11348717637795276>> loc(#loc221)
        %401 = "tpu.Load"(%99) {do_bcast = true, ginfo = #tpu.lg<out_addr = 19712, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 6, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099512804624 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc222)
        %402 = "tpu.Load"(%102) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 18432, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [128], id = 7, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x9x128xsi8, 1099518198432 : i64>) -> tensor<1x128x9x128xsi8> loc(#loc223)
        %403 = "tpu.Conv2D"(%395, %398, %399) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 8, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x256x7x10x!quant.uniform<i8:f32, 0.12726251023622048>>, tensor<1x128x1x256xsi8>, tensor<1x128x1x9xi8>) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.14065365354330708>> loc(#loc224)
        %404 = "tpu.Load"(%100) {do_bcast = true, ginfo = #tpu.lg<out_addr = 28672, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 9, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099514020464 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc225)
        %405 = "tpu.Lut"(%400, %401) {ginfo = #tpu.lg<out_addr = 20480, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 10, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.11348717637795276>>, tensor<1x1x1x256xsi8>) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.071737927559055129>> loc(#loc213)
        %406 = "tpu.Load"(%101) {do_bcast = false, ginfo = #tpu.lg<out_addr = 18432, out_size = 144, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 11, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x9xi8, 1099514015776 : i64>) -> tensor<1x128x1x9xi8> loc(#loc226)
        %407 = "tpu.Store"(%405, %0) {ginfo = #tpu.lg<out_addr = 20480, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 12, stage = 1, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.071737927559055129>>, none) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.071737927559055129>, 8960 : i64> loc(#loc213)
        %408 = "tpu.Lut"(%403, %404) {ginfo = #tpu.lg<out_addr = 21760, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 13, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.14065365354330708>>, tensor<1x1x1x256xsi8>) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.12381014488188977>> loc(#loc214)
        %409 = "tpu.Store"(%408, %0) {ginfo = #tpu.lg<out_addr = 21760, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 14, stage = 1, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.12381014488188977>>, none) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.12381014488188977>, 0 : i64> loc(#loc214)
        %410 = "tpu.Conv2D"(%405, %402, %406) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 31232, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 15, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.071737927559055129>>, tensor<1x128x9x128xsi8>, tensor<1x128x1x9xi8>) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.11165575354330709>> loc(#loc215)
        %411 = "tpu.Store"(%410, %0) {ginfo = #tpu.lg<out_addr = 31232, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 16, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.11165575354330709>>, none) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.11165575354330709>, 89600 : i64> loc(#loc215)
        "tpu.Yield"(%407, %409, %411) : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.071737927559055129>, 8960 : i64>, tensor<1x128x7x10x!quant.uniform<i8:f32, 0.12381014488188977>, 0 : i64>, tensor<1x128x7x10x!quant.uniform<i8:f32, 0.11165575354330709>, 89600 : i64>) -> () loc(#loc812)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 5, 3, 4, 16, -2, 8, 6, 7, -3, 10, 9, -4, 13, 11, 12, -5, 15, 14, 0, 1, 2], group_type = 0 : i64, hsecs = 1 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x256x7x10x!quant.uniform<i8:f32, 0.12726251023622048>, 71680 : i64>) -> (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.071737927559055129>, 8960 : i64>, tensor<1x128x7x10x!quant.uniform<i8:f32, 0.12381014488188977>, 0 : i64>, tensor<1x128x7x10x!quant.uniform<i8:f32, 0.11165575354330709>, 89600 : i64>) loc(#loc812)
      %104 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099515742768 : i64> loc(#loc227)
      %105 = "top.Weight"() : () -> tensor<1x128x1x9xi8, 1099512486800 : i64> loc(#loc228)
      %106 = "top.Weight"() : () -> tensor<1x128x9x128xsi8, 1099512339344 : i64> loc(#loc229)
      %107 = "tpu.Group"(%103#2) ({
        %395 = "tpu.Load"(%103#2) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.11165575354330709>, 89600 : i64>) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.11165575354330709>> loc(#loc231)
        %396 = "tpu.Load"(%104) {do_bcast = true, ginfo = #tpu.lg<out_addr = 18432, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 1, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099515742768 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc232)
        %397 = "tpu.Load"(%106) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 18432, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [128], id = 2, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x9x128xsi8, 1099512339344 : i64>) -> tensor<1x128x9x128xsi8> loc(#loc233)
        %398 = "tpu.Load"(%105) {do_bcast = false, ginfo = #tpu.lg<out_addr = 18688, out_size = 144, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 3, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x9xi8, 1099512486800 : i64>) -> tensor<1x128x1x9xi8> loc(#loc234)
        %399 = "tpu.Lut"(%395, %396) {ginfo = #tpu.lg<out_addr = 20480, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 4, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.11165575354330709>>, tensor<1x1x1x256xsi8>) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.066970493700787406>> loc(#loc235)
        %400 = "tpu.Conv2D"(%399, %397, %398) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 28672, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 5, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.066970493700787406>>, tensor<1x128x9x128xsi8>, tensor<1x128x1x9xi8>) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.074385530708661426>> loc(#loc230)
        %401 = "tpu.Store"(%400, %0) {ginfo = #tpu.lg<out_addr = 28672, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 6, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.074385530708661426>>, none) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.074385530708661426>, 98560 : i64> loc(#loc230)
        "tpu.Yield"(%401) : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.074385530708661426>, 98560 : i64>) -> () loc(#loc230)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 4, 2, 3, 6, -2, 5, 0, 1], group_type = 0 : i64, hsecs = 1 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.11165575354330709>, 89600 : i64>) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.074385530708661426>, 98560 : i64> loc(#loc230)
      %108 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099519059344 : i64> loc(#loc236)
      %109 = "top.Weight"() : () -> tensor<1x128x1x9xi8, 1099515741104 : i64> loc(#loc237)
      %110 = "top.Weight"() : () -> tensor<1x128x9x128xsi8, 1099520113104 : i64> loc(#loc238)
      %111:2 = "tpu.Group"(%107, %103#0) ({
        %395 = "tpu.Load"(%107) {do_bcast = false, ginfo = #tpu.lg<out_addr = 18432, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.074385530708661426>, 98560 : i64>) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.074385530708661426>> loc(#loc241)
        %396 = "tpu.Load"(%108) {do_bcast = true, ginfo = #tpu.lg<out_addr = 19712, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 1, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099519059344 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc242)
        %397 = "tpu.Load"(%103#0) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 2, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.071737927559055129>, 8960 : i64>) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.071737927559055129>> loc(#loc243)
        %398 = "tpu.Load"(%109) {do_bcast = false, ginfo = #tpu.lg<out_addr = 19968, out_size = 144, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 3, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x9xi8, 1099515741104 : i64>) -> tensor<1x128x1x9xi8> loc(#loc244)
        %399 = "tpu.Lut"(%395, %396) {ginfo = #tpu.lg<out_addr = 20480, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 4, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.074385530708661426>>, tensor<1x1x1x256xsi8>) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.065322955905511806>> loc(#loc245)
        %400 = "tpu.Load"(%110) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 18432, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [128], id = 5, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x9x128xsi8, 1099520113104 : i64>) -> tensor<1x128x9x128xsi8> loc(#loc246)
        %401 = "tpu.Add"(%397, %399) {do_relu = false, ginfo = #tpu.lg<out_addr = 28672, out_size = 1280, buffer_addr = 18432, buffer_size = 1280, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 6, stage = 1, slice_idx = 0, group_type = 0>, multipliers = [64, 59], relu_limit = -1.000000e+00 : f64, rshifts = [7]} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.071737927559055129>>, tensor<1x128x7x10x!quant.uniform<i8:f32, 0.065322955905511806>>) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.14145400551181103>> loc(#loc239)
        %402 = "tpu.Store"(%401, %0) {ginfo = #tpu.lg<out_addr = 28672, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 7, stage = 1, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.14145400551181103>>, none) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.14145400551181103>, 107520 : i64> loc(#loc239)
        %403 = "tpu.Conv2D"(%401, %400, %398) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 25856, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 8, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.14145400551181103>>, tensor<1x128x9x128xsi8>, tensor<1x128x1x9xi8>) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.10832528976377953>> loc(#loc240)
        %404 = "tpu.Store"(%403, %0) {ginfo = #tpu.lg<out_addr = 25856, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 9, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.10832528976377953>>, none) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.10832528976377953>, 17920 : i64> loc(#loc240)
        "tpu.Yield"(%402, %404) : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.14145400551181103>, 107520 : i64>, tensor<1x128x7x10x!quant.uniform<i8:f32, 0.10832528976377953>, 17920 : i64>) -> () loc(#loc813)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 4, 2, 9, 3, -2, 6, 5, -3, 8, 7, 0, 1], group_type = 0 : i64, hsecs = 1 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.074385530708661426>, 98560 : i64>, tensor<1x128x7x10x!quant.uniform<i8:f32, 0.071737927559055129>, 8960 : i64>) -> (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.14145400551181103>, 107520 : i64>, tensor<1x128x7x10x!quant.uniform<i8:f32, 0.10832528976377953>, 17920 : i64>) loc(#loc813)
      %112 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099519847632 : i64> loc(#loc247)
      %113 = "top.Weight"() : () -> tensor<1x128x1x9xi8, 1099518122464 : i64> loc(#loc248)
      %114 = "top.Weight"() : () -> tensor<1x128x9x128xsi8, 1099511824656 : i64> loc(#loc249)
      %115 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099512339088 : i64> loc(#loc250)
      %116 = "top.Weight"() : () -> tensor<1x256x1x9xi8, 1099515682800 : i64> loc(#loc251)
      %117 = "top.Weight"() : () -> tensor<1x256x1x256xsi8, 1099512273552 : i64> loc(#loc252)
      %118 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099512273296 : i64> loc(#loc253)
      %119 = "tpu.Group"(%111#1, %111#0, %103#1) ({
        %395 = "tpu.Load"(%111#1) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.10832528976377953>, 17920 : i64>) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.10832528976377953>> loc(#loc255)
        %396 = "tpu.Load"(%112) {do_bcast = true, ginfo = #tpu.lg<out_addr = 4096, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 1, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099519847632 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc256)
        %397 = "tpu.Load"(%114) {do_bcast = false, ginfo = #tpu.lg<out_addr = 8192, out_size = 18432, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [128], id = 2, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x9x128xsi8, 1099511824656 : i64>) -> tensor<1x128x9x128xsi8> loc(#loc257)
        %398 = "tpu.Load"(%113) {do_bcast = false, ginfo = #tpu.lg<out_addr = 27904, out_size = 144, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 3, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x9xi8, 1099518122464 : i64>) -> tensor<1x128x1x9xi8> loc(#loc258)
        %399 = "tpu.Lut"(%395, %396) {ginfo = #tpu.lg<out_addr = 26624, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 4, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.10832528976377953>>, tensor<1x1x1x256xsi8>) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.038552814173228346>> loc(#loc259)
        %400 = "tpu.Load"(%115) {do_bcast = true, ginfo = #tpu.lg<out_addr = 28048, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 5, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099512339088 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc260)
        %401 = "tpu.Load"(%117) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 8192, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 6, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x1x256xsi8, 1099512273552 : i64>) -> tensor<1x256x1x256xsi8> loc(#loc261)
        %402 = "tpu.Conv2D"(%399, %397, %398) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 31232, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 7, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.038552814173228346>>, tensor<1x128x9x128xsi8>, tensor<1x128x1x9xi8>) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.071548071653543308>> loc(#loc262)
        %403 = "tpu.Load"(%111#0) {do_bcast = false, ginfo = #tpu.lg<out_addr = 8192, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 8, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.14145400551181103>, 107520 : i64>) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.14145400551181103>> loc(#loc263)
        %404 = "tpu.Lut"(%402, %400) {ginfo = #tpu.lg<out_addr = 12288, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 9, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.071548071653543308>>, tensor<1x1x1x256xsi8>) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.067866428346456703>> loc(#loc264)
        %405 = "tpu.Load"(%103#1) {do_bcast = false, ginfo = #tpu.lg<out_addr = 20480, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 10, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.12381014488188977>, 0 : i64>) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.12381014488188977>> loc(#loc265)
        %406 = "tpu.Add"(%403, %404) {do_relu = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 1280, buffer_addr = 24576, buffer_size = 1280, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 11, stage = 1, slice_idx = 0, group_type = 0>, multipliers = [73, 35], relu_limit = -1.000000e+00 : f64, rshifts = [6]} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.14145400551181103>>, tensor<1x128x7x10x!quant.uniform<i8:f32, 0.067866428346456703>>) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.12381014488188977>> loc(#loc266)
        %407 = "tpu.Load"(%116) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 288, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 12, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x1x9xi8, 1099515682800 : i64>) -> tensor<1x256x1x9xi8> loc(#loc267)
        %408 = "tpu.Concat"(%406, %405) {axis = 1 : si32, do_relu = false, ginfo = #tpu.lg<out_addr = 8192, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 13, stage = 1, slice_idx = 0, group_type = 0>, multipliers = [1, 1], only_merge = true, relu_limit = -1.000000e+00 : f64, rshifts = [0, 0]} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.12381014488188977>>, tensor<1x128x7x10x!quant.uniform<i8:f32, 0.12381014488188977>>) -> tensor<1x256x7x10x!quant.uniform<i8:f32, 0.12381014488188977>> loc(#loc268)
        %409 = "tpu.Load"(%118) {do_bcast = true, ginfo = #tpu.lg<out_addr = 16384, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 14, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099512273296 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc269)
        %410 = "tpu.Conv2D"(%408, %401, %407) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 15, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x256x7x10x!quant.uniform<i8:f32, 0.12381014488188977>>, tensor<1x256x1x256xsi8>, tensor<1x256x1x9xi8>) -> tensor<1x256x7x10x!quant.uniform<i8:f32, 0.097484318110236223>> loc(#loc270)
        %411 = "tpu.Lut"(%410, %409) {ginfo = #tpu.lg<out_addr = 28672, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 16, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x256x7x10x!quant.uniform<i8:f32, 0.097484318110236223>>, tensor<1x1x1x256xsi8>) -> tensor<1x256x7x10x!quant.uniform<i8:f32, 0.11229675748031497>> loc(#loc254)
        %412 = "tpu.Store"(%411, %0) {ginfo = #tpu.lg<out_addr = 28672, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 17, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x256x7x10x!quant.uniform<i8:f32, 0.11229675748031497>>, none) -> tensor<1x256x7x10x!quant.uniform<i8:f32, 0.11229675748031497>, 89600 : i64> loc(#loc254)
        "tpu.Yield"(%412) : (tensor<1x256x7x10x!quant.uniform<i8:f32, 0.11229675748031497>, 89600 : i64>) -> () loc(#loc254)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 4, 2, 3, -2, 7, 5, 17, 6, -3, 9, 8, -4, 11, 10, -5, 13, 12, -6, 15, 14, -7, 16, 0, 1], group_type = 0 : i64, hsecs = 1 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.10832528976377953>, 17920 : i64>, tensor<1x128x7x10x!quant.uniform<i8:f32, 0.14145400551181103>, 107520 : i64>, tensor<1x128x7x10x!quant.uniform<i8:f32, 0.12381014488188977>, 0 : i64>) -> tensor<1x256x7x10x!quant.uniform<i8:f32, 0.11229675748031497>, 89600 : i64> loc(#loc254)
      %120 = "tpu.Concat"(%93, %94, %119) {axis = 1 : si32, do_relu = false, multipliers = [1, 1, 112], only_merge = false, relu_limit = -1.000000e+00 : f64, rshifts = [0, 0, 7]} : (tensor<1x256x7x10x!quant.uniform<i8:f32, 0.12726251023622048>, 53760 : i64>, tensor<1x256x7x10x!quant.uniform<i8:f32, 0.12726251023622048>, 71680 : i64>, tensor<1x256x7x10x!quant.uniform<i8:f32, 0.11229675748031497>, 89600 : i64>) -> tensor<1x768x7x10x!quant.uniform<i8:f32, 0.12726251023622048>, 0 : i64> loc(#loc271)
      %121 = "top.Weight"() {do_compress = true} : () -> tensor<1x512x1x9xi8, 1099512804880 : i64> loc(#loc272)
      %122 = "top.Weight"() {do_compress = true} : () -> tensor<1x512x1x768xsi8, 1099512833936 : i64> loc(#loc273)
      %123 = "tpu.Conv2D"(%120, %122, %121) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x768x7x10x!quant.uniform<i8:f32, 0.12726251023622048>, 0 : i64>, tensor<1x512x1x768xsi8, 1099512833936 : i64>, tensor<1x512x1x9xi8, 1099512804880 : i64>) -> tensor<1x512x7x10x!quant.uniform<i8:f32, 0.17194342440944882>, 71680 : i64> loc(#loc274)
      %124 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099512832400 : i64> loc(#loc275)
      %125 = "top.Weight"() : () -> tensor<1x256x1x9xi8, 1099514017904 : i64> loc(#loc276)
      %126 = "top.Weight"() : () -> tensor<1x256x1x512xsi8, 1099513721088 : i64> loc(#loc277)
      %127 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099516034608 : i64> loc(#loc278)
      %128:4 = "tpu.Group"(%123) ({
        %395 = "tpu.Load"(%123) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 5120, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [512], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x512x7x10x!quant.uniform<i8:f32, 0.17194342440944882>, 71680 : i64>) -> tensor<1x512x7x10x!quant.uniform<i8:f32, 0.17194342440944882>> loc(#loc283)
        %396 = "tpu.Load"(%124) {do_bcast = true, ginfo = #tpu.lg<out_addr = 29696, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 1, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099512832400 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc284)
        %397 = "tpu.Load"(%126) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 16384, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [512], id = 2, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x1x512xsi8, 1099513721088 : i64>) -> tensor<1x256x1x512xsi8> loc(#loc285)
        %398 = "tpu.Load"(%125) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24064, out_size = 288, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 3, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x1x9xi8, 1099514017904 : i64>) -> tensor<1x256x1x9xi8> loc(#loc286)
        %399 = "tpu.Lut"(%395, %396) {ginfo = #tpu.lg<out_addr = 16384, out_size = 5120, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [512], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 4, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x512x7x10x!quant.uniform<i8:f32, 0.17194342440944882>>, tensor<1x1x1x256xsi8>) -> tensor<1x512x7x10x!quant.uniform<i8:f32, 0.1377606960629921>> loc(#loc287)
        %400 = "tpu.Load"(%127) {do_bcast = true, ginfo = #tpu.lg<out_addr = 28672, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 5, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099516034608 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc288)
        %401 = "tpu.Conv2D"(%399, %397, %398) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 6, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x512x7x10x!quant.uniform<i8:f32, 0.1377606960629921>>, tensor<1x256x1x512xsi8>, tensor<1x256x1x9xi8>) -> tensor<1x256x7x10x!quant.uniform<i8:f32, 0.13783644645669291>> loc(#loc289)
        %402 = "tpu.Lut"(%401, %400) {ginfo = #tpu.lg<out_addr = 0, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 7, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x256x7x10x!quant.uniform<i8:f32, 0.13783644645669291>>, tensor<1x1x1x256xsi8>) -> tensor<1x256x7x10x!quant.uniform<i8:f32, 0.18272077007874016>> loc(#loc279)
        %403 = "tpu.Store"(%402, %0) {ginfo = #tpu.lg<out_addr = 0, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 8, stage = 1, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x256x7x10x!quant.uniform<i8:f32, 0.18272077007874016>>, none) -> tensor<1x256x7x10x!quant.uniform<i8:f32, 0.18272077007874016>, 0 : i64> loc(#loc279)
        %404 = "tpu.Pool2D"(%402) {count_include_pad = false, do_relu = false, first_round_mode = #tpu<round_mode HalfAwayFromZero>, ginfo = #tpu.lg<out_addr = 4096, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 9, stage = 1, slice_idx = 0, group_type = 0>, is_adaptive = false, keepdims = true, kernel_shape = [5, 5], pad_value = 0 : i64, pads = [2, 2, 2, 2], pool_mode = #tpu<pool_mode Max>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfAwayFromZero>, strides = [1, 1]} : (tensor<1x256x7x10x!quant.uniform<i8:f32, 0.18272077007874016>>) -> tensor<1x256x7x10x!quant.uniform<i8:f32, 0.18272077007874016>> loc(#loc280)
        %405 = "tpu.Store"(%404, %0) {ginfo = #tpu.lg<out_addr = 4096, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 10, stage = 1, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x256x7x10x!quant.uniform<i8:f32, 0.18272077007874016>>, none) -> tensor<1x256x7x10x!quant.uniform<i8:f32, 0.18272077007874016>, 17920 : i64> loc(#loc280)
        %406 = "tpu.Pool2D"(%404) {count_include_pad = false, do_relu = false, first_round_mode = #tpu<round_mode HalfAwayFromZero>, ginfo = #tpu.lg<out_addr = 0, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 11, stage = 1, slice_idx = 0, group_type = 0>, is_adaptive = false, keepdims = true, kernel_shape = [5, 5], pad_value = 0 : i64, pads = [2, 2, 2, 2], pool_mode = #tpu<pool_mode Max>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfAwayFromZero>, strides = [1, 1]} : (tensor<1x256x7x10x!quant.uniform<i8:f32, 0.18272077007874016>>) -> tensor<1x256x7x10x!quant.uniform<i8:f32, 0.18272077007874016>> loc(#loc281)
        %407 = "tpu.Store"(%406, %0) {ginfo = #tpu.lg<out_addr = 0, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 12, stage = 1, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x256x7x10x!quant.uniform<i8:f32, 0.18272077007874016>>, none) -> tensor<1x256x7x10x!quant.uniform<i8:f32, 0.18272077007874016>, 35840 : i64> loc(#loc281)
        %408 = "tpu.Pool2D"(%406) {count_include_pad = false, do_relu = false, first_round_mode = #tpu<round_mode HalfAwayFromZero>, ginfo = #tpu.lg<out_addr = 21504, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 13, stage = 1, slice_idx = 0, group_type = 0>, is_adaptive = false, keepdims = true, kernel_shape = [5, 5], pad_value = 0 : i64, pads = [2, 2, 2, 2], pool_mode = #tpu<pool_mode Max>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfAwayFromZero>, strides = [1, 1]} : (tensor<1x256x7x10x!quant.uniform<i8:f32, 0.18272077007874016>>) -> tensor<1x256x7x10x!quant.uniform<i8:f32, 0.18272077007874016>> loc(#loc282)
        %409 = "tpu.Store"(%408, %0) {ginfo = #tpu.lg<out_addr = 21504, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 14, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x256x7x10x!quant.uniform<i8:f32, 0.18272077007874016>>, none) -> tensor<1x256x7x10x!quant.uniform<i8:f32, 0.18272077007874016>, 53760 : i64> loc(#loc282)
        "tpu.Yield"(%403, %405, %407, %409) : (tensor<1x256x7x10x!quant.uniform<i8:f32, 0.18272077007874016>, 0 : i64>, tensor<1x256x7x10x!quant.uniform<i8:f32, 0.18272077007874016>, 17920 : i64>, tensor<1x256x7x10x!quant.uniform<i8:f32, 0.18272077007874016>, 35840 : i64>, tensor<1x256x7x10x!quant.uniform<i8:f32, 0.18272077007874016>, 53760 : i64>) -> () loc(#loc814)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 4, 2, 3, -2, 6, 5, 14, -3, 7, -4, 9, 8, -5, 11, 10, -6, 13, 12, 0, 1], group_type = 0 : i64, hsecs = 1 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x512x7x10x!quant.uniform<i8:f32, 0.17194342440944882>, 71680 : i64>) -> (tensor<1x256x7x10x!quant.uniform<i8:f32, 0.18272077007874016>, 0 : i64>, tensor<1x256x7x10x!quant.uniform<i8:f32, 0.18272077007874016>, 17920 : i64>, tensor<1x256x7x10x!quant.uniform<i8:f32, 0.18272077007874016>, 35840 : i64>, tensor<1x256x7x10x!quant.uniform<i8:f32, 0.18272077007874016>, 53760 : i64>) loc(#loc814)
      %129 = "tpu.Concat"(%128#0, %128#1, %128#2, %128#3) {axis = 1 : si32, do_relu = false, multipliers = [1, 1, 1, 1], only_merge = true, relu_limit = -1.000000e+00 : f64, rshifts = [0, 0, 0, 0]} : (tensor<1x256x7x10x!quant.uniform<i8:f32, 0.18272077007874016>, 0 : i64>, tensor<1x256x7x10x!quant.uniform<i8:f32, 0.18272077007874016>, 17920 : i64>, tensor<1x256x7x10x!quant.uniform<i8:f32, 0.18272077007874016>, 35840 : i64>, tensor<1x256x7x10x!quant.uniform<i8:f32, 0.18272077007874016>, 53760 : i64>) -> tensor<1x1024x7x10x!quant.uniform<i8:f32, 0.18272077007874016>, 0 : i64> loc(#loc290)
      %130 = "top.Weight"() {do_compress = true} : () -> tensor<1x512x1x9xi8, 1099513243952 : i64> loc(#loc291)
      %131 = "top.Weight"() {do_compress = true} : () -> tensor<1x512x1x1024xsi8, 1099518433296 : i64> loc(#loc292)
      %132 = "tpu.Conv2D"(%129, %131, %130) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x1024x7x10x!quant.uniform<i8:f32, 0.18272077007874016>, 0 : i64>, tensor<1x512x1x1024xsi8, 1099518433296 : i64>, tensor<1x512x1x9xi8, 1099513243952 : i64>) -> tensor<1x512x7x10x!quant.uniform<i8:f32, 0.10026667401574803>, 71680 : i64> loc(#loc293)
      %133 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099512120464 : i64> loc(#loc294)
      %134 = "tpu.Lut"(%132, %133) : (tensor<1x512x7x10x!quant.uniform<i8:f32, 0.10026667401574803>, 71680 : i64>, tensor<1x1x1x256xsi8, 1099512120464 : i64>) -> tensor<1x512x7x10x!quant.uniform<i8:f32, 0.033823980314960631>, 0 : i64> loc(#loc295)
      %135 = "top.Weight"() {do_compress = true} : () -> tensor<1x512x1x9xi8, 1099512115856 : i64> loc(#loc296)
      %136 = "top.Weight"() {do_compress = true} : () -> tensor<1x512x1x512xsi8, 1099519849552 : i64> loc(#loc297)
      %137 = "tpu.Conv2D"(%134, %136, %135) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x512x7x10x!quant.uniform<i8:f32, 0.033823980314960631>, 0 : i64>, tensor<1x512x1x512xsi8, 1099519849552 : i64>, tensor<1x512x1x9xi8, 1099512115856 : i64>) -> tensor<1x512x7x10x!quant.uniform<i8:f32, 0.17076360157480314>, 35840 : i64> loc(#loc298)
      %138 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099512109968 : i64> loc(#loc299)
      %139 = "tpu.Lut"(%137, %138) : (tensor<1x512x7x10x!quant.uniform<i8:f32, 0.17076360157480314>, 35840 : i64>, tensor<1x1x1x256xsi8, 1099512109968 : i64>) -> tensor<1x512x7x10x!quant.uniform<i8:f32, 0.068039255905511811>, 80640 : i64> loc(#loc300)
      %140 = "tpu.Slice"(%139, %0, %0, %0, %0) {axes = [], ends = [9223372036854775807, 256, 9223372036854775807, 9223372036854775807], hasparamConvert_axes = [1], offset = [0, 0, 0, 0], steps = [1, 1, 1, 1]} : (tensor<1x512x7x10x!quant.uniform<i8:f32, 0.068039255905511811>, 80640 : i64>, none, none, none, none) -> tensor<1x256x7x10x!quant.uniform<i8:f32, 0.068039255905511811>, 80640 : i64> loc(#loc301)
      %141 = "tpu.Slice"(%139, %0, %0, %0, %0) {axes = [], ends = [9223372036854775807, 512, 9223372036854775807, 9223372036854775807], hasparamConvert_axes = [1], offset = [0, 256, 0, 0], steps = [1, 1, 1, 1]} : (tensor<1x512x7x10x!quant.uniform<i8:f32, 0.068039255905511811>, 80640 : i64>, none, none, none, none) -> tensor<1x256x7x10x!quant.uniform<i8:f32, 0.068039255905511811>, 98560 : i64> loc(#loc302)
      %142 = "top.Weight"() {do_compress = true} : () -> tensor<1x512x1x9xi8, 1099513317216 : i64> loc(#loc303)
      %143 = "top.Weight"() {do_compress = true} : () -> tensor<1x512x1x256xsi8, 1099511974416 : i64> loc(#loc304)
      %144 = "tpu.Conv2D"(%141, %143, %142) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x256x7x10x!quant.uniform<i8:f32, 0.068039255905511811>, 98560 : i64>, tensor<1x512x1x256xsi8, 1099511974416 : i64>, tensor<1x512x1x9xi8, 1099513317216 : i64>) -> tensor<1x512x7x10x!quant.uniform<i8:f32, 0.10691841732283465>, 0 : i64> loc(#loc305)
      %145 = "tpu.Reshape"(%144) {flatten_start_dim = -1 : i64, shape = [1, 4, 128, 70]} : (tensor<1x512x7x10x!quant.uniform<i8:f32, 0.10691841732283465>, 0 : i64>) -> tensor<1x4x128x70x!quant.uniform<i8:f32, 0.10691841732283465>, 0 : i64> loc(#loc306)
      %146 = "tpu.Slice"(%145, %0, %0, %0, %0) {axes = [], ends = [9223372036854775807, 9223372036854775807, 32, 9223372036854775807], hasparamConvert_axes = [2], offset = [0, 0, 0, 0], steps = [1, 1, 1, 1]} : (tensor<1x4x128x70x!quant.uniform<i8:f32, 0.10691841732283465>, 0 : i64>, none, none, none, none) -> tensor<1x4x32x70x!quant.uniform<i8:f32, 0.10691841732283465>, 35840 : i64> loc(#loc307)
      %147 = "tpu.Slice"(%145, %0, %0, %0, %0) {axes = [], ends = [9223372036854775807, 9223372036854775807, 64, 9223372036854775807], hasparamConvert_axes = [2], offset = [0, 0, 32, 0], steps = [1, 1, 1, 1]} : (tensor<1x4x128x70x!quant.uniform<i8:f32, 0.10691841732283465>, 0 : i64>, none, none, none, none) -> tensor<1x4x32x70x!quant.uniform<i8:f32, 0.10691841732283465>, 116480 : i64> loc(#loc308)
      %148 = "tpu.Cast"(%147) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x4x32x70x!quant.uniform<i8:f32, 0.10691841732283465>, 116480 : i64>) -> tensor<1x4x32x70xbf16, 53760 : i64> loc(#loc309)
      %149 = "tpu.Slice"(%145, %0, %0, %0, %0) {axes = [], ends = [9223372036854775807, 9223372036854775807, 128, 9223372036854775807], hasparamConvert_axes = [2], offset = [0, 0, 64, 0], steps = [1, 1, 1, 1]} : (tensor<1x4x128x70x!quant.uniform<i8:f32, 0.10691841732283465>, 0 : i64>, none, none, none, none) -> tensor<1x4x64x70x!quant.uniform<i8:f32, 0.10691841732283465>, 116480 : i64> loc(#loc310)
      %150 = "tpu.Cast"(%149) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x4x64x70x!quant.uniform<i8:f32, 0.10691841732283465>, 116480 : i64>) -> tensor<1x4x64x70xbf16, 0 : i64> loc(#loc311)
      %151 = "tpu.Permute"(%146, %0) {order = [0, 1, 3, 2]} : (tensor<1x4x32x70x!quant.uniform<i8:f32, 0.10691841732283465>, 35840 : i64>, none) -> tensor<1x4x70x32x!quant.uniform<i8:f32, 0.10691841732283465>, 134400 : i64> loc(#loc312)
      %152 = "tpu.Cast"(%151) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x4x70x32x!quant.uniform<i8:f32, 0.10691841732283465>, 134400 : i64>) -> tensor<1x4x70x32xbf16, 35840 : i64> loc(#loc313)
      %153 = "tpu.Reshape"(%149) {flatten_start_dim = -1 : i64, shape = [1, 256, 7, 10]} : (tensor<1x4x64x70x!quant.uniform<i8:f32, 0.10691841732283465>, 116480 : i64>) -> tensor<1x256x7x10x!quant.uniform<i8:f32, 0.10691841732283465>, 116480 : i64> loc(#loc314)
      %154 = "tpu.MatMul"(%152, %148, %0, %0, %0) {do_relu = false, fuse_rq = false, hdim_is_batch = false, input_zp = 0 : i64, keep_dims = true, left_reuse = 1 : i64, left_transpose = false, multipliers = [1], output_transpose = false, quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, right_transpose = false, right_zp = 0 : i64, round_mode = #tpu<round_mode HalfAwayFromZero>, rshifts = [0]} : (tensor<1x4x70x32xbf16, 35840 : i64>, tensor<1x4x32x70xbf16, 53760 : i64>, none, none, none) -> tensor<1x4x70x70x!quant.calibrated<bf16<-642.17901610000001:642.17901610000001>>, 134400 : i64> loc(#loc315)
      %155 = "tpu.Cast"(%154) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x4x70x70x!quant.calibrated<bf16<-642.17901610000001:642.17901610000001>>, 134400 : i64>) -> tensor<1x4x70x70x!quant.uniform<i8:bf16, 5.0565276858267714>, 35840 : i64> loc(#loc316)
      %156 = "top.Weight"() {do_compress = true} : () -> tensor<1x256x1x9xi8, 1099517461232 : i64> loc(#loc317)
      %157 = "top.Weight"() {do_compress = true} : () -> tensor<1x256x3x3xsi8, 1099515086128 : i64> loc(#loc318)
      %158 = "tpu.Conv2D"(%153, %157, %156) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, group = 256 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x256x7x10x!quant.uniform<i8:f32, 0.10691841732283465>, 116480 : i64>, tensor<1x256x3x3xsi8, 1099515086128 : i64>, tensor<1x256x1x9xi8, 1099517461232 : i64>) -> tensor<1x256x7x10x!quant.uniform<i8:f32, 0.083028171653543315>, 134400 : i64> loc(#loc319)
      %159 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099517466096 : i64> loc(#loc320)
      %160 = "tpu.Lut"(%155, %159) : (tensor<1x4x70x70x!quant.uniform<i8:bf16, 5.0565276858267714>, 35840 : i64>, tensor<1x1x1x256xsi8, 1099517466096 : i64>) -> tensor<1x4x70x70x!quant.uniform<i8:f32, 0.89387623867836719>, 55440 : i64> loc(#loc321)
      %161 = "tpu.Cast"(%160) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x4x70x70x!quant.uniform<i8:f32, 0.89387623867836719>, 55440 : i64>) -> tensor<1x4x70x70xbf16, 152320 : i64> loc(#loc322)
      %162 = "top.Weight"() : () -> tensor<1x1x32x8xbf16, 1099515742256 : i64> loc(#loc323)
      %163 = "top.Weight"() : () -> tensor<1x1x32x8xbf16, 1099513248560 : i64> loc(#loc324)
      %164 = "top.Weight"() : () -> tensor<1x1x32x8xbf16, 1099512105744 : i64> loc(#loc325)
      %165 = "top.Weight"() : () -> tensor<1x1x32x8xbf16, 1099512272784 : i64> loc(#loc326)
      %166 = "tpu.Softmax"(%161, %162, %163, %164, %165, %0) {axis = 3 : si32, beta = 1.000000e+00 : f64, log = false, round_mode = #tpu<round_mode HalfAwayFromZero>} : (tensor<1x4x70x70xbf16, 152320 : i64>, tensor<1x1x32x8xbf16, 1099515742256 : i64>, tensor<1x1x32x8xbf16, 1099513248560 : i64>, tensor<1x1x32x8xbf16, 1099512105744 : i64>, tensor<1x1x32x8xbf16, 1099512272784 : i64>, none) -> tensor<1x4x70x70x!quant.calibrated<bf16<-1.000000e+00:1.000000e+00>>, 35840 : i64> loc(#loc327)
      %167 = "tpu.Cast"(%166) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x4x70x70x!quant.calibrated<bf16<-1.000000e+00:1.000000e+00>>, 35840 : i64>) -> tensor<1x4x70x70x!quant.uniform<i8:bf16, 0.0078740157480314959>, 152320 : i64> loc(#loc328)
      %168 = "tpu.Permute"(%167, %0) {order = [0, 1, 3, 2]} : (tensor<1x4x70x70x!quant.uniform<i8:bf16, 0.0078740157480314959>, 152320 : i64>, none) -> tensor<1x4x70x70x!quant.uniform<i8:f32, 0.0078740157480314959>, 35840 : i64> loc(#loc329)
      %169 = "tpu.Cast"(%168) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x4x70x70x!quant.uniform<i8:f32, 0.0078740157480314959>, 35840 : i64>) -> tensor<1x4x70x70xbf16, 152320 : i64> loc(#loc330)
      %170 = "tpu.MatMul"(%150, %169, %0, %0, %0) {do_relu = false, fuse_rq = false, hdim_is_batch = false, input_zp = 0 : i64, keep_dims = true, left_reuse = 1 : i64, left_transpose = false, multipliers = [1], output_transpose = false, quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, right_transpose = false, right_zp = 0 : i64, round_mode = #tpu<round_mode HalfAwayFromZero>, rshifts = [0]} : (tensor<1x4x64x70xbf16, 0 : i64>, tensor<1x4x70x70xbf16, 152320 : i64>, none, none, none) -> tensor<1x4x64x70x!quant.calibrated<bf16<-13.307008700000001:13.307008700000001>>, 35840 : i64> loc(#loc331)
      %171 = "tpu.Reshape"(%170) {flatten_start_dim = -1 : i64, shape = [1, 256, 7, 10]} : (tensor<1x4x64x70x!quant.calibrated<bf16<-13.307008700000001:13.307008700000001>>, 35840 : i64>) -> tensor<1x256x7x10x!quant.calibrated<bf16<-13.307008700000001:13.307008700000001>>, 35840 : i64> loc(#loc332)
      %172 = "tpu.Cast"(%171) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x256x7x10x!quant.calibrated<bf16<-13.307008700000001:13.307008700000001>>, 35840 : i64>) -> tensor<1x256x7x10x!quant.uniform<i8:bf16, 0.10477959606299213>, 152320 : i64> loc(#loc333)
      %173 = "top.Weight"() : () -> tensor<1x256x1x9xi8, 1099514341424 : i64> loc(#loc334)
      %174 = "top.Weight"() : () -> tensor<1x256x1x256xsi8, 1099518347040 : i64> loc(#loc335)
      %175 = "tpu.Group"(%172, %158, %141) ({
        %395 = "tpu.Load"(%172) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x7x10x!quant.uniform<i8:bf16, 0.10477959606299213>, 152320 : i64>) -> tensor<1x256x7x10x!quant.uniform<i8:bf16, 0.10477959606299213>> loc(#loc337)
        %396 = "tpu.Load"(%158) {do_bcast = false, ginfo = #tpu.lg<out_addr = 28672, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 1, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x7x10x!quant.uniform<i8:f32, 0.083028171653543315>, 134400 : i64>) -> tensor<1x256x7x10x!quant.uniform<i8:f32, 0.083028171653543315>> loc(#loc338)
        %397 = "tpu.Load"(%174) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 8192, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 2, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x1x256xsi8, 1099518347040 : i64>) -> tensor<1x256x1x256xsi8> loc(#loc339)
        %398 = "tpu.Load"(%173) {do_bcast = false, ginfo = #tpu.lg<out_addr = 14848, out_size = 288, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 3, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x1x9xi8, 1099514341424 : i64>) -> tensor<1x256x1x9xi8> loc(#loc340)
        %399 = "tpu.Add"(%395, %396) {do_relu = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 2560, buffer_addr = 10752, buffer_size = 2560, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 4, stage = 1, slice_idx = 0, group_type = 0>, multipliers = [69, 55], relu_limit = -1.000000e+00 : f64, rshifts = [6]} : (tensor<1x256x7x10x!quant.uniform<i8:bf16, 0.10477959606299213>>, tensor<1x256x7x10x!quant.uniform<i8:f32, 0.083028171653543315>>) -> tensor<1x256x7x10x!quant.uniform<i8:f32, 0.096369322834645665>> loc(#loc341)
        %400 = "tpu.Load"(%141) {do_bcast = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 5, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x7x10x!quant.uniform<i8:f32, 0.068039255905511811>, 98560 : i64>) -> tensor<1x256x7x10x!quant.uniform<i8:f32, 0.068039255905511811>> loc(#loc342)
        %401 = "tpu.Conv2D"(%399, %397, %398) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 20480, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 6, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x256x7x10x!quant.uniform<i8:f32, 0.096369322834645665>>, tensor<1x256x1x256xsi8>, tensor<1x256x1x9xi8>) -> tensor<1x256x7x10x!quant.uniform<i8:f32, 0.078137698425196855>> loc(#loc343)
        %402 = "tpu.Add"(%400, %401) {do_relu = false, ginfo = #tpu.lg<out_addr = 8192, out_size = 2560, buffer_addr = 0, buffer_size = 2560, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 7, stage = 1, slice_idx = 0, group_type = 0>, multipliers = [64, 73], relu_limit = -1.000000e+00 : f64, rshifts = [7]} : (tensor<1x256x7x10x!quant.uniform<i8:f32, 0.068039255905511811>>, tensor<1x256x7x10x!quant.uniform<i8:f32, 0.078137698425196855>>) -> tensor<1x256x7x10x!quant.uniform<i8:f32, 0.13549065748031497>> loc(#loc336)
        %403 = "tpu.Store"(%402, %0) {ginfo = #tpu.lg<out_addr = 8192, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 8, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x256x7x10x!quant.uniform<i8:f32, 0.13549065748031497>>, none) -> tensor<1x256x7x10x!quant.uniform<i8:f32, 0.13549065748031497>, 0 : i64> loc(#loc336)
        "tpu.Yield"(%403) : (tensor<1x256x7x10x!quant.uniform<i8:f32, 0.13549065748031497>, 0 : i64>) -> () loc(#loc336)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 4, 2, 3, -2, 6, 5, 8, -3, 7, 0, 1], group_type = 0 : i64, hsecs = 1 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x256x7x10x!quant.uniform<i8:bf16, 0.10477959606299213>, 152320 : i64>, tensor<1x256x7x10x!quant.uniform<i8:f32, 0.083028171653543315>, 134400 : i64>, tensor<1x256x7x10x!quant.uniform<i8:f32, 0.068039255905511811>, 98560 : i64>) -> tensor<1x256x7x10x!quant.uniform<i8:f32, 0.13549065748031497>, 0 : i64> loc(#loc336)
      %176 = "top.Weight"() {do_compress = true} : () -> tensor<1x512x1x9xi8, 1099514072944 : i64> loc(#loc344)
      %177 = "top.Weight"() {do_compress = true} : () -> tensor<1x512x1x256xsi8, 1099513587968 : i64> loc(#loc345)
      %178 = "tpu.Conv2D"(%175, %177, %176) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x256x7x10x!quant.uniform<i8:f32, 0.13549065748031497>, 0 : i64>, tensor<1x512x1x256xsi8, 1099513587968 : i64>, tensor<1x512x1x9xi8, 1099514072944 : i64>) -> tensor<1x512x7x10x!quant.uniform<i8:f32, 0.079393296850393702>, 116480 : i64> loc(#loc346)
      %179 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099512115600 : i64> loc(#loc347)
      %180 = "top.Weight"() : () -> tensor<1x256x1x9xi8, 1099511972112 : i64> loc(#loc348)
      %181 = "top.Weight"() : () -> tensor<1x256x1x512xsi8, 1099513882752 : i64> loc(#loc349)
      %182 = "tpu.Group"(%178) ({
        %395 = "tpu.Load"(%178) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 5120, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [512], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x512x7x10x!quant.uniform<i8:f32, 0.079393296850393702>, 116480 : i64>) -> tensor<1x512x7x10x!quant.uniform<i8:f32, 0.079393296850393702>> loc(#loc351)
        %396 = "tpu.Load"(%179) {do_bcast = true, ginfo = #tpu.lg<out_addr = 29696, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 1, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099512115600 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc352)
        %397 = "tpu.Load"(%181) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 16384, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [512], id = 2, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x1x512xsi8, 1099513882752 : i64>) -> tensor<1x256x1x512xsi8> loc(#loc353)
        %398 = "tpu.Load"(%180) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24064, out_size = 288, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 3, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x1x9xi8, 1099511972112 : i64>) -> tensor<1x256x1x9xi8> loc(#loc354)
        %399 = "tpu.Lut"(%395, %396) {ginfo = #tpu.lg<out_addr = 16384, out_size = 5120, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [512], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 4, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x512x7x10x!quant.uniform<i8:f32, 0.079393296850393702>>, tensor<1x1x1x256xsi8>) -> tensor<1x512x7x10x!quant.uniform<i8:f32, 0.045423841732283463>> loc(#loc355)
        %400 = "tpu.Conv2D"(%399, %397, %398) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 21504, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 5, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x512x7x10x!quant.uniform<i8:f32, 0.045423841732283463>>, tensor<1x256x1x512xsi8>, tensor<1x256x1x9xi8>) -> tensor<1x256x7x10x!quant.uniform<i8:f32, 0.073047495275590554>> loc(#loc350)
        %401 = "tpu.Store"(%400, %0) {ginfo = #tpu.lg<out_addr = 21504, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 6, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x256x7x10x!quant.uniform<i8:f32, 0.073047495275590554>>, none) -> tensor<1x256x7x10x!quant.uniform<i8:f32, 0.073047495275590554>, 17920 : i64> loc(#loc350)
        "tpu.Yield"(%401) : (tensor<1x256x7x10x!quant.uniform<i8:f32, 0.073047495275590554>, 17920 : i64>) -> () loc(#loc350)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 4, 2, 3, 6, -2, 5, 0, 1], group_type = 0 : i64, hsecs = 1 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x512x7x10x!quant.uniform<i8:f32, 0.079393296850393702>, 116480 : i64>) -> tensor<1x256x7x10x!quant.uniform<i8:f32, 0.073047495275590554>, 17920 : i64> loc(#loc350)
      %183 = "tpu.Group"(%175, %182, %140) ({
        %395 = "tpu.Load"(%175) {do_bcast = false, ginfo = #tpu.lg<out_addr = 20480, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x7x10x!quant.uniform<i8:f32, 0.13549065748031497>, 0 : i64>) -> tensor<1x256x7x10x!quant.uniform<i8:f32, 0.13549065748031497>> loc(#loc357)
        %396 = "tpu.Load"(%182) {do_bcast = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 1, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x7x10x!quant.uniform<i8:f32, 0.073047495275590554>, 17920 : i64>) -> tensor<1x256x7x10x!quant.uniform<i8:f32, 0.073047495275590554>> loc(#loc358)
        %397 = "tpu.Load"(%140) {do_bcast = false, ginfo = #tpu.lg<out_addr = 8192, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 2, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x7x10x!quant.uniform<i8:f32, 0.068039255905511811>, 80640 : i64>) -> tensor<1x256x7x10x!quant.uniform<i8:f32, 0.068039255905511811>> loc(#loc359)
        %398 = "tpu.Add"(%395, %396) {do_relu = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 2560, buffer_addr = 24576, buffer_size = 2560, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 3, stage = 1, slice_idx = 0, group_type = 0>, multipliers = [89, 48], relu_limit = -1.000000e+00 : f64, rshifts = [6]} : (tensor<1x256x7x10x!quant.uniform<i8:f32, 0.13549065748031497>>, tensor<1x256x7x10x!quant.uniform<i8:f32, 0.073047495275590554>>) -> tensor<1x256x7x10x!quant.uniform<i8:f32, 0.097358230708661411>> loc(#loc360)
        %399 = "tpu.Concat"(%397, %398) {axis = 1 : si32, do_relu = false, ginfo = #tpu.lg<out_addr = 0, out_size = 5120, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [512], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 4, stage = 1, slice_idx = 0, group_type = 0>, multipliers = [89, 1], only_merge = false, relu_limit = -1.000000e+00 : f64, rshifts = [7, 0]} : (tensor<1x256x7x10x!quant.uniform<i8:f32, 0.068039255905511811>>, tensor<1x256x7x10x!quant.uniform<i8:f32, 0.097358230708661411>>) -> tensor<1x512x7x10x!quant.uniform<i8:f32, 0.097358230708661411>> loc(#loc356)
        %400 = "tpu.Store"(%399, %0) {ginfo = #tpu.lg<out_addr = 0, out_size = 5120, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [512], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 5, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x512x7x10x!quant.uniform<i8:f32, 0.097358230708661411>>, none) -> tensor<1x512x7x10x!quant.uniform<i8:f32, 0.097358230708661411>, 116480 : i64> loc(#loc356)
        "tpu.Yield"(%400) : (tensor<1x512x7x10x!quant.uniform<i8:f32, 0.097358230708661411>, 116480 : i64>) -> () loc(#loc356)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 3, 2, 5, -2, 4, 0, 1], group_type = 0 : i64, hsecs = 1 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x256x7x10x!quant.uniform<i8:f32, 0.13549065748031497>, 0 : i64>, tensor<1x256x7x10x!quant.uniform<i8:f32, 0.073047495275590554>, 17920 : i64>, tensor<1x256x7x10x!quant.uniform<i8:f32, 0.068039255905511811>, 80640 : i64>) -> tensor<1x512x7x10x!quant.uniform<i8:f32, 0.097358230708661411>, 116480 : i64> loc(#loc356)
      %184 = "top.Weight"() {do_compress = true} : () -> tensor<1x512x1x9xi8, 1099512120720 : i64> loc(#loc361)
      %185 = "top.Weight"() {do_compress = true} : () -> tensor<1x512x1x512xsi8, 1099514077552 : i64> loc(#loc362)
      %186 = "tpu.Conv2D"(%183, %185, %184) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x512x7x10x!quant.uniform<i8:f32, 0.097358230708661411>, 116480 : i64>, tensor<1x512x1x512xsi8, 1099514077552 : i64>, tensor<1x512x1x9xi8, 1099512120720 : i64>) -> tensor<1x512x7x10x!quant.uniform<i8:f32, 0.095413177952755904>, 0 : i64> loc(#loc363)
      %187 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099514035824 : i64> loc(#loc364)
      %188 = "tpu.Lut"(%186, %187) : (tensor<1x512x7x10x!quant.uniform<i8:f32, 0.095413177952755904>, 0 : i64>, tensor<1x1x1x256xsi8, 1099514035824 : i64>) -> tensor<1x512x7x10x!quant.uniform<i8:f32, 0.02761857559055118>, 179200 : i64> loc(#loc365)
      %189 = "top.Weight"() : () -> tensor<1x512x2x2xsi8, 1099513719040 : i64> loc(#loc366)
      %190 = "top.Weight"() : () -> tensor<1x512x1x5xi8, 1099512106256 : i64> loc(#loc367)
      %191 = "tpu.Group"(%188, %82) ({
        %395 = "tpu.Load"(%188) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 2048, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [512], d_idx = [0], d_slice = [1], h_idx = [0, 2, 5], h_slice = [3, 3, 2], w_idx = [0], w_slice = [10], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x512x7x10x!quant.uniform<i8:f32, 0.02761857559055118>, 179200 : i64>) -> tensor<1x512x7x10x!quant.uniform<i8:f32, 0.02761857559055118>> loc(#loc369)
        %396 = "tpu.Load"(%189) {do_bcast = false, ginfo = #tpu.lg<out_addr = 28672, out_size = 1024, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [512], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [2], w_idx = [0], w_slice = [2], id = 1, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x512x2x2xsi8, 1099513719040 : i64>) -> tensor<1x512x2x2xsi8> loc(#loc370)
        %397 = "tpu.Load"(%190) {do_bcast = false, ginfo = #tpu.lg<out_addr = 10752, out_size = 320, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [512], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [5], id = 2, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x512x1x5xi8, 1099512106256 : i64>) -> tensor<1x512x1x5xi8> loc(#loc371)
        %398 = "tpu.Load"(%82) {do_bcast = false, ginfo = #tpu.lg<out_addr = 20480, out_size = 3584, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0, 5, 10], h_slice = [5, 5, 4], w_idx = [0], w_slice = [20], id = 3, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x14x20x!quant.uniform<i8:f32, 0.089566330708661421>, 430080 : i64>) -> tensor<1x256x14x20x!quant.uniform<i8:f32, 0.089566330708661421>> loc(#loc372)
        %399 = "tpu.Deconv"(%395, %396, %397) {dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 7168, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [512], d_idx = [0], d_slice = [1], h_idx = [0, 5, 10], h_slice = [5, 5, 4], w_idx = [0], w_slice = [20], id = 4, stage = 1, slice_idx = 0, group_type = 0>, group = 512 : i64, inserts = [0, 0], kernel_shape = [2, 2], pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, strides = [2, 2], with_bias = false} : (tensor<1x512x7x10x!quant.uniform<i8:f32, 0.02761857559055118>>, tensor<1x512x2x2xsi8>, tensor<1x512x1x5xi8>) -> tensor<1x512x14x20x!quant.uniform<i8:f32, 0.055347491338582674>> loc(#loc373)
        %400 = "tpu.Concat"(%399, %398) {axis = 1 : si32, do_relu = false, ginfo = #tpu.lg<out_addr = 0, out_size = 10752, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [768], d_idx = [0], d_slice = [1], h_idx = [0, 5, 10], h_slice = [5, 5, 4], w_idx = [0], w_slice = [20], id = 5, stage = 1, slice_idx = 0, group_type = 0>, multipliers = [79, 1], only_merge = false, relu_limit = -1.000000e+00 : f64, rshifts = [7, 0]} : (tensor<1x512x14x20x!quant.uniform<i8:f32, 0.055347491338582674>>, tensor<1x256x14x20x!quant.uniform<i8:f32, 0.089566330708661421>>) -> tensor<1x768x14x20x!quant.uniform<i8:f32, 0.089566330708661421>> loc(#loc368)
        %401 = "tpu.Store"(%400, %0) {ginfo = #tpu.lg<out_addr = 0, out_size = 10752, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [768], d_idx = [0], d_slice = [1], h_idx = [0, 5, 10], h_slice = [5, 5, 4], w_idx = [0], w_slice = [20], id = 6, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x768x14x20x!quant.uniform<i8:f32, 0.089566330708661421>>, none) -> tensor<1x768x14x20x!quant.uniform<i8:f32, 0.089566330708661421>, 215040 : i64> loc(#loc368)
        "tpu.Yield"(%401) : (tensor<1x768x14x20x!quant.uniform<i8:f32, 0.089566330708661421>, 215040 : i64>) -> () loc(#loc368)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 4, 3, 6, -2, 5, 0, 1, 2], group_type = 0 : i64, hsecs = 3 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x512x7x10x!quant.uniform<i8:f32, 0.02761857559055118>, 179200 : i64>, tensor<1x256x14x20x!quant.uniform<i8:f32, 0.089566330708661421>, 430080 : i64>) -> tensor<1x768x14x20x!quant.uniform<i8:f32, 0.089566330708661421>, 215040 : i64> loc(#loc368)
      %192 = "top.Weight"() {do_compress = true} : () -> tensor<1x256x1x9xi8, 1099514020720 : i64> loc(#loc374)
      %193 = "top.Weight"() {do_compress = true} : () -> tensor<1x256x1x768xsi8, 1099511628048 : i64> loc(#loc375)
      %194 = "tpu.Conv2D"(%191, %193, %192) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x768x14x20x!quant.uniform<i8:f32, 0.089566330708661421>, 215040 : i64>, tensor<1x256x1x768xsi8, 1099511628048 : i64>, tensor<1x256x1x9xi8, 1099514020720 : i64>) -> tensor<1x256x14x20x!quant.uniform<i8:f32, 0.10067717795275591>, 430080 : i64> loc(#loc376)
      %195 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099515685104 : i64> loc(#loc377)
      %196 = "tpu.Lut"(%194, %195) : (tensor<1x256x14x20x!quant.uniform<i8:f32, 0.10067717795275591>, 430080 : i64>, tensor<1x1x1x256xsi8, 1099515685104 : i64>) -> tensor<1x256x14x20x!quant.uniform<i8:f32, 0.088586537795275588>, 215040 : i64> loc(#loc378)
      %197 = "tpu.Slice"(%196, %0, %0, %0, %0) {axes = [], ends = [9223372036854775807, 128, 9223372036854775807, 9223372036854775807], hasparamConvert_axes = [1], offset = [0, 0, 0, 0], steps = [1, 1, 1, 1]} : (tensor<1x256x14x20x!quant.uniform<i8:f32, 0.088586537795275588>, 215040 : i64>, none, none, none, none) -> tensor<1x128x14x20x!quant.uniform<i8:f32, 0.088586537795275588>, 215040 : i64> loc(#loc379)
      %198 = "tpu.Slice"(%196, %0, %0, %0, %0) {axes = [], ends = [9223372036854775807, 256, 9223372036854775807, 9223372036854775807], hasparamConvert_axes = [1], offset = [0, 128, 0, 0], steps = [1, 1, 1, 1]} : (tensor<1x256x14x20x!quant.uniform<i8:f32, 0.088586537795275588>, 215040 : i64>, none, none, none, none) -> tensor<1x128x14x20x!quant.uniform<i8:f32, 0.088586537795275588>, 250880 : i64> loc(#loc380)
      %199 = "top.Weight"() {do_compress = true} : () -> tensor<1x64x1x9xi8, 1099515089264 : i64> loc(#loc381)
      %200 = "top.Weight"() {do_compress = true} : () -> tensor<1x64x9x128xsi8, 1099515960624 : i64> loc(#loc382)
      %201 = "tpu.Conv2D"(%198, %200, %199) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.088586537795275588>, 250880 : i64>, tensor<1x64x9x128xsi8, 1099515960624 : i64>, tensor<1x64x1x9xi8, 1099515089264 : i64>) -> tensor<1x64x14x20x!quant.uniform<i8:f32, 0.082954617322834656>, 286720 : i64> loc(#loc383)
      %202 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099516034864 : i64> loc(#loc384)
      %203 = "top.Weight"() : () -> tensor<1x128x1x9xi8, 1099517493232 : i64> loc(#loc385)
      %204 = "top.Weight"() : () -> tensor<1x128x9x64xsi8, 1099516035120 : i64> loc(#loc386)
      %205 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099516108848 : i64> loc(#loc387)
      %206 = "tpu.Group"(%201, %198, %197) ({
        %395 = "tpu.Load"(%201) {do_bcast = false, ginfo = #tpu.lg<out_addr = 28672, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 6], h_slice = [8, 8], w_idx = [0], w_slice = [20], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.082954617322834656>, 286720 : i64>) -> tensor<1x64x14x20x!quant.uniform<i8:f32, 0.082954617322834656>> loc(#loc389)
        %396 = "tpu.Load"(%202) {do_bcast = true, ginfo = #tpu.lg<out_addr = 11520, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 1, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099516034864 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc390)
        %397 = "tpu.Load"(%204) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 9216, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [64], id = 2, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x9x64xsi8, 1099516035120 : i64>) -> tensor<1x128x9x64xsi8> loc(#loc391)
        %398 = "tpu.Load"(%203) {do_bcast = false, ginfo = #tpu.lg<out_addr = 11776, out_size = 144, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 3, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x9xi8, 1099517493232 : i64>) -> tensor<1x128x1x9xi8> loc(#loc392)
        %399 = "tpu.Lut"(%395, %396) {ginfo = #tpu.lg<out_addr = 24576, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 6], h_slice = [8, 8], w_idx = [0], w_slice = [20], id = 4, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.082954617322834656>>, tensor<1x1x1x256xsi8>) -> tensor<1x64x14x20x!quant.uniform<i8:f32, 0.067874364566929135>> loc(#loc393)
        %400 = "tpu.Load"(%205) {do_bcast = true, ginfo = #tpu.lg<out_addr = 19200, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 5, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099516108848 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc394)
        %401 = "tpu.Conv2D"(%399, %397, %398) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 20480, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 7], h_slice = [7, 7], w_idx = [0], w_slice = [20], id = 6, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.067874364566929135>>, tensor<1x128x9x64xsi8>, tensor<1x128x1x9xi8>) -> tensor<1x128x14x20x!quant.uniform<i8:f32, 0.09488212440944882>> loc(#loc395)
        %402 = "tpu.Load"(%198) {do_bcast = false, ginfo = #tpu.lg<out_addr = 9216, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 7], h_slice = [7, 7], w_idx = [0], w_slice = [20], id = 7, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.088586537795275588>, 250880 : i64>) -> tensor<1x128x14x20x!quant.uniform<i8:f32, 0.088586537795275588>> loc(#loc396)
        %403 = "tpu.Lut"(%401, %400) {ginfo = #tpu.lg<out_addr = 12288, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 7], h_slice = [7, 7], w_idx = [0], w_slice = [20], id = 8, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.09488212440944882>>, tensor<1x1x1x256xsi8>) -> tensor<1x128x14x20x!quant.uniform<i8:f32, 0.06702242283464567>> loc(#loc397)
        %404 = "tpu.Load"(%197) {do_bcast = false, ginfo = #tpu.lg<out_addr = 20480, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 7], h_slice = [7, 7], w_idx = [0], w_slice = [20], id = 9, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.088586537795275588>, 215040 : i64>) -> tensor<1x128x14x20x!quant.uniform<i8:f32, 0.088586537795275588>> loc(#loc398)
        %405 = "tpu.Add"(%402, %403) {do_relu = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 2304, buffer_addr = 16384, buffer_size = 2304, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 7], h_slice = [7, 7], w_idx = [0], w_slice = [20], id = 10, stage = 1, slice_idx = 0, group_type = 0>, multipliers = [65, 49], relu_limit = -1.000000e+00 : f64, rshifts = [6]} : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.088586537795275588>>, tensor<1x128x14x20x!quant.uniform<i8:f32, 0.06702242283464567>>) -> tensor<1x128x14x20x!quant.uniform<i8:f32, 0.086766787401574799>> loc(#loc399)
        %406 = "tpu.Concat"(%404, %402, %405) {axis = 1 : si32, do_relu = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 6912, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [384], d_idx = [0], d_slice = [1], h_idx = [0, 7], h_slice = [7, 7], w_idx = [0], w_slice = [20], id = 11, stage = 1, slice_idx = 0, group_type = 0>, multipliers = [1, 1, 125], only_merge = false, relu_limit = -1.000000e+00 : f64, rshifts = [0, 0, 7]} : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.088586537795275588>>, tensor<1x128x14x20x!quant.uniform<i8:f32, 0.088586537795275588>>, tensor<1x128x14x20x!quant.uniform<i8:f32, 0.086766787401574799>>) -> tensor<1x384x14x20x!quant.uniform<i8:f32, 0.088586537795275588>> loc(#loc388)
        %407 = "tpu.Store"(%406, %0) {ginfo = #tpu.lg<out_addr = 12288, out_size = 6912, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [384], d_idx = [0], d_slice = [1], h_idx = [0, 7], h_slice = [7, 7], w_idx = [0], w_slice = [20], id = 12, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x384x14x20x!quant.uniform<i8:f32, 0.088586537795275588>>, none) -> tensor<1x384x14x20x!quant.uniform<i8:f32, 0.088586537795275588>, 304640 : i64> loc(#loc388)
        "tpu.Yield"(%407) : (tensor<1x384x14x20x!quant.uniform<i8:f32, 0.088586537795275588>, 304640 : i64>) -> () loc(#loc388)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 4, 2, 3, -2, 6, 5, 12, -3, 8, 7, -4, 10, 9, -5, 11, 0, 1], group_type = 0 : i64, hsecs = 2 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.082954617322834656>, 286720 : i64>, tensor<1x128x14x20x!quant.uniform<i8:f32, 0.088586537795275588>, 250880 : i64>, tensor<1x128x14x20x!quant.uniform<i8:f32, 0.088586537795275588>, 215040 : i64>) -> tensor<1x384x14x20x!quant.uniform<i8:f32, 0.088586537795275588>, 304640 : i64> loc(#loc388)
      %207 = "top.Weight"() : () -> tensor<1x256x1x9xi8, 1099516109104 : i64> loc(#loc400)
      %208 = "top.Weight"() : () -> tensor<1x256x1x384xsi8, 1099516111408 : i64> loc(#loc401)
      %209 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099513243440 : i64> loc(#loc402)
      %210 = "tpu.Group"(%206) ({
        %395 = "tpu.Load"(%206) {do_bcast = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 6912, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [384], d_idx = [0], d_slice = [1], h_idx = [0, 7], h_slice = [7, 7], w_idx = [0], w_slice = [20], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x384x14x20x!quant.uniform<i8:f32, 0.088586537795275588>, 304640 : i64>) -> tensor<1x384x14x20x!quant.uniform<i8:f32, 0.088586537795275588>> loc(#loc404)
        %396 = "tpu.Load"(%208) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 12288, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [384], id = 1, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x1x384xsi8, 1099516111408 : i64>) -> tensor<1x256x1x384xsi8> loc(#loc405)
        %397 = "tpu.Load"(%207) {do_bcast = false, ginfo = #tpu.lg<out_addr = 19200, out_size = 288, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 2, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x1x9xi8, 1099516109104 : i64>) -> tensor<1x256x1x9xi8> loc(#loc406)
        %398 = "tpu.Load"(%209) {do_bcast = true, ginfo = #tpu.lg<out_addr = 19488, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 3, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099513243440 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc407)
        %399 = "tpu.Conv2D"(%395, %396, %397) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 20480, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0, 7], h_slice = [7, 7], w_idx = [0], w_slice = [20], id = 4, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x384x14x20x!quant.uniform<i8:f32, 0.088586537795275588>>, tensor<1x256x1x384xsi8>, tensor<1x256x1x9xi8>) -> tensor<1x256x14x20x!quant.uniform<i8:f32, 0.076606081889763783>> loc(#loc408)
        %400 = "tpu.Lut"(%399, %398) {ginfo = #tpu.lg<out_addr = 25088, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0, 7], h_slice = [7, 7], w_idx = [0], w_slice = [20], id = 5, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x256x14x20x!quant.uniform<i8:f32, 0.076606081889763783>>, tensor<1x1x1x256xsi8>) -> tensor<1x256x14x20x!quant.uniform<i8:f32, 0.033747037795275589>> loc(#loc403)
        %401 = "tpu.Store"(%400, %0) {ginfo = #tpu.lg<out_addr = 25088, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0, 7], h_slice = [7, 7], w_idx = [0], w_slice = [20], id = 6, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x256x14x20x!quant.uniform<i8:f32, 0.033747037795275589>>, none) -> tensor<1x256x14x20x!quant.uniform<i8:f32, 0.033747037795275589>, 215040 : i64> loc(#loc403)
        "tpu.Yield"(%401) : (tensor<1x256x14x20x!quant.uniform<i8:f32, 0.033747037795275589>, 215040 : i64>) -> () loc(#loc403)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 4, 3, 6, -2, 5, 0, 1, 2], group_type = 0 : i64, hsecs = 2 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x384x14x20x!quant.uniform<i8:f32, 0.088586537795275588>, 304640 : i64>) -> tensor<1x256x14x20x!quant.uniform<i8:f32, 0.033747037795275589>, 215040 : i64> loc(#loc403)
      %211 = "top.Weight"() : () -> tensor<1x256x2x2xsi8, 1099512832912 : i64> loc(#loc409)
      %212 = "top.Weight"() : () -> tensor<1x256x1x5xi8, 1099516209712 : i64> loc(#loc410)
      %213 = "top.Weight"() : () -> tensor<1x128x1x9xi8, 1099516214384 : i64> loc(#loc411)
      %214 = "top.Weight"() : () -> tensor<1x128x1x512xsi8, 1099516215536 : i64> loc(#loc412)
      %215 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099517465840 : i64> loc(#loc413)
      %216 = "tpu.Group"(%210, %45) ({
        %395 = "tpu.Load"(%210) {do_bcast = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 1536, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0, 2, 4, 6, 8, 10, 12], h_slice = [2, 2, 2, 2, 2, 2, 2], w_idx = [0], w_slice = [20], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x14x20x!quant.uniform<i8:f32, 0.033747037795275589>, 215040 : i64>) -> tensor<1x256x14x20x!quant.uniform<i8:f32, 0.033747037795275589>> loc(#loc415)
        %396 = "tpu.Load"(%211) {do_bcast = false, ginfo = #tpu.lg<out_addr = 18432, out_size = 512, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [2], w_idx = [0], w_slice = [2], id = 1, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x2x2xsi8, 1099512832912 : i64>) -> tensor<1x256x2x2xsi8> loc(#loc416)
        %397 = "tpu.Load"(%212) {do_bcast = false, ginfo = #tpu.lg<out_addr = 18944, out_size = 160, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [5], id = 2, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x1x5xi8, 1099516209712 : i64>) -> tensor<1x256x1x5xi8> loc(#loc417)
        %398 = "tpu.Load"(%45) {do_bcast = false, ginfo = #tpu.lg<out_addr = 20480, out_size = 5120, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0, 4, 8, 12, 16, 20, 24], h_slice = [4, 4, 4, 4, 4, 4, 4], w_idx = [0], w_slice = [40], id = 3, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x28x40x!quant.uniform<i8:f32, 0.045217366141732281>, 501760 : i64>) -> tensor<1x256x28x40x!quant.uniform<i8:f32, 0.045217366141732281>> loc(#loc418)
        %399 = "tpu.Deconv"(%395, %396, %397) {dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 25600, out_size = 5120, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0, 4, 8, 12, 16, 20, 24], h_slice = [4, 4, 4, 4, 4, 4, 4], w_idx = [0], w_slice = [40], id = 4, stage = 1, slice_idx = 0, group_type = 0>, group = 256 : i64, inserts = [0, 0], kernel_shape = [2, 2], pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, strides = [2, 2], with_bias = false} : (tensor<1x256x14x20x!quant.uniform<i8:f32, 0.033747037795275589>>, tensor<1x256x2x2xsi8>, tensor<1x256x1x5xi8>) -> tensor<1x256x28x40x!quant.uniform<i8:f32, 0.033747037795275589>> loc(#loc419)
        %400 = "tpu.Load"(%214) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 8192, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [512], id = 5, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x512xsi8, 1099516215536 : i64>) -> tensor<1x128x1x512xsi8> loc(#loc420)
        %401 = "tpu.Load"(%213) {do_bcast = false, ginfo = #tpu.lg<out_addr = 19104, out_size = 144, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 6, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x9xi8, 1099516214384 : i64>) -> tensor<1x128x1x9xi8> loc(#loc421)
        %402 = "tpu.Concat"(%399, %398) {axis = 1 : si32, do_relu = false, ginfo = #tpu.lg<out_addr = 8192, out_size = 10240, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [512], d_idx = [0], d_slice = [1], h_idx = [0, 4, 8, 12, 16, 20, 24], h_slice = [4, 4, 4, 4, 4, 4, 4], w_idx = [0], w_slice = [40], id = 7, stage = 1, slice_idx = 0, group_type = 0>, multipliers = [95, 1], only_merge = false, relu_limit = -1.000000e+00 : f64, rshifts = [7, 0]} : (tensor<1x256x28x40x!quant.uniform<i8:f32, 0.033747037795275589>>, tensor<1x256x28x40x!quant.uniform<i8:f32, 0.045217366141732281>>) -> tensor<1x512x28x40x!quant.uniform<i8:f32, 0.045217366141732281>> loc(#loc422)
        %403 = "tpu.Load"(%215) {do_bcast = true, ginfo = #tpu.lg<out_addr = 30720, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 8, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099517465840 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc423)
        %404 = "tpu.Conv2D"(%402, %400, %401) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 20480, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 4, 8, 12, 16, 20, 24], h_slice = [4, 4, 4, 4, 4, 4, 4], w_idx = [0], w_slice = [40], id = 9, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x512x28x40x!quant.uniform<i8:f32, 0.045217366141732281>>, tensor<1x128x1x512xsi8>, tensor<1x128x1x9xi8>) -> tensor<1x128x28x40x!quant.uniform<i8:f32, 0.053747642519685036>> loc(#loc424)
        %405 = "tpu.Lut"(%404, %403) {ginfo = #tpu.lg<out_addr = 8192, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 4, 8, 12, 16, 20, 24], h_slice = [4, 4, 4, 4, 4, 4, 4], w_idx = [0], w_slice = [40], id = 10, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x128x28x40x!quant.uniform<i8:f32, 0.053747642519685036>>, tensor<1x1x1x256xsi8>) -> tensor<1x128x28x40x!quant.uniform<i8:f32, 0.033639893700787397>> loc(#loc414)
        %406 = "tpu.Store"(%405, %0) {ginfo = #tpu.lg<out_addr = 8192, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 4, 8, 12, 16, 20, 24], h_slice = [4, 4, 4, 4, 4, 4, 4], w_idx = [0], w_slice = [40], id = 11, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x128x28x40x!quant.uniform<i8:f32, 0.033639893700787397>>, none) -> tensor<1x128x28x40x!quant.uniform<i8:f32, 0.033639893700787397>, 286720 : i64> loc(#loc414)
        "tpu.Yield"(%406) : (tensor<1x128x28x40x!quant.uniform<i8:f32, 0.033639893700787397>, 286720 : i64>) -> () loc(#loc414)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 4, 3, 11, -2, 7, 5, 6, -3, 9, 8, -4, 10, 0, 1, 2], group_type = 0 : i64, hsecs = 7 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x256x14x20x!quant.uniform<i8:f32, 0.033747037795275589>, 215040 : i64>, tensor<1x256x28x40x!quant.uniform<i8:f32, 0.045217366141732281>, 501760 : i64>) -> tensor<1x128x28x40x!quant.uniform<i8:f32, 0.033639893700787397>, 286720 : i64> loc(#loc414)
      %217 = "tpu.Slice"(%216, %0, %0, %0, %0) {axes = [], ends = [9223372036854775807, 64, 9223372036854775807, 9223372036854775807], hasparamConvert_axes = [1], offset = [0, 0, 0, 0], steps = [1, 1, 1, 1]} : (tensor<1x128x28x40x!quant.uniform<i8:f32, 0.033639893700787397>, 286720 : i64>, none, none, none, none) -> tensor<1x64x28x40x!quant.uniform<i8:f32, 0.033639893700787397>, 286720 : i64> loc(#loc425)
      %218 = "tpu.Slice"(%216, %0, %0, %0, %0) {axes = [], ends = [9223372036854775807, 128, 9223372036854775807, 9223372036854775807], hasparamConvert_axes = [1], offset = [0, 64, 0, 0], steps = [1, 1, 1, 1]} : (tensor<1x128x28x40x!quant.uniform<i8:f32, 0.033639893700787397>, 286720 : i64>, none, none, none, none) -> tensor<1x64x28x40x!quant.uniform<i8:f32, 0.033639893700787397>, 358400 : i64> loc(#loc426)
      %219 = "top.Weight"() {do_compress = true} : () -> tensor<1x32x1x9xi8, 1099513316928 : i64> loc(#loc427)
      %220 = "top.Weight"() {do_compress = true} : () -> tensor<1x32x9x64xsi8, 1099517474800 : i64> loc(#loc428)
      %221 = "tpu.Conv2D"(%218, %220, %219) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.033639893700787397>, 358400 : i64>, tensor<1x32x9x64xsi8, 1099517474800 : i64>, tensor<1x32x1x9xi8, 1099513316928 : i64>) -> tensor<1x32x28x40x!quant.uniform<i8:f32, 0.078409998425196858>, 0 : i64> loc(#loc429)
      %222 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099517494384 : i64> loc(#loc430)
      %223 = "top.Weight"() : () -> tensor<1x64x1x9xi8, 1099515702256 : i64> loc(#loc431)
      %224 = "top.Weight"() : () -> tensor<1x64x9x32xsi8, 1099517494640 : i64> loc(#loc432)
      %225 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099517513120 : i64> loc(#loc433)
      %226 = "tpu.Group"(%221, %218) ({
        %395 = "tpu.Load"(%221) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 2432, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0, 13], h_slice = [15, 15], w_idx = [0], w_slice = [40], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x32x28x40x!quant.uniform<i8:f32, 0.078409998425196858>, 0 : i64>) -> tensor<1x32x28x40x!quant.uniform<i8:f32, 0.078409998425196858>> loc(#loc435)
        %396 = "tpu.Load"(%222) {do_bcast = true, ginfo = #tpu.lg<out_addr = 6784, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 1, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099517494384 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc436)
        %397 = "tpu.Load"(%224) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [32], id = 2, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x9x32xsi8, 1099517494640 : i64>) -> tensor<1x64x9x32xsi8> loc(#loc437)
        %398 = "tpu.Load"(%223) {do_bcast = false, ginfo = #tpu.lg<out_addr = 20864, out_size = 72, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 3, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x9xi8, 1099515702256 : i64>) -> tensor<1x64x1x9xi8> loc(#loc438)
        %399 = "tpu.Lut"(%395, %396) {ginfo = #tpu.lg<out_addr = 4096, out_size = 2432, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0, 13], h_slice = [15, 15], w_idx = [0], w_slice = [40], id = 4, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x32x28x40x!quant.uniform<i8:f32, 0.078409998425196858>>, tensor<1x1x1x256xsi8>) -> tensor<1x32x28x40x!quant.uniform<i8:f32, 0.053174068503937008>> loc(#loc439)
        %400 = "tpu.Load"(%225) {do_bcast = true, ginfo = #tpu.lg<out_addr = 31488, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 5, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099517513120 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc440)
        %401 = "tpu.Conv2D"(%399, %397, %398) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 8192, out_size = 4480, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 14], h_slice = [14, 14], w_idx = [0], w_slice = [40], id = 6, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x32x28x40x!quant.uniform<i8:f32, 0.053174068503937008>>, tensor<1x64x9x32xsi8>, tensor<1x64x1x9xi8>) -> tensor<1x64x28x40x!quant.uniform<i8:f32, 0.10656669212598426>> loc(#loc441)
        %402 = "tpu.Load"(%218) {do_bcast = false, ginfo = #tpu.lg<out_addr = 2304, out_size = 4480, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 14], h_slice = [14, 14], w_idx = [0], w_slice = [40], id = 7, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.033639893700787397>, 358400 : i64>) -> tensor<1x64x28x40x!quant.uniform<i8:f32, 0.033639893700787397>> loc(#loc442)
        %403 = "tpu.Lut"(%401, %400) {ginfo = #tpu.lg<out_addr = 16384, out_size = 4480, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 14], h_slice = [14, 14], w_idx = [0], w_slice = [40], id = 8, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.10656669212598426>>, tensor<1x1x1x256xsi8>) -> tensor<1x64x28x40x!quant.uniform<i8:f32, 0.065026581102362202>> loc(#loc443)
        %404 = "tpu.Add"(%402, %403) {do_relu = false, ginfo = #tpu.lg<out_addr = 8192, out_size = 4480, buffer_addr = 27008, buffer_size = 4480, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 14], h_slice = [14, 14], w_idx = [0], w_slice = [40], id = 9, stage = 1, slice_idx = 0, group_type = 0>, multipliers = [65, 126], relu_limit = -1.000000e+00 : f64, rshifts = [7]} : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.033639893700787397>>, tensor<1x64x28x40x!quant.uniform<i8:f32, 0.065026581102362202>>) -> tensor<1x64x28x40x!quant.uniform<i8:f32, 0.06556938425196851>> loc(#loc434)
        %405 = "tpu.Store"(%404, %0) {ginfo = #tpu.lg<out_addr = 8192, out_size = 4480, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 14], h_slice = [14, 14], w_idx = [0], w_slice = [40], id = 10, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.06556938425196851>>, none) -> tensor<1x64x28x40x!quant.uniform<i8:f32, 0.06556938425196851>, 430080 : i64> loc(#loc434)
        "tpu.Yield"(%405) : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.06556938425196851>, 430080 : i64>) -> () loc(#loc434)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 4, 2, 3, 10, -2, 6, 5, -3, 8, 7, -4, 9, 0, 1], group_type = 0 : i64, hsecs = 2 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x32x28x40x!quant.uniform<i8:f32, 0.078409998425196858>, 0 : i64>, tensor<1x64x28x40x!quant.uniform<i8:f32, 0.033639893700787397>, 358400 : i64>) -> tensor<1x64x28x40x!quant.uniform<i8:f32, 0.06556938425196851>, 430080 : i64> loc(#loc434)
      %227 = "top.Weight"() : () -> tensor<1x128x1x9xi8, 1099518103200 : i64> loc(#loc444)
      %228 = "top.Weight"() : () -> tensor<1x128x1x192xsi8, 1099513857024 : i64> loc(#loc445)
      %229 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099518123616 : i64> loc(#loc446)
      %230 = "tpu.Group"(%217, %218, %226) ({
        %395 = "tpu.Load"(%217) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 7, 14, 21], h_slice = [7, 7, 7, 7], w_idx = [0], w_slice = [40], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.033639893700787397>, 286720 : i64>) -> tensor<1x64x28x40x!quant.uniform<i8:f32, 0.033639893700787397>> loc(#loc448)
        %396 = "tpu.Load"(%218) {do_bcast = false, ginfo = #tpu.lg<out_addr = 28672, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 7, 14, 21], h_slice = [7, 7, 7, 7], w_idx = [0], w_slice = [40], id = 1, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.033639893700787397>, 358400 : i64>) -> tensor<1x64x28x40x!quant.uniform<i8:f32, 0.033639893700787397>> loc(#loc442)
        %397 = "tpu.Load"(%226) {do_bcast = false, ginfo = #tpu.lg<out_addr = 20992, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 7, 14, 21], h_slice = [7, 7, 7, 7], w_idx = [0], w_slice = [40], id = 2, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.06556938425196851>, 430080 : i64>) -> tensor<1x64x28x40x!quant.uniform<i8:f32, 0.06556938425196851>> loc(#loc449)
        %398 = "tpu.Load"(%228) {do_bcast = false, ginfo = #tpu.lg<out_addr = 8192, out_size = 3072, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [192], id = 3, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x192xsi8, 1099513857024 : i64>) -> tensor<1x128x1x192xsi8> loc(#loc450)
        %399 = "tpu.Load"(%227) {do_bcast = false, ginfo = #tpu.lg<out_addr = 15872, out_size = 144, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 4, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x9xi8, 1099518103200 : i64>) -> tensor<1x128x1x9xi8> loc(#loc451)
        %400 = "tpu.Concat"(%395, %396, %397) {axis = 1 : si32, do_relu = false, ginfo = #tpu.lg<out_addr = 0, out_size = 6912, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [192], d_idx = [0], d_slice = [1], h_idx = [0, 7, 14, 21], h_slice = [7, 7, 7, 7], w_idx = [0], w_slice = [40], id = 5, stage = 1, slice_idx = 0, group_type = 0>, multipliers = [124, 124, 121], only_merge = false, relu_limit = -1.000000e+00 : f64, rshifts = [8, 8, 7]} : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.033639893700787397>>, tensor<1x64x28x40x!quant.uniform<i8:f32, 0.033639893700787397>>, tensor<1x64x28x40x!quant.uniform<i8:f32, 0.06556938425196851>>) -> tensor<1x192x28x40x!quant.uniform<i8:f32, 0.069080994488188982>> loc(#loc452)
        %401 = "tpu.Load"(%229) {do_bcast = true, ginfo = #tpu.lg<out_addr = 6912, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 6, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099518123616 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc453)
        %402 = "tpu.Conv2D"(%400, %398, %399) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 7, 14, 21], h_slice = [7, 7, 7, 7], w_idx = [0], w_slice = [40], id = 7, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x192x28x40x!quant.uniform<i8:f32, 0.069080994488188982>>, tensor<1x128x1x192xsi8>, tensor<1x128x1x9xi8>) -> tensor<1x128x28x40x!quant.uniform<i8:f32, 8.958010e-02>> loc(#loc454)
        %403 = "tpu.Lut"(%402, %401) {ginfo = #tpu.lg<out_addr = 11264, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 7, 14, 21], h_slice = [7, 7, 7, 7], w_idx = [0], w_slice = [40], id = 8, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x128x28x40x!quant.uniform<i8:f32, 8.958010e-02>>, tensor<1x1x1x256xsi8>) -> tensor<1x128x28x40x!quant.uniform<i8:f32, 0.066392995275590547>> loc(#loc447)
        %404 = "tpu.Store"(%403, %0) {ginfo = #tpu.lg<out_addr = 11264, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 7, 14, 21], h_slice = [7, 7, 7, 7], w_idx = [0], w_slice = [40], id = 9, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x128x28x40x!quant.uniform<i8:f32, 0.066392995275590547>>, none) -> tensor<1x128x28x40x!quant.uniform<i8:f32, 0.066392995275590547>, 0 : i64> loc(#loc447)
        "tpu.Yield"(%404) : (tensor<1x128x28x40x!quant.uniform<i8:f32, 0.066392995275590547>, 0 : i64>) -> () loc(#loc447)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 5, 3, 4, 9, -2, 7, 6, -3, 8, 0, 1, 2], group_type = 0 : i64, hsecs = 4 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.033639893700787397>, 286720 : i64>, tensor<1x64x28x40x!quant.uniform<i8:f32, 0.033639893700787397>, 358400 : i64>, tensor<1x64x28x40x!quant.uniform<i8:f32, 0.06556938425196851>, 430080 : i64>) -> tensor<1x128x28x40x!quant.uniform<i8:f32, 0.066392995275590547>, 0 : i64> loc(#loc447)
      %231 = "top.Weight"() {do_compress = true} : () -> tensor<1x128x1x9xi8, 1099514025328 : i64> loc(#loc455)
      %232 = "top.Weight"() {do_compress = true} : () -> tensor<1x128x9x128xsi8, 1099512125328 : i64> loc(#loc456)
      %233 = "tpu.Conv2D"(%230, %232, %231) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [2, 2], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x28x40x!quant.uniform<i8:f32, 0.066392995275590547>, 0 : i64>, tensor<1x128x9x128xsi8, 1099512125328 : i64>, tensor<1x128x1x9xi8, 1099514025328 : i64>) -> tensor<1x128x14x20x!quant.uniform<i8:f32, 0.10872806850393701>, 286720 : i64> loc(#loc457)
      %234 = "top.Weight"() {do_compress = true} : () -> tensor<1x64x1x9xi8, 1099518123872 : i64> loc(#loc458)
      %235 = "top.Weight"() {do_compress = true} : () -> tensor<1x64x9x128xsi8, 1099518124704 : i64> loc(#loc459)
      %236 = "tpu.Conv2D"(%230, %235, %234) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x28x40x!quant.uniform<i8:f32, 0.066392995275590547>, 0 : i64>, tensor<1x64x9x128xsi8, 1099518124704 : i64>, tensor<1x64x1x9xi8, 1099518123872 : i64>) -> tensor<1x64x28x40x!quant.uniform<i8:f32, 0.1367103094488189>, 358400 : i64> loc(#loc460)
      %237 = "top.Weight"() : () -> tensor<1x128x1x9xi8, 1099518412576 : i64> loc(#loc461)
      %238 = "top.Weight"() : () -> tensor<1x128x3x3xsi8, 1099518414880 : i64> loc(#loc462)
      %239 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099518416032 : i64> loc(#loc463)
      %240:2 = "tpu.Group"(%230, %233) ({
        %395 = "tpu.Load"(%230) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 9728, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 13], h_slice = [15, 15], w_idx = [0], w_slice = [40], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x28x40x!quant.uniform<i8:f32, 0.066392995275590547>, 0 : i64>) -> tensor<1x128x28x40x!quant.uniform<i8:f32, 0.066392995275590547>> loc(#loc466)
        %396 = "tpu.Load"(%238) {do_bcast = false, ginfo = #tpu.lg<out_addr = 9984, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [3], w_idx = [0], w_slice = [3], id = 1, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x3x3xsi8, 1099518414880 : i64>) -> tensor<1x128x3x3xsi8> loc(#loc467)
        %397 = "tpu.Load"(%237) {do_bcast = false, ginfo = #tpu.lg<out_addr = 10240, out_size = 144, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 2, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x9xi8, 1099518412576 : i64>) -> tensor<1x128x1x9xi8> loc(#loc468)
        %398 = "tpu.Load"(%233) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 7], h_slice = [7, 7], w_idx = [0], w_slice = [20], id = 3, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.10872806850393701>, 286720 : i64>) -> tensor<1x128x14x20x!quant.uniform<i8:f32, 0.10872806850393701>> loc(#loc469)
        %399 = "tpu.Load"(%239) {do_bcast = true, ginfo = #tpu.lg<out_addr = 9728, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 4, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099518416032 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc470)
        %400 = "tpu.Conv2D"(%395, %396, %397) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 8960, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 14], h_slice = [14, 14], w_idx = [0], w_slice = [40], id = 5, stage = 1, slice_idx = 0, group_type = 0>, group = 128 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x28x40x!quant.uniform<i8:f32, 0.066392995275590547>>, tensor<1x128x3x3xsi8>, tensor<1x128x1x9xi8>) -> tensor<1x128x28x40x!quant.uniform<i8:f32, 0.073945248031496061>> loc(#loc464)
        %401 = "tpu.Store"(%400, %0) {ginfo = #tpu.lg<out_addr = 12288, out_size = 8960, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 14], h_slice = [14, 14], w_idx = [0], w_slice = [40], id = 6, stage = 1, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x128x28x40x!quant.uniform<i8:f32, 0.073945248031496061>>, none) -> tensor<1x128x28x40x!quant.uniform<i8:f32, 0.073945248031496061>, 465920 : i64> loc(#loc464)
        %402 = "tpu.Lut"(%398, %399) {ginfo = #tpu.lg<out_addr = 28672, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 7], h_slice = [7, 7], w_idx = [0], w_slice = [20], id = 7, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.10872806850393701>>, tensor<1x1x1x256xsi8>) -> tensor<1x128x14x20x!quant.uniform<i8:f32, 0.087846805511811024>> loc(#loc465)
        %403 = "tpu.Store"(%402, %0) {ginfo = #tpu.lg<out_addr = 28672, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 7], h_slice = [7, 7], w_idx = [0], w_slice = [20], id = 8, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.087846805511811024>>, none) -> tensor<1x128x14x20x!quant.uniform<i8:f32, 0.087846805511811024>, 430080 : i64> loc(#loc465)
        "tpu.Yield"(%401, %403) : (tensor<1x128x28x40x!quant.uniform<i8:f32, 0.073945248031496061>, 465920 : i64>, tensor<1x128x14x20x!quant.uniform<i8:f32, 0.087846805511811024>, 430080 : i64>) -> () loc(#loc815)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 5, 3, 4, 8, -2, 7, 6, 0, 1, 2], group_type = 0 : i64, hsecs = 2 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x128x28x40x!quant.uniform<i8:f32, 0.066392995275590547>, 0 : i64>, tensor<1x128x14x20x!quant.uniform<i8:f32, 0.10872806850393701>, 286720 : i64>) -> (tensor<1x128x28x40x!quant.uniform<i8:f32, 0.073945248031496061>, 465920 : i64>, tensor<1x128x14x20x!quant.uniform<i8:f32, 0.087846805511811024>, 430080 : i64>) loc(#loc815)
      %241 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099520737424 : i64> loc(#loc471)
      %242 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099517460720 : i64> loc(#loc472)
      %243:2 = "tpu.Group"(%236, %240#0) ({
        %395 = "tpu.Load"(%236) {do_bcast = false, ginfo = #tpu.lg<out_addr = 20480, out_size = 3200, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 10, 19], h_slice = [10, 9, 9], w_idx = [0], w_slice = [40], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.1367103094488189>, 358400 : i64>) -> tensor<1x64x28x40x!quant.uniform<i8:f32, 0.1367103094488189>> loc(#loc475)
        %396 = "tpu.Load"(%241) {do_bcast = true, ginfo = #tpu.lg<out_addr = 28672, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 1, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099520737424 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc476)
        %397 = "tpu.Load"(%240#0) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 10, 19], h_slice = [10, 9, 9], w_idx = [0], w_slice = [40], id = 2, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x28x40x!quant.uniform<i8:f32, 0.073945248031496061>, 465920 : i64>) -> tensor<1x128x28x40x!quant.uniform<i8:f32, 0.073945248031496061>> loc(#loc477)
        %398 = "tpu.Load"(%242) {do_bcast = true, ginfo = #tpu.lg<out_addr = 24576, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 3, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099517460720 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc478)
        %399 = "tpu.Lut"(%395, %396) {ginfo = #tpu.lg<out_addr = 16384, out_size = 3200, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 10, 19], h_slice = [10, 9, 9], w_idx = [0], w_slice = [40], id = 4, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.1367103094488189>>, tensor<1x1x1x256xsi8>) -> tensor<1x64x28x40x!quant.uniform<i8:f32, 0.06752622125984252>> loc(#loc473)
        %400 = "tpu.Store"(%399, %0) {ginfo = #tpu.lg<out_addr = 16384, out_size = 3200, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 10, 19], h_slice = [10, 9, 9], w_idx = [0], w_slice = [40], id = 5, stage = 1, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.06752622125984252>>, none) -> tensor<1x64x28x40x!quant.uniform<i8:f32, 0.06752622125984252>, 286720 : i64> loc(#loc473)
        %401 = "tpu.Lut"(%397, %398) {ginfo = #tpu.lg<out_addr = 8192, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 10, 19], h_slice = [10, 9, 9], w_idx = [0], w_slice = [40], id = 6, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x128x28x40x!quant.uniform<i8:f32, 0.073945248031496061>>, tensor<1x1x1x256xsi8>) -> tensor<1x128x28x40x!quant.uniform<i8:f32, 0.074797672440944873>> loc(#loc474)
        %402 = "tpu.Store"(%401, %0) {ginfo = #tpu.lg<out_addr = 8192, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 10, 19], h_slice = [10, 9, 9], w_idx = [0], w_slice = [40], id = 7, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x128x28x40x!quant.uniform<i8:f32, 0.074797672440944873>>, none) -> tensor<1x128x28x40x!quant.uniform<i8:f32, 0.074797672440944873>, 0 : i64> loc(#loc474)
        "tpu.Yield"(%400, %402) : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.06752622125984252>, 286720 : i64>, tensor<1x128x28x40x!quant.uniform<i8:f32, 0.074797672440944873>, 0 : i64>) -> () loc(#loc816)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 4, 2, 3, 7, -2, 6, 5, 0, 1], group_type = 0 : i64, hsecs = 3 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [-2, 1], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.1367103094488189>, 358400 : i64>, tensor<1x128x28x40x!quant.uniform<i8:f32, 0.073945248031496061>, 465920 : i64>) -> (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.06752622125984252>, 286720 : i64>, tensor<1x128x28x40x!quant.uniform<i8:f32, 0.074797672440944873>, 0 : i64>) loc(#loc816)
      %244 = "top.Weight"() : () -> tensor<1x64x1x9xi8, 1099518416288 : i64> loc(#loc479)
      %245 = "top.Weight"() : () -> tensor<1x64x9x64xsi8, 1099519301904 : i64> loc(#loc480)
      %246 = "top.Weight"() : () -> tensor<1x128x1x9xi8, 1099515702832 : i64> loc(#loc481)
      %247 = "top.Weight"() : () -> tensor<1x128x1x128xsi8, 1099518416912 : i64> loc(#loc482)
      %248 = "top.Weight"() : () -> tensor<1x256x1x9xi8, 1099518958736 : i64> loc(#loc483)
      %249 = "top.Weight"() : () -> tensor<1x256x1x384xsi8, 1099518961040 : i64> loc(#loc484)
      %250 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099519059600 : i64> loc(#loc485)
      %251 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099519059856 : i64> loc(#loc486)
      %252:3 = "tpu.Group"(%240#1, %210, %243#0, %243#1) ({
        %395 = "tpu.Load"(%240#1) {do_bcast = false, ginfo = #tpu.lg<out_addr = 19456, out_size = 768, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 2, 4, 6, 8, 10, 12], h_slice = [2, 2, 2, 2, 2, 2, 2], w_idx = [0], w_slice = [20], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.087846805511811024>, 430080 : i64>) -> tensor<1x128x14x20x!quant.uniform<i8:f32, 0.087846805511811024>> loc(#loc490)
        %396 = "tpu.Load"(%210) {do_bcast = false, ginfo = #tpu.lg<out_addr = 28672, out_size = 1536, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0, 2, 4, 6, 8, 10, 12], h_slice = [2, 2, 2, 2, 2, 2, 2], w_idx = [0], w_slice = [20], id = 1, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x14x20x!quant.uniform<i8:f32, 0.033747037795275589>, 215040 : i64>) -> tensor<1x256x14x20x!quant.uniform<i8:f32, 0.033747037795275589>> loc(#loc415)
        %397 = "tpu.Load"(%243#0) {do_bcast = false, ginfo = #tpu.lg<out_addr = 16896, out_size = 1920, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 3, 7, 11, 15, 19, 23], h_slice = [5, 6, 6, 6, 6, 6, 5], w_idx = [0], w_slice = [40], id = 2, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.06752622125984252>, 286720 : i64>) -> tensor<1x64x28x40x!quant.uniform<i8:f32, 0.06752622125984252>> loc(#loc491)
        %398 = "tpu.Load"(%245) {do_bcast = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [64], id = 3, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x9x64xsi8, 1099519301904 : i64>) -> tensor<1x64x9x64xsi8> loc(#loc492)
        %399 = "tpu.Load"(%244) {do_bcast = false, ginfo = #tpu.lg<out_addr = 32176, out_size = 72, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 4, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x9xi8, 1099518416288 : i64>) -> tensor<1x64x1x9xi8> loc(#loc493)
        %400 = "tpu.Concat"(%395, %396) {axis = 1 : si32, do_relu = false, ginfo = #tpu.lg<out_addr = 25088, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [384], d_idx = [0], d_slice = [1], h_idx = [0, 2, 4, 6, 8, 10, 12], h_slice = [2, 2, 2, 2, 2, 2, 2], w_idx = [0], w_slice = [20], id = 5, stage = 1, slice_idx = 0, group_type = 0>, multipliers = [1, 98], only_merge = false, relu_limit = -1.000000e+00 : f64, rshifts = [0, 8]} : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.087846805511811024>>, tensor<1x256x14x20x!quant.uniform<i8:f32, 0.033747037795275589>>) -> tensor<1x384x14x20x!quant.uniform<i8:f32, 0.087846805511811024>> loc(#loc494)
        %401 = "tpu.Load"(%243#1) {do_bcast = false, ginfo = #tpu.lg<out_addr = 28672, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 4, 8, 12, 16, 20, 24], h_slice = [4, 4, 4, 4, 4, 4, 4], w_idx = [0], w_slice = [40], id = 6, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x28x40x!quant.uniform<i8:f32, 0.074797672440944873>, 0 : i64>) -> tensor<1x128x28x40x!quant.uniform<i8:f32, 0.074797672440944873>> loc(#loc495)
        %402 = "tpu.Load"(%247) {do_bcast = false, ginfo = #tpu.lg<out_addr = 23040, out_size = 2048, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [128], id = 7, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x128xsi8, 1099518416912 : i64>) -> tensor<1x128x1x128xsi8> loc(#loc496)
        %403 = "tpu.Load"(%246) {do_bcast = false, ginfo = #tpu.lg<out_addr = 31776, out_size = 144, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 8, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x9xi8, 1099515702832 : i64>) -> tensor<1x128x1x9xi8> loc(#loc497)
        %404 = "tpu.Conv2D"(%397, %398, %399) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 27392, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 4, 8, 12, 16, 20, 24], h_slice = [4, 4, 4, 4, 4, 4, 4], w_idx = [0], w_slice = [40], id = 9, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.06752622125984252>>, tensor<1x64x9x64xsi8>, tensor<1x64x1x9xi8>) -> tensor<1x64x28x40x!quant.uniform<i8:f32, 0.22355868110236218>> loc(#loc498)
        %405 = "tpu.Load"(%249) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 12288, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [384], id = 10, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x1x384xsi8, 1099518961040 : i64>) -> tensor<1x256x1x384xsi8> loc(#loc499)
        %406 = "tpu.Load"(%248) {do_bcast = false, ginfo = #tpu.lg<out_addr = 31232, out_size = 288, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 11, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x1x9xi8, 1099518958736 : i64>) -> tensor<1x256x1x9xi8> loc(#loc500)
        %407 = "tpu.Conv2D"(%401, %402, %403) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 16896, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 4, 8, 12, 16, 20, 24], h_slice = [4, 4, 4, 4, 4, 4, 4], w_idx = [0], w_slice = [40], id = 12, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x28x40x!quant.uniform<i8:f32, 0.074797672440944873>>, tensor<1x128x1x128xsi8>, tensor<1x128x1x9xi8>) -> tensor<1x128x28x40x!quant.uniform<i8:f32, 0.073418750393700785>> loc(#loc501)
        %408 = "tpu.Load"(%250) {do_bcast = true, ginfo = #tpu.lg<out_addr = 31520, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 13, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099519059600 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc502)
        %409 = "tpu.Conv2D"(%400, %405, %406) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 19456, out_size = 1536, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0, 2, 4, 6, 8, 10, 12], h_slice = [2, 2, 2, 2, 2, 2, 2], w_idx = [0], w_slice = [20], id = 14, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x384x14x20x!quant.uniform<i8:f32, 0.087846805511811024>>, tensor<1x256x1x384xsi8>, tensor<1x256x1x9xi8>) -> tensor<1x256x14x20x!quant.uniform<i8:f32, 0.10009854960629921>> loc(#loc487)
        %410 = "tpu.Load"(%251) {do_bcast = true, ginfo = #tpu.lg<out_addr = 31920, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 15, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099519059856 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc503)
        %411 = "tpu.Store"(%409, %0) {ginfo = #tpu.lg<out_addr = 19456, out_size = 1536, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0, 2, 4, 6, 8, 10, 12], h_slice = [2, 2, 2, 2, 2, 2, 2], w_idx = [0], w_slice = [20], id = 16, stage = 1, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x256x14x20x!quant.uniform<i8:f32, 0.10009854960629921>>, none) -> tensor<1x256x14x20x!quant.uniform<i8:f32, 0.10009854960629921>, 609280 : i64> loc(#loc487)
        %412 = "tpu.Lut"(%404, %408) {ginfo = #tpu.lg<out_addr = 25088, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 4, 8, 12, 16, 20, 24], h_slice = [4, 4, 4, 4, 4, 4, 4], w_idx = [0], w_slice = [40], id = 17, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.22355868110236218>>, tensor<1x1x1x256xsi8>) -> tensor<1x64x28x40x!quant.uniform<i8:f32, 0.14614416141732284>> loc(#loc488)
        %413 = "tpu.Store"(%412, %0) {ginfo = #tpu.lg<out_addr = 25088, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 4, 8, 12, 16, 20, 24], h_slice = [4, 4, 4, 4, 4, 4, 4], w_idx = [0], w_slice = [40], id = 18, stage = 1, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.14614416141732284>>, none) -> tensor<1x64x28x40x!quant.uniform<i8:f32, 0.14614416141732284>, 358400 : i64> loc(#loc488)
        %414 = "tpu.Lut"(%407, %410) {ginfo = #tpu.lg<out_addr = 20480, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 4, 8, 12, 16, 20, 24], h_slice = [4, 4, 4, 4, 4, 4, 4], w_idx = [0], w_slice = [40], id = 19, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x128x28x40x!quant.uniform<i8:f32, 0.073418750393700785>>, tensor<1x1x1x256xsi8>) -> tensor<1x128x28x40x!quant.uniform<i8:f32, 0.095037182677165349>> loc(#loc489)
        %415 = "tpu.Store"(%414, %0) {ginfo = #tpu.lg<out_addr = 20480, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 4, 8, 12, 16, 20, 24], h_slice = [4, 4, 4, 4, 4, 4, 4], w_idx = [0], w_slice = [40], id = 20, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x128x28x40x!quant.uniform<i8:f32, 0.095037182677165349>>, none) -> tensor<1x128x28x40x!quant.uniform<i8:f32, 0.095037182677165349>, 465920 : i64> loc(#loc489)
        "tpu.Yield"(%411, %413, %415) : (tensor<1x256x14x20x!quant.uniform<i8:f32, 0.10009854960629921>, 609280 : i64>, tensor<1x64x28x40x!quant.uniform<i8:f32, 0.14614416141732284>, 358400 : i64>, tensor<1x128x28x40x!quant.uniform<i8:f32, 0.095037182677165349>, 465920 : i64>) -> () loc(#loc817)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 5, 2, 3, 4, -2, 9, 6, 7, 8, 20, -3, 12, 10, 11, -4, 14, 13, -5, 17, 15, 16, -6, 19, 18, 0, 1], group_type = 0 : i64, hsecs = 7 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [-6, 1], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [1], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.087846805511811024>, 430080 : i64>, tensor<1x256x14x20x!quant.uniform<i8:f32, 0.033747037795275589>, 215040 : i64>, tensor<1x64x28x40x!quant.uniform<i8:f32, 0.06752622125984252>, 286720 : i64>, tensor<1x128x28x40x!quant.uniform<i8:f32, 0.074797672440944873>, 0 : i64>) -> (tensor<1x256x14x20x!quant.uniform<i8:f32, 0.10009854960629921>, 609280 : i64>, tensor<1x64x28x40x!quant.uniform<i8:f32, 0.14614416141732284>, 358400 : i64>, tensor<1x128x28x40x!quant.uniform<i8:f32, 0.095037182677165349>, 465920 : i64>) loc(#loc817)
      %253 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099512105488 : i64> loc(#loc504)
      %254 = "top.Weight"() : () -> tensor<1x64x1x9xi8, 1099518105504 : i64> loc(#loc505)
      %255 = "top.Weight"() : () -> tensor<1x64x1x64xsi8, 1099519097552 : i64> loc(#loc506)
      %256 = "top.Weight"() : () -> tensor<1x128x1x9xi8, 1099519101648 : i64> loc(#loc507)
      %257 = "top.Weight"() : () -> tensor<1x128x3x3xsi8, 1099519102800 : i64> loc(#loc508)
      %258:3 = "tpu.Group"(%252#0, %252#1, %252#2) ({
        %395 = "tpu.Load"(%252#0) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 3584, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0, 5, 10], h_slice = [5, 5, 4], w_idx = [0], w_slice = [20], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x14x20x!quant.uniform<i8:f32, 0.10009854960629921>, 609280 : i64>) -> tensor<1x256x14x20x!quant.uniform<i8:f32, 0.10009854960629921>> loc(#loc512)
        %396 = "tpu.Load"(%253) {do_bcast = true, ginfo = #tpu.lg<out_addr = 31360, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 1, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099512105488 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc513)
        %397 = "tpu.Load"(%252#1) {do_bcast = false, ginfo = #tpu.lg<out_addr = 18688, out_size = 3200, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 10, 19], h_slice = [10, 9, 9], w_idx = [0], w_slice = [40], id = 2, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.14614416141732284>, 358400 : i64>) -> tensor<1x64x28x40x!quant.uniform<i8:f32, 0.14614416141732284>> loc(#loc514)
        %398 = "tpu.Load"(%255) {do_bcast = false, ginfo = #tpu.lg<out_addr = 11008, out_size = 512, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [64], id = 3, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x64xsi8, 1099519097552 : i64>) -> tensor<1x64x1x64xsi8> loc(#loc515)
        %399 = "tpu.Load"(%254) {do_bcast = false, ginfo = #tpu.lg<out_addr = 11664, out_size = 72, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 4, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x9xi8, 1099518105504 : i64>) -> tensor<1x64x1x9xi8> loc(#loc516)
        %400 = "tpu.Lut"(%395, %396) {ginfo = #tpu.lg<out_addr = 7168, out_size = 3584, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0, 5, 10], h_slice = [5, 5, 4], w_idx = [0], w_slice = [20], id = 5, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x256x14x20x!quant.uniform<i8:f32, 0.10009854960629921>>, tensor<1x1x1x256xsi8>) -> tensor<1x256x14x20x!quant.uniform<i8:f32, 0.088813670866141725>> loc(#loc509)
        %401 = "tpu.Load"(%252#2) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 7168, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 9, 18], h_slice = [11, 11, 10], w_idx = [0], w_slice = [40], id = 6, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x28x40x!quant.uniform<i8:f32, 0.095037182677165349>, 465920 : i64>) -> tensor<1x128x28x40x!quant.uniform<i8:f32, 0.095037182677165349>> loc(#loc517)
        %402 = "tpu.Load"(%257) {do_bcast = false, ginfo = #tpu.lg<out_addr = 10752, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [3], w_idx = [0], w_slice = [3], id = 7, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x3x3xsi8, 1099519102800 : i64>) -> tensor<1x128x3x3xsi8> loc(#loc518)
        %403 = "tpu.Load"(%256) {do_bcast = false, ginfo = #tpu.lg<out_addr = 11520, out_size = 144, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 8, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x9xi8, 1099519101648 : i64>) -> tensor<1x128x1x9xi8> loc(#loc519)
        %404 = "tpu.Store"(%400, %0) {ginfo = #tpu.lg<out_addr = 7168, out_size = 3584, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0, 5, 10], h_slice = [5, 5, 4], w_idx = [0], w_slice = [20], id = 9, stage = 1, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x256x14x20x!quant.uniform<i8:f32, 0.088813670866141725>>, none) -> tensor<1x256x14x20x!quant.uniform<i8:f32, 0.088813670866141725>, 107520 : i64> loc(#loc509)
        %405 = "tpu.Conv2D"(%397, %398, %399) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 28160, out_size = 3200, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 10, 19], h_slice = [10, 9, 9], w_idx = [0], w_slice = [40], id = 10, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.14614416141732284>>, tensor<1x64x1x64xsi8>, tensor<1x64x1x9xi8>) -> tensor<1x64x28x40x!quant.uniform<i8:f32, 0.18704145511811024>> loc(#loc510)
        %406 = "tpu.Store"(%405, %0) {ginfo = #tpu.lg<out_addr = 28160, out_size = 3200, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 10, 19], h_slice = [10, 9, 9], w_idx = [0], w_slice = [40], id = 11, stage = 1, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.18704145511811024>>, none) -> tensor<1x64x28x40x!quant.uniform<i8:f32, 0.18704145511811024>, 0 : i64> loc(#loc510)
        %407 = "tpu.Conv2D"(%401, %402, %403) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 10, 19], h_slice = [10, 9, 9], w_idx = [0], w_slice = [40], id = 12, stage = 1, slice_idx = 0, group_type = 0>, group = 128 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x28x40x!quant.uniform<i8:f32, 0.095037182677165349>>, tensor<1x128x3x3xsi8>, tensor<1x128x1x9xi8>) -> tensor<1x128x28x40x!quant.uniform<i8:f32, 0.087665404724409451>> loc(#loc511)
        %408 = "tpu.Store"(%407, %0) {ginfo = #tpu.lg<out_addr = 12288, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 10, 19], h_slice = [10, 9, 9], w_idx = [0], w_slice = [40], id = 13, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x128x28x40x!quant.uniform<i8:f32, 0.087665404724409451>>, none) -> tensor<1x128x28x40x!quant.uniform<i8:f32, 0.087665404724409451>, 215040 : i64> loc(#loc511)
        "tpu.Yield"(%404, %406, %408) : (tensor<1x256x14x20x!quant.uniform<i8:f32, 0.088813670866141725>, 107520 : i64>, tensor<1x64x28x40x!quant.uniform<i8:f32, 0.18704145511811024>, 0 : i64>, tensor<1x128x28x40x!quant.uniform<i8:f32, 0.087665404724409451>, 215040 : i64>) -> () loc(#loc818)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 5, 2, 3, 4, 13, -2, 10, 6, 7, 8, 9, -3, 12, 11, 0, 1], group_type = 0 : i64, hsecs = 3 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [1], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x256x14x20x!quant.uniform<i8:f32, 0.10009854960629921>, 609280 : i64>, tensor<1x64x28x40x!quant.uniform<i8:f32, 0.14614416141732284>, 358400 : i64>, tensor<1x128x28x40x!quant.uniform<i8:f32, 0.095037182677165349>, 465920 : i64>) -> (tensor<1x256x14x20x!quant.uniform<i8:f32, 0.088813670866141725>, 107520 : i64>, tensor<1x64x28x40x!quant.uniform<i8:f32, 0.18704145511811024>, 0 : i64>, tensor<1x128x28x40x!quant.uniform<i8:f32, 0.087665404724409451>, 215040 : i64>) loc(#loc818)
      %259 = "tpu.Slice"(%258#0, %0, %0, %0, %0) {axes = [], ends = [9223372036854775807, 128, 9223372036854775807, 9223372036854775807], hasparamConvert_axes = [1], offset = [0, 0, 0, 0], steps = [1, 1, 1, 1]} : (tensor<1x256x14x20x!quant.uniform<i8:f32, 0.088813670866141725>, 107520 : i64>, none, none, none, none) -> tensor<1x128x14x20x!quant.uniform<i8:f32, 0.088813670866141725>, 107520 : i64> loc(#loc520)
      %260 = "tpu.Slice"(%258#0, %0, %0, %0, %0) {axes = [], ends = [9223372036854775807, 256, 9223372036854775807, 9223372036854775807], hasparamConvert_axes = [1], offset = [0, 128, 0, 0], steps = [1, 1, 1, 1]} : (tensor<1x256x14x20x!quant.uniform<i8:f32, 0.088813670866141725>, 107520 : i64>, none, none, none, none) -> tensor<1x128x14x20x!quant.uniform<i8:f32, 0.088813670866141725>, 143360 : i64> loc(#loc521)
      %261 = "top.Weight"() : () -> tensor<1x64x1x9xi8, 1099519847056 : i64> loc(#loc522)
      %262 = "top.Weight"() : () -> tensor<1x64x9x128xsi8, 1099520950672 : i64> loc(#loc523)
      %263 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099513243696 : i64> loc(#loc524)
      %264 = "top.Weight"() : () -> tensor<1x128x1x9xi8, 1099519106256 : i64> loc(#loc525)
      %265 = "top.Weight"() : () -> tensor<1x128x1x128xsi8, 1099519124304 : i64> loc(#loc526)
      %266 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099519140688 : i64> loc(#loc527)
      %267:2 = "tpu.Group"(%260, %258#2) ({
        %395 = "tpu.Load"(%262) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 9216, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [128], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x9x128xsi8, 1099520950672 : i64>) -> tensor<1x64x9x128xsi8> loc(#loc530)
        %396 = "tpu.Load"(%260) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 6], h_slice = [8, 8], w_idx = [0], w_slice = [20], id = 1, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.088813670866141725>, 143360 : i64>) -> tensor<1x128x14x20x!quant.uniform<i8:f32, 0.088813670866141725>> loc(#loc531)
        %397 = "tpu.Load"(%261) {do_bcast = false, ginfo = #tpu.lg<out_addr = 11920, out_size = 72, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 2, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x9xi8, 1099519847056 : i64>) -> tensor<1x64x1x9xi8> loc(#loc532)
        %398 = "tpu.Load"(%258#2) {do_bcast = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 8960, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 14], h_slice = [14, 14], w_idx = [0], w_slice = [40], id = 3, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x28x40x!quant.uniform<i8:f32, 0.087665404724409451>, 215040 : i64>) -> tensor<1x128x28x40x!quant.uniform<i8:f32, 0.087665404724409451>> loc(#loc533)
        %399 = "tpu.Load"(%263) {do_bcast = true, ginfo = #tpu.lg<out_addr = 11520, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 4, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099513243696 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc534)
        %400 = "tpu.Conv2D"(%396, %395, %397) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 30208, out_size = 1152, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 7], h_slice = [7, 7], w_idx = [0], w_slice = [20], id = 5, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.088813670866141725>>, tensor<1x64x9x128xsi8>, tensor<1x64x1x9xi8>) -> tensor<1x64x14x20x!quant.uniform<i8:f32, 0.081290147244094488>> loc(#loc535)
        %401 = "tpu.Load"(%265) {do_bcast = false, ginfo = #tpu.lg<out_addr = 9472, out_size = 2048, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [128], id = 6, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x128xsi8, 1099519124304 : i64>) -> tensor<1x128x1x128xsi8> loc(#loc536)
        %402 = "tpu.Load"(%264) {do_bcast = false, ginfo = #tpu.lg<out_addr = 11776, out_size = 144, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 7, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x9xi8, 1099519106256 : i64>) -> tensor<1x128x1x9xi8> loc(#loc537)
        %403 = "tpu.Lut"(%398, %399) {ginfo = #tpu.lg<out_addr = 21248, out_size = 8960, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 14], h_slice = [14, 14], w_idx = [0], w_slice = [40], id = 8, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x128x28x40x!quant.uniform<i8:f32, 0.087665404724409451>>, tensor<1x1x1x256xsi8>) -> tensor<1x128x28x40x!quant.uniform<i8:f32, 0.085545637007874014>> loc(#loc538)
        %404 = "tpu.Load"(%266) {do_bcast = true, ginfo = #tpu.lg<out_addr = 9216, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 9, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099519140688 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc539)
        %405 = "tpu.Conv2D"(%403, %401, %402) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 8960, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 14], h_slice = [14, 14], w_idx = [0], w_slice = [40], id = 10, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x28x40x!quant.uniform<i8:f32, 0.085545637007874014>>, tensor<1x128x1x128xsi8>, tensor<1x128x1x9xi8>) -> tensor<1x128x28x40x!quant.uniform<i8:f32, 0.18496433779527557>> loc(#loc528)
        %406 = "tpu.Store"(%405, %0) {ginfo = #tpu.lg<out_addr = 12288, out_size = 8960, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 14], h_slice = [14, 14], w_idx = [0], w_slice = [40], id = 11, stage = 1, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x128x28x40x!quant.uniform<i8:f32, 0.18496433779527557>>, none) -> tensor<1x128x28x40x!quant.uniform<i8:f32, 0.18496433779527557>, 376320 : i64> loc(#loc528)
        %407 = "tpu.Lut"(%400, %404) {ginfo = #tpu.lg<out_addr = 28672, out_size = 1152, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 7], h_slice = [7, 7], w_idx = [0], w_slice = [20], id = 12, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.081290147244094488>>, tensor<1x1x1x256xsi8>) -> tensor<1x64x14x20x!quant.uniform<i8:f32, 0.046340440944881892>> loc(#loc529)
        %408 = "tpu.Store"(%407, %0) {ginfo = #tpu.lg<out_addr = 28672, out_size = 1152, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 7], h_slice = [7, 7], w_idx = [0], w_slice = [20], id = 13, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.046340440944881892>>, none) -> tensor<1x64x14x20x!quant.uniform<i8:f32, 0.046340440944881892>, 358400 : i64> loc(#loc529)
        "tpu.Yield"(%406, %408) : (tensor<1x128x28x40x!quant.uniform<i8:f32, 0.18496433779527557>, 376320 : i64>, tensor<1x64x14x20x!quant.uniform<i8:f32, 0.046340440944881892>, 358400 : i64>) -> () loc(#loc819)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 5, 3, 4, 13, -2, 8, 6, 7, -3, 10, 9, 0, -4, 12, 11, 1, 2], group_type = 0 : i64, hsecs = 2 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.088813670866141725>, 143360 : i64>, tensor<1x128x28x40x!quant.uniform<i8:f32, 0.087665404724409451>, 215040 : i64>) -> (tensor<1x128x28x40x!quant.uniform<i8:f32, 0.18496433779527557>, 376320 : i64>, tensor<1x64x14x20x!quant.uniform<i8:f32, 0.046340440944881892>, 358400 : i64>) loc(#loc819)
      %268 = "top.Weight"() : () -> tensor<1x128x1x9xi8, 1099519140944 : i64> loc(#loc540)
      %269 = "top.Weight"() : () -> tensor<1x128x9x64xsi8, 1099519179216 : i64> loc(#loc541)
      %270 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099519252944 : i64> loc(#loc542)
      %271 = "top.Weight"() : () -> tensor<1x4x1x9xi8, 1099521173008 : i64> loc(#loc543)
      %272 = "top.Weight"() : () -> tensor<1x4x1x128xsi8, 1099519253200 : i64> loc(#loc544)
      %273:2 = "tpu.Group"(%267#1, %267#0) ({
        %395 = "tpu.Load"(%269) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 9216, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [64], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x9x64xsi8, 1099519179216 : i64>) -> tensor<1x128x9x64xsi8> loc(#loc547)
        %396 = "tpu.Load"(%267#1) {do_bcast = false, ginfo = #tpu.lg<out_addr = 30208, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 6], h_slice = [8, 8], w_idx = [0], w_slice = [20], id = 1, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.046340440944881892>, 358400 : i64>) -> tensor<1x64x14x20x!quant.uniform<i8:f32, 0.046340440944881892>> loc(#loc548)
        %397 = "tpu.Load"(%268) {do_bcast = false, ginfo = #tpu.lg<out_addr = 31744, out_size = 144, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 2, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x9xi8, 1099519140944 : i64>) -> tensor<1x128x1x9xi8> loc(#loc549)
        %398 = "tpu.Load"(%267#0) {do_bcast = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 8960, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 14], h_slice = [14, 14], w_idx = [0], w_slice = [40], id = 3, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x28x40x!quant.uniform<i8:f32, 0.18496433779527557>, 376320 : i64>) -> tensor<1x128x28x40x!quant.uniform<i8:f32, 0.18496433779527557>> loc(#loc550)
        %399 = "tpu.Load"(%270) {do_bcast = true, ginfo = #tpu.lg<out_addr = 31488, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 4, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099519252944 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc551)
        %400 = "tpu.Conv2D"(%396, %395, %397) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 9344, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 7], h_slice = [7, 7], w_idx = [0], w_slice = [20], id = 5, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.046340440944881892>>, tensor<1x128x9x64xsi8>, tensor<1x128x1x9xi8>) -> tensor<1x128x14x20x!quant.uniform<i8:f32, 0.12029746929133858>> loc(#loc545)
        %401 = "tpu.Load"(%272) {do_bcast = false, ginfo = #tpu.lg<out_addr = 9216, out_size = 128, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [4], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [128], id = 6, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x4x1x128xsi8, 1099519253200 : i64>) -> tensor<1x4x1x128xsi8> loc(#loc552)
        %402 = "tpu.Load"(%271) {do_bcast = false, ginfo = #tpu.lg<out_addr = 12208, out_size = 9, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [4], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 7, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x4x1x9xi8, 1099521173008 : i64>) -> tensor<1x4x1x9xi8> loc(#loc553)
        %403 = "tpu.Store"(%400, %0) {ginfo = #tpu.lg<out_addr = 9344, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 7], h_slice = [7, 7], w_idx = [0], w_slice = [20], id = 8, stage = 1, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.12029746929133858>>, none) -> tensor<1x128x14x20x!quant.uniform<i8:f32, 0.12029746929133858>, 219520 : i64> loc(#loc545)
        %404 = "tpu.Lut"(%398, %399) {ginfo = #tpu.lg<out_addr = 21248, out_size = 8960, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 14], h_slice = [14, 14], w_idx = [0], w_slice = [40], id = 9, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x128x28x40x!quant.uniform<i8:f32, 0.18496433779527557>>, tensor<1x1x1x256xsi8>) -> tensor<1x128x28x40x!quant.uniform<i8:f32, 0.12700925905511812>> loc(#loc554)
        %405 = "tpu.Conv2D"(%404, %401, %402) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 11648, out_size = 560, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [4], d_idx = [0], d_slice = [1], h_idx = [0, 14], h_slice = [14, 14], w_idx = [0], w_slice = [40], id = 10, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x28x40x!quant.uniform<i8:f32, 0.12700925905511812>>, tensor<1x4x1x128xsi8>, tensor<1x4x1x9xi8>) -> tensor<1x4x28x40x!quant.uniform<i8:f32, 0.18704145511811024>> loc(#loc546)
        %406 = "tpu.Store"(%405, %0) {ginfo = #tpu.lg<out_addr = 11648, out_size = 560, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [4], d_idx = [0], d_slice = [1], h_idx = [0, 14], h_slice = [14, 14], w_idx = [0], w_slice = [40], id = 11, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x4x28x40x!quant.uniform<i8:f32, 0.18704145511811024>>, none) -> tensor<1x4x28x40x!quant.uniform<i8:f32, 0.18704145511811024>, 215040 : i64> loc(#loc546)
        "tpu.Yield"(%403, %406) : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.12029746929133858>, 219520 : i64>, tensor<1x4x28x40x!quant.uniform<i8:f32, 0.18704145511811024>, 215040 : i64>) -> () loc(#loc820)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 5, 3, 4, 11, -2, 9, 6, 7, 8, 0, -3, 10, 1, 2], group_type = 0 : i64, hsecs = 2 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.046340440944881892>, 358400 : i64>, tensor<1x128x28x40x!quant.uniform<i8:f32, 0.18496433779527557>, 376320 : i64>) -> (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.12029746929133858>, 219520 : i64>, tensor<1x4x28x40x!quant.uniform<i8:f32, 0.18704145511811024>, 215040 : i64>) loc(#loc820)
      %274 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099519253712 : i64> loc(#loc555)
      %275:2 = "tpu.Group"(%273#0, %258#1, %273#1, %260) ({
        %395 = "tpu.Load"(%273#0) {do_bcast = false, ginfo = #tpu.lg<out_addr = 5040, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 7], h_slice = [7, 7], w_idx = [0], w_slice = [20], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.12029746929133858>, 219520 : i64>) -> tensor<1x128x14x20x!quant.uniform<i8:f32, 0.12029746929133858>> loc(#loc558)
        %396 = "tpu.Load"(%274) {do_bcast = true, ginfo = #tpu.lg<out_addr = 28672, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 1, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099519253712 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc559)
        %397 = "tpu.Load"(%258#1) {do_bcast = false, ginfo = #tpu.lg<out_addr = 8192, out_size = 4480, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 14], h_slice = [14, 14], w_idx = [0], w_slice = [40], id = 2, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.18704145511811024>, 0 : i64>) -> tensor<1x64x28x40x!quant.uniform<i8:f32, 0.18704145511811024>> loc(#loc560)
        %398 = "tpu.Load"(%273#1) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 560, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [4], d_idx = [0], d_slice = [1], h_idx = [0, 14], h_slice = [14, 14], w_idx = [0], w_slice = [40], id = 3, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x4x28x40x!quant.uniform<i8:f32, 0.18704145511811024>, 215040 : i64>) -> tensor<1x4x28x40x!quant.uniform<i8:f32, 0.18704145511811024>> loc(#loc561)
        %399 = "tpu.Lut"(%395, %396) {ginfo = #tpu.lg<out_addr = 16384, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 7], h_slice = [7, 7], w_idx = [0], w_slice = [20], id = 4, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.12029746929133858>>, tensor<1x1x1x256xsi8>) -> tensor<1x128x14x20x!quant.uniform<i8:f32, 0.087472546456692917>> loc(#loc562)
        %400 = "tpu.Load"(%260) {do_bcast = false, ginfo = #tpu.lg<out_addr = 20480, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 7], h_slice = [7, 7], w_idx = [0], w_slice = [20], id = 5, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.088813670866141725>, 143360 : i64>) -> tensor<1x128x14x20x!quant.uniform<i8:f32, 0.088813670866141725>> loc(#loc531)
        %401 = "tpu.Concat"(%397, %398) {axis = 1 : si32, do_relu = false, ginfo = #tpu.lg<out_addr = 0, out_size = 5040, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [68], d_idx = [0], d_slice = [1], h_idx = [0, 14], h_slice = [14, 14], w_idx = [0], w_slice = [40], id = 6, stage = 1, slice_idx = 0, group_type = 0>, multipliers = [1, 1], only_merge = true, relu_limit = -1.000000e+00 : f64, rshifts = [0, 0]} : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.18704145511811024>>, tensor<1x4x28x40x!quant.uniform<i8:f32, 0.18704145511811024>>) -> tensor<1x68x28x40x!quant.uniform<i8:f32, 0.18704145511811024>> loc(#loc556)
        %402 = "tpu.Store"(%401, %0) {ginfo = #tpu.lg<out_addr = 0, out_size = 5040, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [68], d_idx = [0], d_slice = [1], h_idx = [0, 14], h_slice = [14, 14], w_idx = [0], w_slice = [40], id = 7, stage = 1, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x68x28x40x!quant.uniform<i8:f32, 0.18704145511811024>>, none) -> tensor<1x68x28x40x!quant.uniform<i8:f32, 0.18704145511811024>, 366528 : i64> loc(#loc556)
        %403 = "tpu.Add"(%400, %399) {do_relu = false, ginfo = #tpu.lg<out_addr = 12672, out_size = 2304, buffer_addr = 8192, buffer_size = 2304, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 7], h_slice = [7, 7], w_idx = [0], w_slice = [20], id = 8, stage = 1, slice_idx = 0, group_type = 0>, multipliers = [90, 89], relu_limit = -1.000000e+00 : f64, rshifts = [7]} : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.088813670866141725>>, tensor<1x128x14x20x!quant.uniform<i8:f32, 0.087472546456692917>>) -> tensor<1x128x14x20x!quant.uniform<i8:f32, 0.12498954724409449>> loc(#loc557)
        %404 = "tpu.Store"(%403, %0) {ginfo = #tpu.lg<out_addr = 12672, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 7], h_slice = [7, 7], w_idx = [0], w_slice = [20], id = 9, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.12498954724409449>>, none) -> tensor<1x128x14x20x!quant.uniform<i8:f32, 0.12498954724409449>, 255360 : i64> loc(#loc557)
        "tpu.Yield"(%402, %404) : (tensor<1x68x28x40x!quant.uniform<i8:f32, 0.18704145511811024>, 366528 : i64>, tensor<1x128x14x20x!quant.uniform<i8:f32, 0.12498954724409449>, 255360 : i64>) -> () loc(#loc821)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 4, 2, 3, 9, -2, 6, 5, -3, 8, 7, 0, 1], group_type = 0 : i64, hsecs = 2 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.12029746929133858>, 219520 : i64>, tensor<1x64x28x40x!quant.uniform<i8:f32, 0.18704145511811024>, 0 : i64>, tensor<1x4x28x40x!quant.uniform<i8:f32, 0.18704145511811024>, 215040 : i64>, tensor<1x128x14x20x!quant.uniform<i8:f32, 0.088813670866141725>, 143360 : i64>) -> (tensor<1x68x28x40x!quant.uniform<i8:f32, 0.18704145511811024>, 366528 : i64>, tensor<1x128x14x20x!quant.uniform<i8:f32, 0.12498954724409449>, 255360 : i64>) loc(#loc821)
      %276 = "tpu.Reshape"(%275#0) {flatten_start_dim = -1 : i64, shape = [1, 68, -1]} : (tensor<1x68x28x40x!quant.uniform<i8:f32, 0.18704145511811024>, 366528 : i64>) -> tensor<1x68x1120x!quant.uniform<i8:f32, 0.18704145511811024>, 366528 : i64> loc(#loc563)
      %277 = "tpu.Concat"(%259, %260, %275#1) {axis = 1 : si32, do_relu = false, multipliers = [63, 63, 89], only_merge = false, relu_limit = -1.000000e+00 : f64, rshifts = [6, 6, 6]} : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.088813670866141725>, 107520 : i64>, tensor<1x128x14x20x!quant.uniform<i8:f32, 0.088813670866141725>, 143360 : i64>, tensor<1x128x14x20x!quant.uniform<i8:f32, 0.12498954724409449>, 255360 : i64>) -> tensor<1x384x14x20x!quant.uniform<i8:f32, 0.089340173228346462>, 0 : i64> loc(#loc564)
      %278 = "top.Weight"() : () -> tensor<1x256x1x9xi8, 1099519253968 : i64> loc(#loc565)
      %279 = "top.Weight"() : () -> tensor<1x256x1x384xsi8, 1099513341056 : i64> loc(#loc566)
      %280 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099519256272 : i64> loc(#loc567)
      %281 = "tpu.Group"(%277) ({
        %395 = "tpu.Load"(%277) {do_bcast = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 6912, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [384], d_idx = [0], d_slice = [1], h_idx = [0, 7], h_slice = [7, 7], w_idx = [0], w_slice = [20], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x384x14x20x!quant.uniform<i8:f32, 0.089340173228346462>, 0 : i64>) -> tensor<1x384x14x20x!quant.uniform<i8:f32, 0.089340173228346462>> loc(#loc569)
        %396 = "tpu.Load"(%279) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 12288, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [384], id = 1, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x1x384xsi8, 1099513341056 : i64>) -> tensor<1x256x1x384xsi8> loc(#loc570)
        %397 = "tpu.Load"(%278) {do_bcast = false, ginfo = #tpu.lg<out_addr = 19200, out_size = 288, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 2, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x1x9xi8, 1099519253968 : i64>) -> tensor<1x256x1x9xi8> loc(#loc571)
        %398 = "tpu.Load"(%280) {do_bcast = true, ginfo = #tpu.lg<out_addr = 19488, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 3, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099519256272 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc572)
        %399 = "tpu.Conv2D"(%395, %396, %397) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 20480, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0, 7], h_slice = [7, 7], w_idx = [0], w_slice = [20], id = 4, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x384x14x20x!quant.uniform<i8:f32, 0.089340173228346462>>, tensor<1x256x1x384xsi8>, tensor<1x256x1x9xi8>) -> tensor<1x256x14x20x!quant.uniform<i8:f32, 0.097046754330708667>> loc(#loc573)
        %400 = "tpu.Lut"(%399, %398) {ginfo = #tpu.lg<out_addr = 25088, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0, 7], h_slice = [7, 7], w_idx = [0], w_slice = [20], id = 5, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x256x14x20x!quant.uniform<i8:f32, 0.097046754330708667>>, tensor<1x1x1x256xsi8>) -> tensor<1x256x14x20x!quant.uniform<i8:f32, 0.089820559055118113>> loc(#loc568)
        %401 = "tpu.Store"(%400, %0) {ginfo = #tpu.lg<out_addr = 25088, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0, 7], h_slice = [7, 7], w_idx = [0], w_slice = [20], id = 6, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x256x14x20x!quant.uniform<i8:f32, 0.089820559055118113>>, none) -> tensor<1x256x14x20x!quant.uniform<i8:f32, 0.089820559055118113>, 215040 : i64> loc(#loc568)
        "tpu.Yield"(%401) : (tensor<1x256x14x20x!quant.uniform<i8:f32, 0.089820559055118113>, 215040 : i64>) -> () loc(#loc568)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 4, 3, 6, -2, 5, 0, 1, 2], group_type = 0 : i64, hsecs = 2 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x384x14x20x!quant.uniform<i8:f32, 0.089340173228346462>, 0 : i64>) -> tensor<1x256x14x20x!quant.uniform<i8:f32, 0.089820559055118113>, 215040 : i64> loc(#loc568)
      %282 = "top.Weight"() {do_compress = true} : () -> tensor<1x256x1x9xi8, 1099519103952 : i64> loc(#loc574)
      %283 = "top.Weight"() {do_compress = true} : () -> tensor<1x256x9x256xsi8, 1099517513376 : i64> loc(#loc575)
      %284 = "tpu.Conv2D"(%281, %283, %282) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [2, 2], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x256x14x20x!quant.uniform<i8:f32, 0.089820559055118113>, 215040 : i64>, tensor<1x256x9x256xsi8, 1099517513376 : i64>, tensor<1x256x1x9xi8, 1099519103952 : i64>) -> tensor<1x256x7x10x!quant.uniform<i8:f32, 0.085836335433070857>, 0 : i64> loc(#loc576)
      %285 = "top.Weight"() : () -> tensor<1x64x1x9xi8, 1099519294544 : i64> loc(#loc577)
      %286 = "top.Weight"() : () -> tensor<1x64x9x256xsi8, 1099513439360 : i64> loc(#loc578)
      %287 = "top.Weight"() : () -> tensor<1x256x1x9xi8, 1099519295632 : i64> loc(#loc579)
      %288 = "top.Weight"() : () -> tensor<1x256x3x3xsi8, 1099519297936 : i64> loc(#loc580)
      %289:2 = "tpu.Group"(%281) ({
        %395 = "tpu.Load"(%286) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 18432, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [256], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x9x256xsi8, 1099513439360 : i64>) -> tensor<1x64x9x256xsi8> loc(#loc583)
        %396 = "tpu.Load"(%285) {do_bcast = false, ginfo = #tpu.lg<out_addr = 20384, out_size = 72, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 1, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x9xi8, 1099519294544 : i64>) -> tensor<1x64x1x9xi8> loc(#loc584)
        %397 = "tpu.Load"(%281) {do_bcast = false, ginfo = #tpu.lg<out_addr = 20480, out_size = 5120, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0, 6], h_slice = [8, 8], w_idx = [0], w_slice = [20], id = 2, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x14x20x!quant.uniform<i8:f32, 0.089820559055118113>, 215040 : i64>) -> tensor<1x256x14x20x!quant.uniform<i8:f32, 0.089820559055118113>> loc(#loc585)
        %398 = "tpu.Load"(%288) {do_bcast = false, ginfo = #tpu.lg<out_addr = 19584, out_size = 512, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [3], w_idx = [0], w_slice = [3], id = 3, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x3x3xsi8, 1099519297936 : i64>) -> tensor<1x256x3x3xsi8> loc(#loc586)
        %399 = "tpu.Load"(%287) {do_bcast = false, ginfo = #tpu.lg<out_addr = 20096, out_size = 288, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 4, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x1x9xi8, 1099519295632 : i64>) -> tensor<1x256x1x9xi8> loc(#loc587)
        %400 = "tpu.Conv2D"(%397, %395, %396) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 18432, out_size = 1152, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 7], h_slice = [7, 7], w_idx = [0], w_slice = [20], id = 5, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x256x14x20x!quant.uniform<i8:f32, 0.089820559055118113>>, tensor<1x64x9x256xsi8>, tensor<1x64x1x9xi8>) -> tensor<1x64x14x20x!quant.uniform<i8:f32, 0.10876133700787401>> loc(#loc581)
        %401 = "tpu.Store"(%400, %0) {ginfo = #tpu.lg<out_addr = 18432, out_size = 1152, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 7], h_slice = [7, 7], w_idx = [0], w_slice = [20], id = 6, stage = 1, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.10876133700787401>>, none) -> tensor<1x64x14x20x!quant.uniform<i8:f32, 0.10876133700787401>, 89600 : i64> loc(#loc581)
        %402 = "tpu.Conv2D"(%397, %398, %399) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 25600, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0, 7], h_slice = [7, 7], w_idx = [0], w_slice = [20], id = 7, stage = 1, slice_idx = 0, group_type = 0>, group = 256 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x256x14x20x!quant.uniform<i8:f32, 0.089820559055118113>>, tensor<1x256x3x3xsi8>, tensor<1x256x1x9xi8>) -> tensor<1x256x14x20x!quant.uniform<i8:f32, 0.085803107086614164>> loc(#loc582)
        %403 = "tpu.Store"(%402, %0) {ginfo = #tpu.lg<out_addr = 25600, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0, 7], h_slice = [7, 7], w_idx = [0], w_slice = [20], id = 8, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x256x14x20x!quant.uniform<i8:f32, 0.085803107086614164>>, none) -> tensor<1x256x14x20x!quant.uniform<i8:f32, 0.085803107086614164>, 17920 : i64> loc(#loc582)
        "tpu.Yield"(%401, %403) : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.10876133700787401>, 89600 : i64>, tensor<1x256x14x20x!quant.uniform<i8:f32, 0.085803107086614164>, 17920 : i64>) -> () loc(#loc822)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 5, 3, 4, 8, -2, 7, 6, 0, 1, -3, 2], group_type = 0 : i64, hsecs = 2 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x256x14x20x!quant.uniform<i8:f32, 0.089820559055118113>, 215040 : i64>) -> (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.10876133700787401>, 89600 : i64>, tensor<1x256x14x20x!quant.uniform<i8:f32, 0.085803107086614164>, 17920 : i64>) loc(#loc822)
      %290 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099519300816 : i64> loc(#loc588)
      %291 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099519301072 : i64> loc(#loc589)
      %292 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099512832144 : i64> loc(#loc590)
      %293 = "top.Weight"() : () -> tensor<1x64x1x9xi8, 1099520440784 : i64> loc(#loc591)
      %294 = "top.Weight"() : () -> tensor<1x64x9x64xsi8, 1099519142096 : i64> loc(#loc592)
      %295 = "top.Weight"() : () -> tensor<1x128x1x9xi8, 1099519293392 : i64> loc(#loc593)
      %296 = "top.Weight"() : () -> tensor<1x128x1x256xsi8, 1099519338768 : i64> loc(#loc594)
      %297:3 = "tpu.Group"(%284, %289#0, %289#1, %188) ({
        %395 = "tpu.Load"(%284) {do_bcast = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 1536, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0, 4], h_slice = [4, 3], w_idx = [0], w_slice = [10], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x7x10x!quant.uniform<i8:f32, 0.085836335433070857>, 0 : i64>) -> tensor<1x256x7x10x!quant.uniform<i8:f32, 0.085836335433070857>> loc(#loc598)
        %396 = "tpu.Load"(%290) {do_bcast = true, ginfo = #tpu.lg<out_addr = 19408, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 1, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099519300816 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc599)
        %397 = "tpu.Load"(%289#0) {do_bcast = false, ginfo = #tpu.lg<out_addr = 4608, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 6], h_slice = [8, 8], w_idx = [0], w_slice = [20], id = 2, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.10876133700787401>, 89600 : i64>) -> tensor<1x64x14x20x!quant.uniform<i8:f32, 0.10876133700787401>> loc(#loc600)
        %398 = "tpu.Load"(%291) {do_bcast = true, ginfo = #tpu.lg<out_addr = 31744, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 3, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099519301072 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc601)
        %399 = "tpu.Lut"(%395, %396) {ginfo = #tpu.lg<out_addr = 24576, out_size = 1536, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0, 4], h_slice = [4, 3], w_idx = [0], w_slice = [10], id = 4, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x256x7x10x!quant.uniform<i8:f32, 0.085836335433070857>>, tensor<1x1x1x256xsi8>) -> tensor<1x256x7x10x!quant.uniform<i8:f32, 0.052144893700787398>> loc(#loc602)
        %400 = "tpu.Load"(%289#1) {do_bcast = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0, 7], h_slice = [7, 7], w_idx = [0], w_slice = [20], id = 5, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x14x20x!quant.uniform<i8:f32, 0.085803107086614164>, 17920 : i64>) -> tensor<1x256x14x20x!quant.uniform<i8:f32, 0.085803107086614164>> loc(#loc603)
        %401 = "tpu.Load"(%292) {do_bcast = true, ginfo = #tpu.lg<out_addr = 19072, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 6, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099512832144 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc604)
        %402 = "tpu.Lut"(%397, %398) {ginfo = #tpu.lg<out_addr = 26112, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 6], h_slice = [8, 8], w_idx = [0], w_slice = [20], id = 7, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.10876133700787401>>, tensor<1x1x1x256xsi8>) -> tensor<1x64x14x20x!quant.uniform<i8:f32, 0.066716822834645667>> loc(#loc605)
        %403 = "tpu.Load"(%188) {do_bcast = false, ginfo = #tpu.lg<out_addr = 28672, out_size = 3072, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [512], d_idx = [0], d_slice = [1], h_idx = [0, 4], h_slice = [4, 3], w_idx = [0], w_slice = [10], id = 8, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x512x7x10x!quant.uniform<i8:f32, 0.02761857559055118>, 179200 : i64>) -> tensor<1x512x7x10x!quant.uniform<i8:f32, 0.02761857559055118>> loc(#loc369)
        %404 = "tpu.Lut"(%400, %401) {ginfo = #tpu.lg<out_addr = 4608, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0, 7], h_slice = [7, 7], w_idx = [0], w_slice = [20], id = 9, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x256x14x20x!quant.uniform<i8:f32, 0.085803107086614164>>, tensor<1x1x1x256xsi8>) -> tensor<1x256x14x20x!quant.uniform<i8:f32, 0.063137324409448817>> loc(#loc606)
        %405 = "tpu.Load"(%294) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [64], id = 10, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x9x64xsi8, 1099519142096 : i64>) -> tensor<1x64x9x64xsi8> loc(#loc607)
        %406 = "tpu.Load"(%293) {do_bcast = false, ginfo = #tpu.lg<out_addr = 19328, out_size = 72, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 11, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x9xi8, 1099520440784 : i64>) -> tensor<1x64x1x9xi8> loc(#loc608)
        %407 = "tpu.Concat"(%399, %403) {axis = 1 : si32, do_relu = false, ginfo = #tpu.lg<out_addr = 9216, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [768], d_idx = [0], d_slice = [1], h_idx = [0, 4], h_slice = [4, 3], w_idx = [0], w_slice = [10], id = 12, stage = 1, slice_idx = 0, group_type = 0>, multipliers = [118, 125], only_merge = false, relu_limit = -1.000000e+00 : f64, rshifts = [7, 8]} : (tensor<1x256x7x10x!quant.uniform<i8:f32, 0.052144893700787398>>, tensor<1x512x7x10x!quant.uniform<i8:f32, 0.02761857559055118>>) -> tensor<1x768x7x10x!quant.uniform<i8:f32, 0.056201353543307089>> loc(#loc595)
        %408 = "tpu.Load"(%296) {do_bcast = false, ginfo = #tpu.lg<out_addr = 20480, out_size = 4096, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 13, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x256xsi8, 1099519338768 : i64>) -> tensor<1x128x1x256xsi8> loc(#loc609)
        %409 = "tpu.Load"(%295) {do_bcast = false, ginfo = #tpu.lg<out_addr = 32000, out_size = 144, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 14, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x9xi8, 1099519293392 : i64>) -> tensor<1x128x1x9xi8> loc(#loc610)
        %410 = "tpu.Store"(%407, %0) {ginfo = #tpu.lg<out_addr = 9216, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [768], d_idx = [0], d_slice = [1], h_idx = [0, 4], h_slice = [4, 3], w_idx = [0], w_slice = [10], id = 15, stage = 1, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x768x7x10x!quant.uniform<i8:f32, 0.056201353543307089>>, none) -> tensor<1x768x7x10x!quant.uniform<i8:f32, 0.056201353543307089>, 232960 : i64> loc(#loc595)
        %411 = "tpu.Conv2D"(%402, %405, %406) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 17920, out_size = 1152, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 7], h_slice = [7, 7], w_idx = [0], w_slice = [20], id = 16, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.066716822834645667>>, tensor<1x64x9x64xsi8>, tensor<1x64x1x9xi8>) -> tensor<1x64x14x20x!quant.uniform<i8:f32, 0.18506054645669293>> loc(#loc596)
        %412 = "tpu.Store"(%411, %0) {ginfo = #tpu.lg<out_addr = 17920, out_size = 1152, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 7], h_slice = [7, 7], w_idx = [0], w_slice = [20], id = 17, stage = 1, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.18506054645669293>>, none) -> tensor<1x64x14x20x!quant.uniform<i8:f32, 0.18506054645669293>, 215040 : i64> loc(#loc596)
        %413 = "tpu.Conv2D"(%404, %408, %409) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 7], h_slice = [7, 7], w_idx = [0], w_slice = [20], id = 18, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x256x14x20x!quant.uniform<i8:f32, 0.063137324409448817>>, tensor<1x128x1x256xsi8>, tensor<1x128x1x9xi8>) -> tensor<1x128x14x20x!quant.uniform<i8:f32, 0.086934870866141725>> loc(#loc597)
        %414 = "tpu.Store"(%413, %0) {ginfo = #tpu.lg<out_addr = 12288, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 7], h_slice = [7, 7], w_idx = [0], w_slice = [20], id = 19, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.086934870866141725>>, none) -> tensor<1x128x14x20x!quant.uniform<i8:f32, 0.086934870866141725>, 107520 : i64> loc(#loc597)
        "tpu.Yield"(%410, %412, %414) : (tensor<1x768x7x10x!quant.uniform<i8:f32, 0.056201353543307089>, 232960 : i64>, tensor<1x64x14x20x!quant.uniform<i8:f32, 0.18506054645669293>, 215040 : i64>, tensor<1x128x14x20x!quant.uniform<i8:f32, 0.086934870866141725>, 107520 : i64>) -> () loc(#loc823)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 4, 2, 3, 19, -2, 7, 5, 6, -3, 9, 8, -4, 12, 10, 11, -5, 16, 13, 14, 15, -6, 18, 17, 0, 1], group_type = 0 : i64, hsecs = 2 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x256x7x10x!quant.uniform<i8:f32, 0.085836335433070857>, 0 : i64>, tensor<1x64x14x20x!quant.uniform<i8:f32, 0.10876133700787401>, 89600 : i64>, tensor<1x256x14x20x!quant.uniform<i8:f32, 0.085803107086614164>, 17920 : i64>, tensor<1x512x7x10x!quant.uniform<i8:f32, 0.02761857559055118>, 179200 : i64>) -> (tensor<1x768x7x10x!quant.uniform<i8:f32, 0.056201353543307089>, 232960 : i64>, tensor<1x64x14x20x!quant.uniform<i8:f32, 0.18506054645669293>, 215040 : i64>, tensor<1x128x14x20x!quant.uniform<i8:f32, 0.086934870866141725>, 107520 : i64>) loc(#loc823)
      %298 = "top.Weight"() {do_compress = true} : () -> tensor<1x512x1x9xi8, 1099519371536 : i64> loc(#loc611)
      %299 = "top.Weight"() {do_compress = true} : () -> tensor<1x512x1x768xsi8, 1099515123440 : i64> loc(#loc612)
      %300 = "tpu.Conv2D"(%297#0, %299, %298) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x768x7x10x!quant.uniform<i8:f32, 0.056201353543307089>, 232960 : i64>, tensor<1x512x1x768xsi8, 1099515123440 : i64>, tensor<1x512x1x9xi8, 1099519371536 : i64>) -> tensor<1x512x7x10x!quant.uniform<i8:f32, 0.097153814173228353>, 0 : i64> loc(#loc613)
      %301 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099516211824 : i64> loc(#loc614)
      %302 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099519376144 : i64> loc(#loc615)
      %303 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099519376400 : i64> loc(#loc616)
      %304 = "top.Weight"() : () -> tensor<1x64x1x9xi8, 1099519376656 : i64> loc(#loc617)
      %305 = "top.Weight"() : () -> tensor<1x64x1x64xsi8, 1099519377232 : i64> loc(#loc618)
      %306 = "top.Weight"() : () -> tensor<1x128x1x9xi8, 1099519811408 : i64> loc(#loc619)
      %307 = "top.Weight"() : () -> tensor<1x128x3x3xsi8, 1099512828176 : i64> loc(#loc620)
      %308:3 = "tpu.Group"(%297#1, %297#2, %300) ({
        %395 = "tpu.Load"(%297#1) {do_bcast = false, ginfo = #tpu.lg<out_addr = 18688, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [14], w_idx = [0], w_slice = [20], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.18506054645669293>, 215040 : i64>) -> tensor<1x64x14x20x!quant.uniform<i8:f32, 0.18506054645669293>> loc(#loc624)
        %396 = "tpu.Load"(%301) {do_bcast = true, ginfo = #tpu.lg<out_addr = 4864, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 1, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099516211824 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc625)
        %397 = "tpu.Load"(%297#2) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [14], w_idx = [0], w_slice = [20], id = 2, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.086934870866141725>, 107520 : i64>) -> tensor<1x128x14x20x!quant.uniform<i8:f32, 0.086934870866141725>> loc(#loc626)
        %398 = "tpu.Load"(%302) {do_bcast = true, ginfo = #tpu.lg<out_addr = 4608, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 3, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099519376144 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc627)
        %399 = "tpu.Lut"(%395, %396) {ginfo = #tpu.lg<out_addr = 12800, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [14], w_idx = [0], w_slice = [20], id = 4, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.18506054645669293>>, tensor<1x1x1x256xsi8>) -> tensor<1x64x14x20x!quant.uniform<i8:f32, 0.14534915748031496>> loc(#loc628)
        %400 = "tpu.Load"(%300) {do_bcast = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 5120, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [512], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 5, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x512x7x10x!quant.uniform<i8:f32, 0.097153814173228353>, 0 : i64>) -> tensor<1x512x7x10x!quant.uniform<i8:f32, 0.097153814173228353>> loc(#loc629)
        %401 = "tpu.Load"(%303) {do_bcast = true, ginfo = #tpu.lg<out_addr = 15104, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 6, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099519376400 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc630)
        %402 = "tpu.Lut"(%397, %398) {ginfo = #tpu.lg<out_addr = 0, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [14], w_idx = [0], w_slice = [20], id = 7, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.086934870866141725>>, tensor<1x1x1x256xsi8>) -> tensor<1x128x14x20x!quant.uniform<i8:f32, 0.054128585826771651>> loc(#loc631)
        %403 = "tpu.Load"(%305) {do_bcast = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 512, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [64], id = 8, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x64xsi8, 1099519377232 : i64>) -> tensor<1x64x1x64xsi8> loc(#loc632)
        %404 = "tpu.Load"(%304) {do_bcast = false, ginfo = #tpu.lg<out_addr = 9728, out_size = 72, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 9, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x9xi8, 1099519376656 : i64>) -> tensor<1x64x1x9xi8> loc(#loc633)
        %405 = "tpu.Lut"(%400, %401) {ginfo = #tpu.lg<out_addr = 4608, out_size = 5120, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [512], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 10, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x512x7x10x!quant.uniform<i8:f32, 0.097153814173228353>>, tensor<1x1x1x256xsi8>) -> tensor<1x512x7x10x!quant.uniform<i8:f32, 0.068618894488188975>> loc(#loc621)
        %406 = "tpu.Load"(%307) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [3], w_idx = [0], w_slice = [3], id = 11, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x3x3xsi8, 1099512828176 : i64>) -> tensor<1x128x3x3xsi8> loc(#loc634)
        %407 = "tpu.Load"(%306) {do_bcast = false, ginfo = #tpu.lg<out_addr = 28672, out_size = 144, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 12, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x9xi8, 1099519811408 : i64>) -> tensor<1x128x1x9xi8> loc(#loc635)
        %408 = "tpu.Store"(%405, %0) {ginfo = #tpu.lg<out_addr = 4608, out_size = 5120, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [512], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 13, stage = 1, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x512x7x10x!quant.uniform<i8:f32, 0.068618894488188975>>, none) -> tensor<1x512x7x10x!quant.uniform<i8:f32, 0.068618894488188975>, 232960 : i64> loc(#loc621)
        %409 = "tpu.Conv2D"(%399, %403, %404) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [14], w_idx = [0], w_slice = [20], id = 14, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.14534915748031496>>, tensor<1x64x1x64xsi8>, tensor<1x64x1x9xi8>) -> tensor<1x64x14x20x!quant.uniform<i8:f32, 0.18704145511811024>> loc(#loc622)
        %410 = "tpu.Store"(%409, %0) {ginfo = #tpu.lg<out_addr = 16384, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [14], w_idx = [0], w_slice = [20], id = 15, stage = 1, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.18704145511811024>>, none) -> tensor<1x64x14x20x!quant.uniform<i8:f32, 0.18704145511811024>, 35840 : i64> loc(#loc622)
        %411 = "tpu.Conv2D"(%402, %406, %407) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 8192, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [14], w_idx = [0], w_slice = [20], id = 16, stage = 1, slice_idx = 0, group_type = 0>, group = 128 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.054128585826771651>>, tensor<1x128x3x3xsi8>, tensor<1x128x1x9xi8>) -> tensor<1x128x14x20x!quant.uniform<i8:f32, 0.092046457480314952>> loc(#loc623)
        %412 = "tpu.Store"(%411, %0) {ginfo = #tpu.lg<out_addr = 8192, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [14], w_idx = [0], w_slice = [20], id = 17, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.092046457480314952>>, none) -> tensor<1x128x14x20x!quant.uniform<i8:f32, 0.092046457480314952>, 143360 : i64> loc(#loc623)
        "tpu.Yield"(%408, %410, %412) : (tensor<1x512x7x10x!quant.uniform<i8:f32, 0.068618894488188975>, 232960 : i64>, tensor<1x64x14x20x!quant.uniform<i8:f32, 0.18704145511811024>, 35840 : i64>, tensor<1x128x14x20x!quant.uniform<i8:f32, 0.092046457480314952>, 143360 : i64>) -> () loc(#loc824)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 4, 2, 3, -2, 7, 5, 6, 17, -3, 10, 8, 9, -4, 14, 11, 12, 13, -5, 16, 15, 0, 1], group_type = 0 : i64, hsecs = 1 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.18506054645669293>, 215040 : i64>, tensor<1x128x14x20x!quant.uniform<i8:f32, 0.086934870866141725>, 107520 : i64>, tensor<1x512x7x10x!quant.uniform<i8:f32, 0.097153814173228353>, 0 : i64>) -> (tensor<1x512x7x10x!quant.uniform<i8:f32, 0.068618894488188975>, 232960 : i64>, tensor<1x64x14x20x!quant.uniform<i8:f32, 0.18704145511811024>, 35840 : i64>, tensor<1x128x14x20x!quant.uniform<i8:f32, 0.092046457480314952>, 143360 : i64>) loc(#loc824)
      %309 = "tpu.Slice"(%308#0, %0, %0, %0, %0) {axes = [], ends = [9223372036854775807, 256, 9223372036854775807, 9223372036854775807], hasparamConvert_axes = [1], offset = [0, 0, 0, 0], steps = [1, 1, 1, 1]} : (tensor<1x512x7x10x!quant.uniform<i8:f32, 0.068618894488188975>, 232960 : i64>, none, none, none, none) -> tensor<1x256x7x10x!quant.uniform<i8:f32, 0.068618894488188975>, 232960 : i64> loc(#loc636)
      %310 = "tpu.Slice"(%308#0, %0, %0, %0, %0) {axes = [], ends = [9223372036854775807, 512, 9223372036854775807, 9223372036854775807], hasparamConvert_axes = [1], offset = [0, 256, 0, 0], steps = [1, 1, 1, 1]} : (tensor<1x512x7x10x!quant.uniform<i8:f32, 0.068618894488188975>, 232960 : i64>, none, none, none, none) -> tensor<1x256x7x10x!quant.uniform<i8:f32, 0.068618894488188975>, 250880 : i64> loc(#loc637)
      %311 = "top.Weight"() : () -> tensor<1x128x1x9xi8, 1099519813136 : i64> loc(#loc638)
      %312 = "top.Weight"() : () -> tensor<1x128x1x256xsi8, 1099515090416 : i64> loc(#loc639)
      %313 = "top.Weight"() : () -> tensor<1x128x1x9xi8, 1099518345888 : i64> loc(#loc640)
      %314 = "top.Weight"() : () -> tensor<1x128x1x256xsi8, 1099519814288 : i64> loc(#loc641)
      %315:2 = "tpu.Group"(%310) ({
        %395 = "tpu.Load"(%312) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 4096, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x256xsi8, 1099515090416 : i64>) -> tensor<1x128x1x256xsi8> loc(#loc644)
        %396 = "tpu.Load"(%311) {do_bcast = false, ginfo = #tpu.lg<out_addr = 20480, out_size = 144, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 1, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x9xi8, 1099519813136 : i64>) -> tensor<1x128x1x9xi8> loc(#loc645)
        %397 = "tpu.Load"(%310) {do_bcast = false, ginfo = #tpu.lg<out_addr = 8192, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 2, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x7x10x!quant.uniform<i8:f32, 0.068618894488188975>, 250880 : i64>) -> tensor<1x256x7x10x!quant.uniform<i8:f32, 0.068618894488188975>> loc(#loc646)
        %398 = "tpu.Load"(%314) {do_bcast = false, ginfo = #tpu.lg<out_addr = 4096, out_size = 4096, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 3, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x256xsi8, 1099519814288 : i64>) -> tensor<1x128x1x256xsi8> loc(#loc647)
        %399 = "tpu.Load"(%313) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 144, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 4, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x9xi8, 1099518345888 : i64>) -> tensor<1x128x1x9xi8> loc(#loc648)
        %400 = "tpu.Conv2D"(%397, %395, %396) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 5, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x256x7x10x!quant.uniform<i8:f32, 0.068618894488188975>>, tensor<1x128x1x256xsi8>, tensor<1x128x1x9xi8>) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.05307607952755905>> loc(#loc642)
        %401 = "tpu.Store"(%400, %0) {ginfo = #tpu.lg<out_addr = 16384, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 6, stage = 1, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.05307607952755905>>, none) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.05307607952755905>, 62720 : i64> loc(#loc642)
        %402 = "tpu.Conv2D"(%397, %398, %399) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 7, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x256x7x10x!quant.uniform<i8:f32, 0.068618894488188975>>, tensor<1x128x1x256xsi8>, tensor<1x128x1x9xi8>) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.1182248874015748>> loc(#loc643)
        %403 = "tpu.Store"(%402, %0) {ginfo = #tpu.lg<out_addr = 12288, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 8, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.1182248874015748>>, none) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.1182248874015748>, 53760 : i64> loc(#loc643)
        "tpu.Yield"(%401, %403) : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.05307607952755905>, 62720 : i64>, tensor<1x128x7x10x!quant.uniform<i8:f32, 0.1182248874015748>, 53760 : i64>) -> () loc(#loc825)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 5, 3, 4, 8, -2, 7, 6, 0, 1, -3, 2], group_type = 0 : i64, hsecs = 1 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x256x7x10x!quant.uniform<i8:f32, 0.068618894488188975>, 250880 : i64>) -> (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.05307607952755905>, 62720 : i64>, tensor<1x128x7x10x!quant.uniform<i8:f32, 0.1182248874015748>, 53760 : i64>) loc(#loc825)
      %316 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099519847888 : i64> loc(#loc649)
      %317 = "top.Weight"() : () -> tensor<1x128x1x9xi8, 1099519848144 : i64> loc(#loc650)
      %318 = "top.Weight"() : () -> tensor<1x128x1x128xsi8, 1099518106080 : i64> loc(#loc651)
      %319 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099512804368 : i64> loc(#loc652)
      %320 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099519849296 : i64> loc(#loc653)
      %321 = "top.Weight"() : () -> tensor<1x128x1x9xi8, 1099514340272 : i64> loc(#loc654)
      %322 = "top.Weight"() : () -> tensor<1x128x9x128xsi8, 1099515516656 : i64> loc(#loc655)
      %323 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099520111696 : i64> loc(#loc656)
      %324 = "top.Weight"() : () -> tensor<1x4x1x9xi8, 1099517513072 : i64> loc(#loc657)
      %325 = "top.Weight"() : () -> tensor<1x4x1x128xsi8, 1099519295120 : i64> loc(#loc658)
      %326:4 = "tpu.Group"(%308#2, %315#0, %315#1) ({
        %395 = "tpu.Load"(%308#2) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [14], w_idx = [0], w_slice = [20], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.092046457480314952>, 143360 : i64>) -> tensor<1x128x14x20x!quant.uniform<i8:f32, 0.092046457480314952>> loc(#loc663)
        %396 = "tpu.Load"(%316) {do_bcast = true, ginfo = #tpu.lg<out_addr = 20480, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 1, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099519847888 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc664)
        %397 = "tpu.Load"(%318) {do_bcast = false, ginfo = #tpu.lg<out_addr = 29696, out_size = 2048, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [128], id = 2, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x128xsi8, 1099518106080 : i64>) -> tensor<1x128x1x128xsi8> loc(#loc665)
        %398 = "tpu.Load"(%317) {do_bcast = false, ginfo = #tpu.lg<out_addr = 19968, out_size = 144, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 3, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x9xi8, 1099519848144 : i64>) -> tensor<1x128x1x9xi8> loc(#loc666)
        %399 = "tpu.Lut"(%395, %396) {ginfo = #tpu.lg<out_addr = 25088, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [14], w_idx = [0], w_slice = [20], id = 4, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.092046457480314952>>, tensor<1x1x1x256xsi8>) -> tensor<1x128x14x20x!quant.uniform<i8:f32, 0.0867442937007874>> loc(#loc667)
        %400 = "tpu.Load"(%315#0) {do_bcast = false, ginfo = #tpu.lg<out_addr = 18432, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 5, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.05307607952755905>, 62720 : i64>) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.05307607952755905>> loc(#loc668)
        %401 = "tpu.Load"(%319) {do_bcast = true, ginfo = #tpu.lg<out_addr = 19712, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 6, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099512804368 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc669)
        %402 = "tpu.Load"(%322) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 18432, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [128], id = 7, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x9x128xsi8, 1099515516656 : i64>) -> tensor<1x128x9x128xsi8> loc(#loc670)
        %403 = "tpu.Conv2D"(%399, %397, %398) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 20480, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [14], w_idx = [0], w_slice = [20], id = 8, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.0867442937007874>>, tensor<1x128x1x128xsi8>, tensor<1x128x1x9xi8>) -> tensor<1x128x14x20x!quant.uniform<i8:f32, 0.27910601968503934>> loc(#loc671)
        %404 = "tpu.Load"(%315#1) {do_bcast = false, ginfo = #tpu.lg<out_addr = 28672, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 9, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.1182248874015748>, 53760 : i64>) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.1182248874015748>> loc(#loc672)
        %405 = "tpu.Load"(%320) {do_bcast = true, ginfo = #tpu.lg<out_addr = 19968, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 10, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099519849296 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc673)
        %406 = "tpu.Lut"(%400, %401) {ginfo = #tpu.lg<out_addr = 25088, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 11, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.05307607952755905>>, tensor<1x1x1x256xsi8>) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.033869064566929133>> loc(#loc659)
        %407 = "tpu.Load"(%321) {do_bcast = false, ginfo = #tpu.lg<out_addr = 19712, out_size = 144, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 12, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x9xi8, 1099514340272 : i64>) -> tensor<1x128x1x9xi8> loc(#loc674)
        %408 = "tpu.Store"(%406, %0) {ginfo = #tpu.lg<out_addr = 25088, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 13, stage = 1, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.033869064566929133>>, none) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.033869064566929133>, 179200 : i64> loc(#loc659)
        %409 = "tpu.Lut"(%404, %405) {ginfo = #tpu.lg<out_addr = 18432, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 14, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.1182248874015748>>, tensor<1x1x1x256xsi8>) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.067294007086614177>> loc(#loc660)
        %410 = "tpu.Load"(%323) {do_bcast = true, ginfo = #tpu.lg<out_addr = 19856, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 15, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099520111696 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc675)
        %411 = "tpu.Store"(%409, %0) {ginfo = #tpu.lg<out_addr = 18432, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 16, stage = 1, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.067294007086614177>>, none) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.067294007086614177>, 71680 : i64> loc(#loc660)
        %412 = "tpu.Conv2D"(%406, %402, %407) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 28672, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 17, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.033869064566929133>>, tensor<1x128x9x128xsi8>, tensor<1x128x1x9xi8>) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.1118316874015748>> loc(#loc661)
        %413 = "tpu.Load"(%325) {do_bcast = false, ginfo = #tpu.lg<out_addr = 4608, out_size = 128, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [4], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [128], id = 18, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x4x1x128xsi8, 1099519295120 : i64>) -> tensor<1x4x1x128xsi8> loc(#loc676)
        %414 = "tpu.Load"(%324) {do_bcast = false, ginfo = #tpu.lg<out_addr = 29952, out_size = 9, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [4], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 19, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x4x1x9xi8, 1099517513072 : i64>) -> tensor<1x4x1x9xi8> loc(#loc677)
        %415 = "tpu.Store"(%412, %0) {ginfo = #tpu.lg<out_addr = 28672, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 20, stage = 1, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.1118316874015748>>, none) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.1118316874015748>, 80640 : i64> loc(#loc661)
        %416 = "tpu.Lut"(%403, %410) {ginfo = #tpu.lg<out_addr = 8192, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [14], w_idx = [0], w_slice = [20], id = 21, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.27910601968503934>>, tensor<1x1x1x256xsi8>) -> tensor<1x128x14x20x!quant.uniform<i8:f32, 0.17451503622047246>> loc(#loc678)
        %417 = "tpu.Conv2D"(%416, %413, %414) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 288, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [4], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [14], w_idx = [0], w_slice = [20], id = 22, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.17451503622047246>>, tensor<1x4x1x128xsi8>, tensor<1x4x1x9xi8>) -> tensor<1x4x14x20x!quant.uniform<i8:f32, 0.18704145511811024>> loc(#loc662)
        %418 = "tpu.Store"(%417, %0) {ginfo = #tpu.lg<out_addr = 16384, out_size = 288, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [4], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [14], w_idx = [0], w_slice = [20], id = 23, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x4x14x20x!quant.uniform<i8:f32, 0.18704145511811024>>, none) -> tensor<1x4x14x20x!quant.uniform<i8:f32, 0.18704145511811024>, 89600 : i64> loc(#loc662)
        "tpu.Yield"(%408, %411, %415, %418) : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.033869064566929133>, 179200 : i64>, tensor<1x128x7x10x!quant.uniform<i8:f32, 0.067294007086614177>, 71680 : i64>, tensor<1x128x7x10x!quant.uniform<i8:f32, 0.1118316874015748>, 80640 : i64>, tensor<1x4x14x20x!quant.uniform<i8:f32, 0.18704145511811024>, 89600 : i64>) -> () loc(#loc826)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 4, 2, 3, 23, -2, 8, 5, 6, 7, -3, 11, 9, 10, -4, 14, 12, 13, -5, 17, 15, 16, -6, 21, 18, 19, 20, 0, -7, 22, 1], group_type = 0 : i64, hsecs = 1 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.092046457480314952>, 143360 : i64>, tensor<1x128x7x10x!quant.uniform<i8:f32, 0.05307607952755905>, 62720 : i64>, tensor<1x128x7x10x!quant.uniform<i8:f32, 0.1182248874015748>, 53760 : i64>) -> (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.033869064566929133>, 179200 : i64>, tensor<1x128x7x10x!quant.uniform<i8:f32, 0.067294007086614177>, 71680 : i64>, tensor<1x128x7x10x!quant.uniform<i8:f32, 0.1118316874015748>, 80640 : i64>, tensor<1x4x14x20x!quant.uniform<i8:f32, 0.18704145511811024>, 89600 : i64>) loc(#loc826)
      %327 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099512487952 : i64> loc(#loc679)
      %328 = "top.Weight"() : () -> tensor<1x128x1x9xi8, 1099520111952 : i64> loc(#loc680)
      %329 = "top.Weight"() : () -> tensor<1x128x9x128xsi8, 1099520293328 : i64> loc(#loc681)
      %330:2 = "tpu.Group"(%326#2, %308#1, %326#3) ({
        %395 = "tpu.Load"(%326#2) {do_bcast = false, ginfo = #tpu.lg<out_addr = 23072, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.1118316874015748>, 80640 : i64>) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.1118316874015748>> loc(#loc684)
        %396 = "tpu.Load"(%327) {do_bcast = true, ginfo = #tpu.lg<out_addr = 18720, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 1, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099512487952 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc685)
        %397 = "tpu.Load"(%308#1) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [14], w_idx = [0], w_slice = [20], id = 2, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.18704145511811024>, 35840 : i64>) -> tensor<1x64x14x20x!quant.uniform<i8:f32, 0.18704145511811024>> loc(#loc686)
        %398 = "tpu.Load"(%326#3) {do_bcast = false, ginfo = #tpu.lg<out_addr = 18432, out_size = 288, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [4], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [14], w_idx = [0], w_slice = [20], id = 3, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x4x14x20x!quant.uniform<i8:f32, 0.18704145511811024>, 89600 : i64>) -> tensor<1x4x14x20x!quant.uniform<i8:f32, 0.18704145511811024>> loc(#loc687)
        %399 = "tpu.Lut"(%395, %396) {ginfo = #tpu.lg<out_addr = 28672, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 4, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.1118316874015748>>, tensor<1x1x1x256xsi8>) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.065595275590551186>> loc(#loc688)
        %400 = "tpu.Load"(%329) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 18432, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [128], id = 5, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x9x128xsi8, 1099520293328 : i64>) -> tensor<1x128x9x128xsi8> loc(#loc689)
        %401 = "tpu.Load"(%328) {do_bcast = false, ginfo = #tpu.lg<out_addr = 18976, out_size = 144, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 6, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x9xi8, 1099520111952 : i64>) -> tensor<1x128x1x9xi8> loc(#loc690)
        %402 = "tpu.Concat"(%397, %398) {axis = 1 : si32, do_relu = false, ginfo = #tpu.lg<out_addr = 20480, out_size = 2592, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [68], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [14], w_idx = [0], w_slice = [20], id = 7, stage = 1, slice_idx = 0, group_type = 0>, multipliers = [1, 1], only_merge = true, relu_limit = -1.000000e+00 : f64, rshifts = [0, 0]} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.18704145511811024>>, tensor<1x4x14x20x!quant.uniform<i8:f32, 0.18704145511811024>>) -> tensor<1x68x14x20x!quant.uniform<i8:f32, 0.18704145511811024>> loc(#loc682)
        %403 = "tpu.Store"(%402, %0) {ginfo = #tpu.lg<out_addr = 20480, out_size = 2592, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [68], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [14], w_idx = [0], w_slice = [20], id = 8, stage = 1, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x68x14x20x!quant.uniform<i8:f32, 0.18704145511811024>>, none) -> tensor<1x68x14x20x!quant.uniform<i8:f32, 0.18704145511811024>, 99680 : i64> loc(#loc682)
        %404 = "tpu.Conv2D"(%399, %400, %401) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 26880, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 9, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.065595275590551186>>, tensor<1x128x9x128xsi8>, tensor<1x128x1x9xi8>) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.085638549606299213>> loc(#loc683)
        %405 = "tpu.Store"(%404, %0) {ginfo = #tpu.lg<out_addr = 26880, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 10, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.085638549606299213>>, none) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.085638549606299213>, 90720 : i64> loc(#loc683)
        "tpu.Yield"(%403, %405) : (tensor<1x68x14x20x!quant.uniform<i8:f32, 0.18704145511811024>, 99680 : i64>, tensor<1x128x7x10x!quant.uniform<i8:f32, 0.085638549606299213>, 90720 : i64>) -> () loc(#loc827)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 4, 2, 3, 10, -2, 7, 5, 6, -3, 9, 8, 0, 1], group_type = 0 : i64, hsecs = 1 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.1118316874015748>, 80640 : i64>, tensor<1x64x14x20x!quant.uniform<i8:f32, 0.18704145511811024>, 35840 : i64>, tensor<1x4x14x20x!quant.uniform<i8:f32, 0.18704145511811024>, 89600 : i64>) -> (tensor<1x68x14x20x!quant.uniform<i8:f32, 0.18704145511811024>, 99680 : i64>, tensor<1x128x7x10x!quant.uniform<i8:f32, 0.085638549606299213>, 90720 : i64>) loc(#loc827)
      %331 = "tpu.Reshape"(%330#0) {flatten_start_dim = -1 : i64, shape = [1, 68, -1]} : (tensor<1x68x14x20x!quant.uniform<i8:f32, 0.18704145511811024>, 99680 : i64>) -> tensor<1x68x280x!quant.uniform<i8:f32, 0.18704145511811024>, 99680 : i64> loc(#loc691)
      %332 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099515747120 : i64> loc(#loc692)
      %333 = "top.Weight"() : () -> tensor<1x128x1x9xi8, 1099520441360 : i64> loc(#loc693)
      %334 = "top.Weight"() : () -> tensor<1x128x9x128xsi8, 1099520737680 : i64> loc(#loc694)
      %335:2 = "tpu.Group"(%330#1, %326#0) ({
        %395 = "tpu.Load"(%330#1) {do_bcast = false, ginfo = #tpu.lg<out_addr = 18432, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.085638549606299213>, 90720 : i64>) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.085638549606299213>> loc(#loc697)
        %396 = "tpu.Load"(%332) {do_bcast = true, ginfo = #tpu.lg<out_addr = 19712, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 1, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099515747120 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc698)
        %397 = "tpu.Load"(%326#0) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 2, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.033869064566929133>, 179200 : i64>) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.033869064566929133>> loc(#loc699)
        %398 = "tpu.Load"(%333) {do_bcast = false, ginfo = #tpu.lg<out_addr = 19968, out_size = 144, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 3, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x9xi8, 1099520441360 : i64>) -> tensor<1x128x1x9xi8> loc(#loc700)
        %399 = "tpu.Lut"(%395, %396) {ginfo = #tpu.lg<out_addr = 20480, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 4, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.085638549606299213>>, tensor<1x1x1x256xsi8>) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.045744881889763778>> loc(#loc701)
        %400 = "tpu.Load"(%334) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 18432, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [128], id = 5, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x9x128xsi8, 1099520737680 : i64>) -> tensor<1x128x9x128xsi8> loc(#loc702)
        %401 = "tpu.Add"(%397, %399) {do_relu = false, ginfo = #tpu.lg<out_addr = 28672, out_size = 1280, buffer_addr = 18432, buffer_size = 1280, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 6, stage = 1, slice_idx = 0, group_type = 0>, multipliers = [71, 96], relu_limit = -1.000000e+00 : f64, rshifts = [7]} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.033869064566929133>>, tensor<1x128x7x10x!quant.uniform<i8:f32, 0.045744881889763778>>) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.060872159842519681>> loc(#loc695)
        %402 = "tpu.Store"(%401, %0) {ginfo = #tpu.lg<out_addr = 28672, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 7, stage = 1, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.060872159842519681>>, none) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.060872159842519681>, 8960 : i64> loc(#loc695)
        %403 = "tpu.Conv2D"(%401, %400, %398) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 25856, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 8, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.060872159842519681>>, tensor<1x128x9x128xsi8>, tensor<1x128x1x9xi8>) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.075240495275590555>> loc(#loc696)
        %404 = "tpu.Store"(%403, %0) {ginfo = #tpu.lg<out_addr = 25856, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 9, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.075240495275590555>>, none) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.075240495275590555>, 0 : i64> loc(#loc696)
        "tpu.Yield"(%402, %404) : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.060872159842519681>, 8960 : i64>, tensor<1x128x7x10x!quant.uniform<i8:f32, 0.075240495275590555>, 0 : i64>) -> () loc(#loc828)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 4, 2, 9, 3, -2, 6, 5, -3, 8, 7, 0, 1], group_type = 0 : i64, hsecs = 1 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.085638549606299213>, 90720 : i64>, tensor<1x128x7x10x!quant.uniform<i8:f32, 0.033869064566929133>, 179200 : i64>) -> (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.060872159842519681>, 8960 : i64>, tensor<1x128x7x10x!quant.uniform<i8:f32, 0.075240495275590555>, 0 : i64>) loc(#loc828)
      %336 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099521177920 : i64> loc(#loc703)
      %337 = "top.Weight"() : () -> tensor<1x128x1x9xi8, 1099521024400 : i64> loc(#loc704)
      %338 = "top.Weight"() : () -> tensor<1x128x9x128xsi8, 1099521025552 : i64> loc(#loc705)
      %339 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099519178960 : i64> loc(#loc706)
      %340 = "top.Weight"() : () -> tensor<1x256x1x9xi8, 1099516212080 : i64> loc(#loc707)
      %341 = "top.Weight"() : () -> tensor<1x256x1x256xsi8, 1099520885136 : i64> loc(#loc708)
      %342 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099516034352 : i64> loc(#loc709)
      %343 = "tpu.Group"(%335#1, %335#0, %326#1) ({
        %395 = "tpu.Load"(%335#1) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.075240495275590555>, 0 : i64>) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.075240495275590555>> loc(#loc711)
        %396 = "tpu.Load"(%336) {do_bcast = true, ginfo = #tpu.lg<out_addr = 4096, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 1, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099521177920 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc712)
        %397 = "tpu.Load"(%338) {do_bcast = false, ginfo = #tpu.lg<out_addr = 8192, out_size = 18432, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [128], id = 2, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x9x128xsi8, 1099521025552 : i64>) -> tensor<1x128x9x128xsi8> loc(#loc713)
        %398 = "tpu.Load"(%337) {do_bcast = false, ginfo = #tpu.lg<out_addr = 27904, out_size = 144, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 3, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x9xi8, 1099521024400 : i64>) -> tensor<1x128x1x9xi8> loc(#loc714)
        %399 = "tpu.Lut"(%395, %396) {ginfo = #tpu.lg<out_addr = 26624, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 4, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.075240495275590555>>, tensor<1x1x1x256xsi8>) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.032335301574803146>> loc(#loc715)
        %400 = "tpu.Load"(%339) {do_bcast = true, ginfo = #tpu.lg<out_addr = 28048, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 5, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099519178960 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc716)
        %401 = "tpu.Load"(%341) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 8192, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 6, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x1x256xsi8, 1099520885136 : i64>) -> tensor<1x256x1x256xsi8> loc(#loc717)
        %402 = "tpu.Conv2D"(%399, %397, %398) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 31232, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 7, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.032335301574803146>>, tensor<1x128x9x128xsi8>, tensor<1x128x1x9xi8>) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.099834111811023626>> loc(#loc718)
        %403 = "tpu.Load"(%335#0) {do_bcast = false, ginfo = #tpu.lg<out_addr = 8192, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 8, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.060872159842519681>, 8960 : i64>) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.060872159842519681>> loc(#loc719)
        %404 = "tpu.Lut"(%402, %400) {ginfo = #tpu.lg<out_addr = 12288, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 9, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.099834111811023626>>, tensor<1x1x1x256xsi8>) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.057052657480314961>> loc(#loc720)
        %405 = "tpu.Load"(%326#1) {do_bcast = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 10, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.067294007086614177>, 71680 : i64>) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.067294007086614177>> loc(#loc721)
        %406 = "tpu.Add"(%403, %404) {do_relu = false, ginfo = #tpu.lg<out_addr = 20480, out_size = 1280, buffer_addr = 24576, buffer_size = 1280, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 11, stage = 1, slice_idx = 0, group_type = 0>, multipliers = [115, 108], relu_limit = -1.000000e+00 : f64, rshifts = [7]} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.060872159842519681>>, tensor<1x128x7x10x!quant.uniform<i8:f32, 0.057052657480314961>>) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.067294007086614177>> loc(#loc722)
        %407 = "tpu.Load"(%340) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 288, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 12, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x1x9xi8, 1099516212080 : i64>) -> tensor<1x256x1x9xi8> loc(#loc723)
        %408 = "tpu.Concat"(%406, %405) {axis = 1 : si32, do_relu = false, ginfo = #tpu.lg<out_addr = 8192, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 13, stage = 1, slice_idx = 0, group_type = 0>, multipliers = [1, 1], only_merge = true, relu_limit = -1.000000e+00 : f64, rshifts = [0, 0]} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.067294007086614177>>, tensor<1x128x7x10x!quant.uniform<i8:f32, 0.067294007086614177>>) -> tensor<1x256x7x10x!quant.uniform<i8:f32, 0.067294007086614177>> loc(#loc724)
        %409 = "tpu.Load"(%342) {do_bcast = true, ginfo = #tpu.lg<out_addr = 16384, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 14, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099516034352 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc725)
        %410 = "tpu.Conv2D"(%408, %401, %407) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 15, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x256x7x10x!quant.uniform<i8:f32, 0.067294007086614177>>, tensor<1x256x1x256xsi8>, tensor<1x256x1x9xi8>) -> tensor<1x256x7x10x!quant.uniform<i8:f32, 0.076410962204724403>> loc(#loc726)
        %411 = "tpu.Lut"(%410, %409) {ginfo = #tpu.lg<out_addr = 28672, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 16, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x256x7x10x!quant.uniform<i8:f32, 0.076410962204724403>>, tensor<1x1x1x256xsi8>) -> tensor<1x256x7x10x!quant.uniform<i8:f32, 0.066229507086614181>> loc(#loc710)
        %412 = "tpu.Store"(%411, %0) {ginfo = #tpu.lg<out_addr = 28672, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 17, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x256x7x10x!quant.uniform<i8:f32, 0.066229507086614181>>, none) -> tensor<1x256x7x10x!quant.uniform<i8:f32, 0.066229507086614181>, 118720 : i64> loc(#loc710)
        "tpu.Yield"(%412) : (tensor<1x256x7x10x!quant.uniform<i8:f32, 0.066229507086614181>, 118720 : i64>) -> () loc(#loc710)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 4, 2, 3, -2, 7, 5, 17, 6, -3, 9, 8, -4, 11, 10, -5, 13, 12, -6, 15, 14, -7, 16, 0, 1], group_type = 0 : i64, hsecs = 1 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.075240495275590555>, 0 : i64>, tensor<1x128x7x10x!quant.uniform<i8:f32, 0.060872159842519681>, 8960 : i64>, tensor<1x128x7x10x!quant.uniform<i8:f32, 0.067294007086614177>, 71680 : i64>) -> tensor<1x256x7x10x!quant.uniform<i8:f32, 0.066229507086614181>, 118720 : i64> loc(#loc710)
      %344 = "tpu.Concat"(%309, %310, %343) {axis = 1 : si32, do_relu = false, multipliers = [1, 1, 123], only_merge = false, relu_limit = -1.000000e+00 : f64, rshifts = [0, 0, 7]} : (tensor<1x256x7x10x!quant.uniform<i8:f32, 0.068618894488188975>, 232960 : i64>, tensor<1x256x7x10x!quant.uniform<i8:f32, 0.068618894488188975>, 250880 : i64>, tensor<1x256x7x10x!quant.uniform<i8:f32, 0.066229507086614181>, 118720 : i64>) -> tensor<1x768x7x10x!quant.uniform<i8:f32, 0.068618894488188975>, 0 : i64> loc(#loc727)
      %345 = "top.Weight"() {do_compress = true} : () -> tensor<1x512x1x9xi8, 1099521173056 : i64> loc(#loc728)
      %346 = "top.Weight"() {do_compress = true} : () -> tensor<1x512x1x768xsi8, 1099519418192 : i64> loc(#loc729)
      %347 = "tpu.Conv2D"(%344, %346, %345) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x768x7x10x!quant.uniform<i8:f32, 0.068618894488188975>, 0 : i64>, tensor<1x512x1x768xsi8, 1099519418192 : i64>, tensor<1x512x1x9xi8, 1099521173056 : i64>) -> tensor<1x512x7x10x!quant.uniform<i8:f32, 0.11152565590551181>, 118720 : i64> loc(#loc730)
      %348 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099521177664 : i64> loc(#loc731)
      %349 = "tpu.Lut"(%347, %348) : (tensor<1x512x7x10x!quant.uniform<i8:f32, 0.11152565590551181>, 118720 : i64>, tensor<1x1x1x256xsi8, 1099521177664 : i64>) -> tensor<1x512x7x10x!quant.uniform<i8:f32, 0.089299721259842507>, 154560 : i64> loc(#loc732)
      %350 = "top.Weight"() {do_compress = true} : () -> tensor<1x64x1x9xi8, 1099519301328 : i64> loc(#loc733)
      %351 = "top.Weight"() {do_compress = true} : () -> tensor<1x64x9x512xsi8, 1099520442512 : i64> loc(#loc734)
      %352 = "tpu.Conv2D"(%349, %351, %350) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x512x7x10x!quant.uniform<i8:f32, 0.089299721259842507>, 154560 : i64>, tensor<1x64x9x512xsi8, 1099520442512 : i64>, tensor<1x64x1x9xi8, 1099519301328 : i64>) -> tensor<1x64x7x10x!quant.uniform<i8:f32, 0.085869496062992128>, 190400 : i64> loc(#loc735)
      %353 = "top.Weight"() : () -> tensor<1x512x1x9xi8, 1099521178752 : i64> loc(#loc736)
      %354 = "top.Weight"() : () -> tensor<1x512x3x3xsi8, 1099521183360 : i64> loc(#loc737)
      %355 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099519107408 : i64> loc(#loc738)
      %356 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099513856768 : i64> loc(#loc739)
      %357 = "top.Weight"() : () -> tensor<1x64x1x9xi8, 1099519300240 : i64> loc(#loc740)
      %358 = "top.Weight"() : () -> tensor<1x64x9x64xsi8, 1099519060112 : i64> loc(#loc741)
      %359 = "top.Weight"() : () -> tensor<1x128x1x9xi8, 1099513586816 : i64> loc(#loc742)
      %360 = "top.Weight"() : () -> tensor<1x128x1x512xsi8, 1099515895088 : i64> loc(#loc743)
      %361 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099517460976 : i64> loc(#loc744)
      %362 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099515894832 : i64> loc(#loc745)
      %363 = "top.Weight"() : () -> tensor<1x64x1x9xi8, 1099521178176 : i64> loc(#loc746)
      %364 = "top.Weight"() : () -> tensor<1x64x1x64xsi8, 1099521187968 : i64> loc(#loc747)
      %365 = "top.Weight"() : () -> tensor<1x128x1x9xi8, 1099521192064 : i64> loc(#loc748)
      %366 = "top.Weight"() : () -> tensor<1x128x3x3xsi8, 1099513227152 : i64> loc(#loc749)
      %367 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099518124448 : i64> loc(#loc750)
      %368 = "top.Weight"() : () -> tensor<1x128x1x9xi8, 1099518957584 : i64> loc(#loc751)
      %369 = "top.Weight"() : () -> tensor<1x128x1x128xsi8, 1099519107920 : i64> loc(#loc752)
      %370 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099519107664 : i64> loc(#loc753)
      %371 = "top.Weight"() : () -> tensor<1x4x1x9xi8, 1099518416864 : i64> loc(#loc754)
      %372 = "top.Weight"() : () -> tensor<1x4x1x128xsi8, 1099512115088 : i64> loc(#loc755)
      %373:2 = "tpu.Group"(%349, %352) ({
        %395 = "tpu.Load"(%349) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 5120, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [512], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x512x7x10x!quant.uniform<i8:f32, 0.089299721259842507>, 154560 : i64>) -> tensor<1x512x7x10x!quant.uniform<i8:f32, 0.089299721259842507>> loc(#loc758)
        %396 = "tpu.Load"(%353) {do_bcast = false, ginfo = #tpu.lg<out_addr = 20480, out_size = 576, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [512], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 1, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x512x1x9xi8, 1099521178752 : i64>) -> tensor<1x512x1x9xi8> loc(#loc759)
        %397 = "tpu.Load"(%354) {do_bcast = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 1024, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [512], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [3], w_idx = [0], w_slice = [3], id = 2, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x512x3x3xsi8, 1099521183360 : i64>) -> tensor<1x512x3x3xsi8> loc(#loc760)
        %398 = "tpu.Load"(%352) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 640, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 3, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x7x10x!quant.uniform<i8:f32, 0.085869496062992128>, 190400 : i64>) -> tensor<1x64x7x10x!quant.uniform<i8:f32, 0.085869496062992128>> loc(#loc761)
        %399 = "tpu.Load"(%355) {do_bcast = true, ginfo = #tpu.lg<out_addr = 28672, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 4, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099519107408 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc762)
        %400 = "tpu.Conv2D"(%395, %397, %396) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 8192, out_size = 5120, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [512], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 5, stage = 1, slice_idx = 0, group_type = 0>, group = 512 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x512x7x10x!quant.uniform<i8:f32, 0.089299721259842507>>, tensor<1x512x3x3xsi8>, tensor<1x512x1x9xi8>) -> tensor<1x512x7x10x!quant.uniform<i8:f32, 0.072498433858267722>> loc(#loc763)
        %401 = "tpu.Load"(%356) {do_bcast = true, ginfo = #tpu.lg<out_addr = 0, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 6, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099513856768 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc764)
        %402 = "tpu.Lut"(%398, %399) {ginfo = #tpu.lg<out_addr = 13312, out_size = 640, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 7, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x64x7x10x!quant.uniform<i8:f32, 0.085869496062992128>>, tensor<1x1x1x256xsi8>) -> tensor<1x64x7x10x!quant.uniform<i8:f32, 0.045245805511811024>> loc(#loc765)
        %403 = "tpu.Load"(%358) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [64], id = 8, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x9x64xsi8, 1099519060112 : i64>) -> tensor<1x64x9x64xsi8> loc(#loc766)
        %404 = "tpu.Load"(%357) {do_bcast = false, ginfo = #tpu.lg<out_addr = 13952, out_size = 72, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 9, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x9xi8, 1099519300240 : i64>) -> tensor<1x64x1x9xi8> loc(#loc767)
        %405 = "tpu.Lut"(%400, %401) {ginfo = #tpu.lg<out_addr = 16384, out_size = 5120, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [512], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 10, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x512x7x10x!quant.uniform<i8:f32, 0.072498433858267722>>, tensor<1x1x1x256xsi8>) -> tensor<1x512x7x10x!quant.uniform<i8:f32, 0.067609959055118116>> loc(#loc768)
        %406 = "tpu.Load"(%360) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 8192, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [512], id = 11, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x512xsi8, 1099515895088 : i64>) -> tensor<1x128x1x512xsi8> loc(#loc769)
        %407 = "tpu.Load"(%359) {do_bcast = false, ginfo = #tpu.lg<out_addr = 10112, out_size = 144, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 12, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x9xi8, 1099513586816 : i64>) -> tensor<1x128x1x9xi8> loc(#loc770)
        %408 = "tpu.Conv2D"(%402, %403, %404) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 9472, out_size = 640, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 13, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x7x10x!quant.uniform<i8:f32, 0.045245805511811024>>, tensor<1x64x9x64xsi8>, tensor<1x64x1x9xi8>) -> tensor<1x64x7x10x!quant.uniform<i8:f32, 0.10588234409448818>> loc(#loc771)
        %409 = "tpu.Load"(%361) {do_bcast = true, ginfo = #tpu.lg<out_addr = 12288, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 14, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099517460976 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc772)
        %410 = "tpu.Conv2D"(%405, %406, %407) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 8192, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 15, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x512x7x10x!quant.uniform<i8:f32, 0.067609959055118116>>, tensor<1x128x1x512xsi8>, tensor<1x128x1x9xi8>) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.078792286614173215>> loc(#loc773)
        %411 = "tpu.Load"(%362) {do_bcast = true, ginfo = #tpu.lg<out_addr = 16384, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 16, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099515894832 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc774)
        %412 = "tpu.Lut"(%408, %409) {ginfo = #tpu.lg<out_addr = 1280, out_size = 640, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 17, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x64x7x10x!quant.uniform<i8:f32, 0.10588234409448818>>, tensor<1x1x1x256xsi8>) -> tensor<1x64x7x10x!quant.uniform<i8:f32, 0.096550419685039371>> loc(#loc775)
        %413 = "tpu.Load"(%364) {do_bcast = false, ginfo = #tpu.lg<out_addr = 4096, out_size = 512, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [64], id = 18, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x64xsi8, 1099521187968 : i64>) -> tensor<1x64x1x64xsi8> loc(#loc776)
        %414 = "tpu.Load"(%363) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 72, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 19, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x9xi8, 1099521178176 : i64>) -> tensor<1x64x1x9xi8> loc(#loc777)
        %415 = "tpu.Lut"(%410, %411) {ginfo = #tpu.lg<out_addr = 0, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 20, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.078792286614173215>>, tensor<1x1x1x256xsi8>) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.04059411968503937>> loc(#loc778)
        %416 = "tpu.Load"(%366) {do_bcast = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [3], w_idx = [0], w_slice = [3], id = 21, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x3x3xsi8, 1099513227152 : i64>) -> tensor<1x128x3x3xsi8> loc(#loc779)
        %417 = "tpu.Load"(%365) {do_bcast = false, ginfo = #tpu.lg<out_addr = 20480, out_size = 144, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 22, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x9xi8, 1099521192064 : i64>) -> tensor<1x128x1x9xi8> loc(#loc780)
        %418 = "tpu.Conv2D"(%412, %413, %414) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 8192, out_size = 640, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 23, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x7x10x!quant.uniform<i8:f32, 0.096550419685039371>>, tensor<1x64x1x64xsi8>, tensor<1x64x1x9xi8>) -> tensor<1x64x7x10x!quant.uniform<i8:f32, 0.18704145511811024>> loc(#loc756)
        %419 = "tpu.Load"(%367) {do_bcast = true, ginfo = #tpu.lg<out_addr = 16384, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 24, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099518124448 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc781)
        %420 = "tpu.Store"(%418, %0) {ginfo = #tpu.lg<out_addr = 8192, out_size = 640, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 25, stage = 1, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x64x7x10x!quant.uniform<i8:f32, 0.18704145511811024>>, none) -> tensor<1x64x7x10x!quant.uniform<i8:f32, 0.18704145511811024>, 0 : i64> loc(#loc756)
        %421 = "tpu.Conv2D"(%415, %416, %417) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 4096, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 26, stage = 1, slice_idx = 0, group_type = 0>, group = 128 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.04059411968503937>>, tensor<1x128x3x3xsi8>, tensor<1x128x1x9xi8>) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.050401120472440941>> loc(#loc782)
        %422 = "tpu.Load"(%369) {do_bcast = false, ginfo = #tpu.lg<out_addr = 8192, out_size = 2048, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [128], id = 27, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x128xsi8, 1099519107920 : i64>) -> tensor<1x128x1x128xsi8> loc(#loc783)
        %423 = "tpu.Load"(%368) {do_bcast = false, ginfo = #tpu.lg<out_addr = 20480, out_size = 144, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 28, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x9xi8, 1099518957584 : i64>) -> tensor<1x128x1x9xi8> loc(#loc784)
        %424 = "tpu.Lut"(%421, %419) {ginfo = #tpu.lg<out_addr = 12288, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 29, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.050401120472440941>>, tensor<1x1x1x256xsi8>) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.047596436220472445>> loc(#loc785)
        %425 = "tpu.Load"(%370) {do_bcast = true, ginfo = #tpu.lg<out_addr = 24576, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 30, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099519107664 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc786)
        %426 = "tpu.Conv2D"(%424, %422, %423) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 31, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.047596436220472445>>, tensor<1x128x1x128xsi8>, tensor<1x128x1x9xi8>) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.1353541094488189>> loc(#loc787)
        %427 = "tpu.Load"(%372) {do_bcast = false, ginfo = #tpu.lg<out_addr = 8192, out_size = 128, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [4], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [128], id = 32, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x4x1x128xsi8, 1099512115088 : i64>) -> tensor<1x4x1x128xsi8> loc(#loc788)
        %428 = "tpu.Load"(%371) {do_bcast = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 9, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [4], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 33, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x4x1x9xi8, 1099518416864 : i64>) -> tensor<1x4x1x9xi8> loc(#loc789)
        %429 = "tpu.Lut"(%426, %425) {ginfo = #tpu.lg<out_addr = 5120, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 34, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.1353541094488189>>, tensor<1x1x1x256xsi8>) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.095107611811023618>> loc(#loc790)
        %430 = "tpu.Conv2D"(%429, %427, %428) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 25216, out_size = 80, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [4], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 35, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.095107611811023618>>, tensor<1x4x1x128xsi8>, tensor<1x4x1x9xi8>) -> tensor<1x4x7x10x!quant.uniform<i8:f32, 0.18704145511811024>> loc(#loc757)
        %431 = "tpu.Store"(%430, %0) {ginfo = #tpu.lg<out_addr = 25216, out_size = 80, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [4], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 36, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x4x7x10x!quant.uniform<i8:f32, 0.18704145511811024>>, none) -> tensor<1x4x7x10x!quant.uniform<i8:f32, 0.18704145511811024>, 4480 : i64> loc(#loc757)
        "tpu.Yield"(%420, %431) : (tensor<1x64x7x10x!quant.uniform<i8:f32, 0.18704145511811024>, 0 : i64>, tensor<1x4x7x10x!quant.uniform<i8:f32, 0.18704145511811024>, 4480 : i64>) -> () loc(#loc829)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 5, 3, 4, 36, -2, 7, 6, -3, 10, 8, 9, -4, 13, 11, 12, -5, 15, 14, -6, 17, 16, -7, 20, 18, 19, -8, 23, 21, 22, -9, 26, 24, 25, -10, 29, 27, 28, -11, 31, 30, 0, -12, 34, 32, 33, 1, -13, 35, 2], group_type = 0 : i64, hsecs = 1 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x512x7x10x!quant.uniform<i8:f32, 0.089299721259842507>, 154560 : i64>, tensor<1x64x7x10x!quant.uniform<i8:f32, 0.085869496062992128>, 190400 : i64>) -> (tensor<1x64x7x10x!quant.uniform<i8:f32, 0.18704145511811024>, 0 : i64>, tensor<1x4x7x10x!quant.uniform<i8:f32, 0.18704145511811024>, 4480 : i64>) loc(#loc829)
      %374 = "tpu.Concat"(%373#0, %373#1) {axis = 1 : si32, do_relu = false, multipliers = [1, 1], only_merge = true, relu_limit = -1.000000e+00 : f64, rshifts = [0, 0]} : (tensor<1x64x7x10x!quant.uniform<i8:f32, 0.18704145511811024>, 0 : i64>, tensor<1x4x7x10x!quant.uniform<i8:f32, 0.18704145511811024>, 4480 : i64>) -> tensor<1x68x7x10x!quant.uniform<i8:f32, 0.18704145511811024>, 0 : i64> loc(#loc791)
      %375 = "tpu.Reshape"(%374) {flatten_start_dim = -1 : i64, shape = [1, 68, -1]} : (tensor<1x68x7x10x!quant.uniform<i8:f32, 0.18704145511811024>, 0 : i64>) -> tensor<1x68x70x!quant.uniform<i8:f32, 0.18704145511811024>, 0 : i64> loc(#loc792)
      %376 = "tpu.Concat"(%276, %331, %375) {axis = 2 : si32, do_relu = false, multipliers = [1, 1, 1], only_merge = false, relu_limit = -1.000000e+00 : f64, rshifts = [0, 0, 0]} : (tensor<1x68x1120x!quant.uniform<i8:f32, 0.18704145511811024>, 366528 : i64>, tensor<1x68x280x!quant.uniform<i8:f32, 0.18704145511811024>, 99680 : i64>, tensor<1x68x70x!quant.uniform<i8:f32, 0.18704145511811024>, 0 : i64>) -> tensor<1x68x1470x!quant.uniform<i8:f32, 0.18704145511811024>, 194048 : i64> loc(#loc793)
      %377 = "tpu.Slice"(%376, %0, %0, %0, %0) {axes = [], ends = [9223372036854775807, 64, 9223372036854775807], hasparamConvert_axes = [1], offset = [0, 0, 0], steps = [1, 1, 1]} : (tensor<1x68x1470x!quant.uniform<i8:f32, 0.18704145511811024>, 194048 : i64>, none, none, none, none) -> tensor<1x64x1470x!quant.uniform<i8:f32, 0.18704145511811024>, 194048 : i64> loc(#loc794)
      %378 = "tpu.Slice"(%376, %0, %0, %0, %0) {axes = [], ends = [9223372036854775807, 68, 9223372036854775807], hasparamConvert_axes = [1], offset = [0, 64, 0], steps = [1, 1, 1]} : (tensor<1x68x1470x!quant.uniform<i8:f32, 0.18704145511811024>, 194048 : i64>, none, none, none, none) -> tensor<1x4x1470x!quant.uniform<i8:f32, 0.18704145511811024>, 288128 : i64> loc(#loc795)
      %379 = "tpu.Reshape"(%377) {flatten_start_dim = -1 : i64, shape = [1, 4, 16, 1470]} : (tensor<1x64x1470x!quant.uniform<i8:f32, 0.18704145511811024>, 194048 : i64>) -> tensor<1x4x16x1470x!quant.uniform<i8:f32, 0.18704145511811024>, 194048 : i64> loc(#loc796)
      %380 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099511627792 : i64> loc(#loc797)
      %381 = "tpu.Lut"(%378, %380) : (tensor<1x4x1470x!quant.uniform<i8:f32, 0.18704145511811024>, 288128 : i64>, tensor<1x1x1x256xsi8, 1099511627792 : i64>) -> tensor<1x4x1470x!quant.uniform<i8:f32, 0.0078740157480314959>, 0 : i64> loc(#loc798)
      %382 = "tpu.Cast"(%379) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x4x16x1470x!quant.uniform<i8:f32, 0.18704145511811024>, 194048 : i64>) -> tensor<1x4x16x1470xbf16, 5888 : i64> loc(#loc799)
      %383 = "top.Weight"() : () -> tensor<1x1x32x8xbf16, 1099515742256 : i64> loc(#loc800)
      %384 = "top.Weight"() : () -> tensor<1x1x32x8xbf16, 1099513248560 : i64> loc(#loc801)
      %385 = "top.Weight"() : () -> tensor<1x1x32x8xbf16, 1099512105744 : i64> loc(#loc802)
      %386 = "top.Weight"() : () -> tensor<1x1x32x8xbf16, 1099512272784 : i64> loc(#loc803)
      %387 = "tpu.Softmax"(%382, %383, %384, %385, %386, %0) {axis = 2 : si32, beta = 1.000000e+00 : f64, log = false, round_mode = #tpu<round_mode HalfAwayFromZero>} : (tensor<1x4x16x1470xbf16, 5888 : i64>, tensor<1x1x32x8xbf16, 1099515742256 : i64>, tensor<1x1x32x8xbf16, 1099513248560 : i64>, tensor<1x1x32x8xbf16, 1099512105744 : i64>, tensor<1x1x32x8xbf16, 1099512272784 : i64>, none) -> tensor<1x4x16x1470x!quant.calibrated<bf16<-1.000000e+00:1.000000e+00>>, 194048 : i64> loc(#loc804)
      %388 = "tpu.Cast"(%387) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x4x16x1470x!quant.calibrated<bf16<-1.000000e+00:1.000000e+00>>, 194048 : i64>) -> tensor<1x4x16x1470x!quant.uniform<i8:bf16, 0.0078740157480314959>, 5888 : i64> loc(#loc805)
      %389 = "tpu.Permute"(%388, %0) {order = [0, 2, 1, 3]} : (tensor<1x4x16x1470x!quant.uniform<i8:bf16, 0.0078740157480314959>, 5888 : i64>, none) -> tensor<1x16x4x1470x!quant.uniform<i8:bf16, 0.0078740157480314959>, 99968 : i64> loc(#loc806)
      %390 = "top.Weight"() {do_compress = true} : () -> tensor<1x1x1x5xi8, 1099511627776 : i64> loc(#loc807)
      %391 = "top.Weight"() {do_compress = true} : () -> tensor<1x1x1x16xsi8, 1099513314608 : i64> loc(#loc808)
      %392 = "tpu.Conv2D"(%389, %391, %390) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = false} : (tensor<1x16x4x1470x!quant.uniform<i8:bf16, 0.0078740157480314959>, 99968 : i64>, tensor<1x1x1x16xsi8, 1099513314608 : i64>, tensor<1x1x1x5xi8, 1099511627776 : i64>) -> tensor<1x1x4x1470x!quant.uniform<i8:f32, 0.074563416535433066>, 5888 : i64> loc(#loc809)
      %393 = "tpu.Cast"(%381) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x4x1470x!quant.uniform<i8:f32, 0.0078740157480314959>, 0 : i64>) -> tensor<1x4x1470xf32, 4398046511104 : i64> loc(#loc810)
      %394 = "tpu.Cast"(%392) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x1x4x1470x!quant.uniform<i8:f32, 0.074563416535433066>, 5888 : i64>) -> tensor<1x1x4x1470xf32, 5497558138880 : i64> loc(#loc811)
      return %394, %393 : tensor<1x1x4x1470xf32, 5497558138880 : i64>, tensor<1x4x1470xf32, 4398046511104 : i64> loc(#loc)
    } loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc2 = loc("/model.0/conv/Conv_output_0_Conv_bias_packed")
#loc3 = loc("/model.0/conv/Conv_output_0_Conv_filter_reordered")
#loc4 = loc("/model.0/act/Mul_output_0_Mul_table")
#loc5 = loc("/model.1/conv/Conv_output_0_Conv_bias_packed")
#loc6 = loc("/model.1/conv/Conv_output_0_Conv_filter_reordered")
#loc7 = loc("/model.1/act/Mul_output_0_Mul_table")
#loc8 = loc("/model.2/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc9 = loc("/model.2/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc10 = loc("/model.2/cv1/act/Mul_output_0_Mul_table")
#loc11 = loc("/model.2/cv1/act/Mul_output_0_Mul")
#loc12 = loc("load_0")
#loc13 = loc("load_/model.0/conv/Conv_output_0_Conv_filter_reordered")
#loc14 = loc("load_/model.0/conv/Conv_output_0_Conv_bias_packed")
#loc15 = loc("load_/model.0/act/Mul_output_0_Mul_table")
#loc16 = loc("/model.0/conv/Conv_output_0_Conv")
#loc17 = loc("load_/model.1/conv/Conv_output_0_Conv_filter_reordered")
#loc18 = loc("load_/model.1/conv/Conv_output_0_Conv_bias_packed")
#loc19 = loc("/model.0/act/Mul_output_0_Mul")
#loc20 = loc("load_/model.1/act/Mul_output_0_Mul_table")
#loc21 = loc("/model.1/conv/Conv_output_0_Conv")
#loc22 = loc("load_/model.2/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc23 = loc("load_/model.2/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc24 = loc("/model.1/act/Mul_output_0_Mul")
#loc25 = loc("load_/model.2/cv1/act/Mul_output_0_Mul_table")
#loc26 = loc("/model.2/cv1/conv/Conv_output_0_Conv")
#loc27 = loc("/model.2/Split_output_0_Split")
#loc28 = loc("/model.2/Split_output_1_Split")
#loc29 = loc("/model.2/m.0/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc30 = loc("/model.2/m.0/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc31 = loc("/model.2/m.0/cv1/act/Mul_output_0_Mul_table")
#loc32 = loc("/model.2/m.0/cv1/act/Mul_output_0_Mul")
#loc33 = loc("load_/model.2/Split_output_1_Split")
#loc34 = loc("load_/model.2/m.0/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc35 = loc("load_/model.2/m.0/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc36 = loc("load_/model.2/m.0/cv1/act/Mul_output_0_Mul_table")
#loc37 = loc("/model.2/m.0/cv1/conv/Conv_output_0_Conv")
#loc38 = loc("/model.2/m.0/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc39 = loc("/model.2/m.0/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc40 = loc("/model.2/m.0/cv2/act/Mul_output_0_Mul_table")
#loc41 = loc("/model.2/m.0/cv2/act/Mul_output_0_Mul")
#loc42 = loc("load_/model.2/m.0/cv1/act/Mul_output_0_Mul")
#loc43 = loc("load_/model.2/m.0/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc44 = loc("load_/model.2/m.0/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc45 = loc("load_/model.2/m.0/cv2/act/Mul_output_0_Mul_table")
#loc46 = loc("/model.2/m.0/cv2/conv/Conv_output_0_Conv")
#loc47 = loc("/model.2/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc48 = loc("/model.2/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc49 = loc("/model.2/cv2/act/Mul_output_0_Mul_table")
#loc50 = loc("/model.2/cv2/act/Mul_output_0_Mul")
#loc51 = loc("load_/model.2/m.0/cv2/act/Mul_output_0_Mul")
#loc52 = loc("load_/model.2/Split_output_0_Split")
#loc53 = loc("/model.2/m.0/Add_output_0_Add")
#loc54 = loc("load_/model.2/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc55 = loc("load_/model.2/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc56 = loc("/model.2/Concat_output_0_Concat")
#loc57 = loc("load_/model.2/cv2/act/Mul_output_0_Mul_table")
#loc58 = loc("/model.2/cv2/conv/Conv_output_0_Conv")
#loc59 = loc("/model.3/conv/Conv_output_0_Conv_bias_packed")
#loc60 = loc("/model.3/conv/Conv_output_0_Conv_filter_reordered")
#loc61 = loc("/model.3/act/Mul_output_0_Mul_table")
#loc62 = loc("/model.4/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc63 = loc("/model.4/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc64 = loc("/model.4/cv1/act/Mul_output_0_Mul_table")
#loc65 = loc("/model.4/cv1/act/Mul_output_0_Mul")
#loc66 = loc("load_/model.3/conv/Conv_output_0_Conv_filter_reordered")
#loc67 = loc("load_/model.2/cv2/act/Mul_output_0_Mul")
#loc68 = loc("load_/model.3/conv/Conv_output_0_Conv_bias_packed")
#loc69 = loc("load_/model.3/act/Mul_output_0_Mul_table")
#loc70 = loc("/model.3/conv/Conv_output_0_Conv")
#loc71 = loc("load_/model.4/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc72 = loc("load_/model.4/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc73 = loc("/model.3/act/Mul_output_0_Mul")
#loc74 = loc("load_/model.4/cv1/act/Mul_output_0_Mul_table")
#loc75 = loc("/model.4/cv1/conv/Conv_output_0_Conv")
#loc76 = loc("/model.4/Split_output_0_Split")
#loc77 = loc("/model.4/Split_output_1_Split")
#loc78 = loc("/model.4/m.0/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc79 = loc("/model.4/m.0/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc80 = loc("/model.4/m.0/cv1/conv/Conv_output_0_Conv")
#loc81 = loc("/model.4/m.0/cv1/act/Mul_output_0_Mul_table")
#loc82 = loc("/model.4/m.0/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc83 = loc("/model.4/m.0/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc84 = loc("/model.4/m.0/cv2/act/Mul_output_0_Mul_table")
#loc85 = loc("/model.4/m.0/Add_output_0_Add")
#loc86 = loc("load_/model.4/m.0/cv1/conv/Conv_output_0_Conv")
#loc87 = loc("load_/model.4/m.0/cv1/act/Mul_output_0_Mul_table")
#loc88 = loc("load_/model.4/m.0/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc89 = loc("load_/model.4/m.0/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc90 = loc("/model.4/m.0/cv1/act/Mul_output_0_Mul")
#loc91 = loc("load_/model.4/m.0/cv2/act/Mul_output_0_Mul_table")
#loc92 = loc("/model.4/m.0/cv2/conv/Conv_output_0_Conv")
#loc93 = loc("load_/model.4/Split_output_1_Split")
#loc94 = loc("/model.4/m.0/cv2/act/Mul_output_0_Mul")
#loc95 = loc("/model.4/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc96 = loc("/model.4/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc97 = loc("/model.4/cv2/act/Mul_output_0_Mul_table")
#loc98 = loc("/model.4/cv2/act/Mul_output_0_Mul")
#loc99 = loc("load_/model.4/Split_output_0_Split")
#loc100 = loc("load_/model.4/m.0/Add_output_0_Add")
#loc101 = loc("load_/model.4/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc102 = loc("load_/model.4/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc103 = loc("/model.4/Concat_output_0_Concat")
#loc104 = loc("load_/model.4/cv2/act/Mul_output_0_Mul_table")
#loc105 = loc("/model.4/cv2/conv/Conv_output_0_Conv")
#loc106 = loc("/model.5/conv/Conv_output_0_Conv_bias_packed")
#loc107 = loc("/model.5/conv/Conv_output_0_Conv_filter_reordered")
#loc108 = loc("/model.5/conv/Conv_output_0_Conv")
#loc109 = loc("/model.5/act/Mul_output_0_Mul_table")
#loc110 = loc("/model.6/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc111 = loc("/model.6/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc112 = loc("/model.6/cv1/conv/Conv_output_0_Conv")
#loc113 = loc("load_/model.5/conv/Conv_output_0_Conv")
#loc114 = loc("load_/model.5/act/Mul_output_0_Mul_table")
#loc115 = loc("load_/model.6/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc116 = loc("load_/model.6/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc117 = loc("/model.5/act/Mul_output_0_Mul")
#loc118 = loc("/model.6/cv1/act/Mul_output_0_Mul_table")
#loc119 = loc("/model.6/cv1/act/Mul_output_0_Mul")
#loc120 = loc("/model.6/Split_output_0_Split")
#loc121 = loc("/model.6/Split_output_1_Split")
#loc122 = loc("/model.6/m.0/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc123 = loc("/model.6/m.0/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc124 = loc("/model.6/m.0/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc125 = loc("/model.6/m.0/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc126 = loc("/model.6/m.0/cv1/act/Mul_output_0_Mul_table")
#loc127 = loc("/model.6/m.0/cv2/act/Mul_output_0_Mul_table")
#loc128 = loc("/model.6/m.0/m/m.0/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc129 = loc("/model.6/m.0/m/m.0/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc130 = loc("/model.6/m.0/m/m.0/cv1/act/Mul_output_0_Mul_table")
#loc131 = loc("/model.6/m.0/m/m.0/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc132 = loc("/model.6/m.0/m/m.0/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc133 = loc("/model.6/m.0/m/m.0/cv2/act/Mul_output_0_Mul_table")
#loc134 = loc("/model.6/m.0/m/m.1/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc135 = loc("/model.6/m.0/m/m.1/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc136 = loc("/model.6/m.0/m/m.1/cv1/act/Mul_output_0_Mul_table")
#loc137 = loc("/model.6/m.0/m/m.1/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc138 = loc("/model.6/m.0/m/m.1/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc139 = loc("/model.6/m.0/m/m.1/cv2/act/Mul_output_0_Mul_table")
#loc140 = loc("/model.6/m.0/cv3/conv/Conv_output_0_Conv_bias_packed")
#loc141 = loc("/model.6/m.0/cv3/conv/Conv_output_0_Conv_filter_reordered")
#loc142 = loc("/model.6/m.0/cv3/conv/Conv_output_0_Conv")
#loc143 = loc("load_/model.6/Split_output_1_Split")
#loc144 = loc("load_/model.6/m.0/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc145 = loc("load_/model.6/m.0/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc146 = loc("load_/model.6/m.0/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc147 = loc("load_/model.6/m.0/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc148 = loc("/model.6/m.0/cv1/conv/Conv_output_0_Conv")
#loc149 = loc("load_/model.6/m.0/cv1/act/Mul_output_0_Mul_table")
#loc150 = loc("/model.6/m.0/cv2/conv/Conv_output_0_Conv")
#loc151 = loc("load_/model.6/m.0/cv2/act/Mul_output_0_Mul_table")
#loc152 = loc("/model.6/m.0/cv1/act/Mul_output_0_Mul")
#loc153 = loc("load_/model.6/m.0/m/m.0/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc154 = loc("load_/model.6/m.0/m/m.0/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc155 = loc("/model.6/m.0/cv2/act/Mul_output_0_Mul")
#loc156 = loc("load_/model.6/m.0/m/m.0/cv1/act/Mul_output_0_Mul_table")
#loc157 = loc("/model.6/m.0/m/m.0/cv1/conv/Conv_output_0_Conv")
#loc158 = loc("load_/model.6/m.0/m/m.0/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc159 = loc("load_/model.6/m.0/m/m.0/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc160 = loc("/model.6/m.0/m/m.0/cv1/act/Mul_output_0_Mul")
#loc161 = loc("load_/model.6/m.0/m/m.0/cv2/act/Mul_output_0_Mul_table")
#loc162 = loc("/model.6/m.0/m/m.0/cv2/conv/Conv_output_0_Conv")
#loc163 = loc("/model.6/m.0/m/m.0/cv2/act/Mul_output_0_Mul")
#loc164 = loc("load_/model.6/m.0/m/m.1/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc165 = loc("load_/model.6/m.0/m/m.1/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc166 = loc("/model.6/m.0/m/m.0/Add_output_0_Add")
#loc167 = loc("load_/model.6/m.0/m/m.1/cv1/act/Mul_output_0_Mul_table")
#loc168 = loc("/model.6/m.0/m/m.1/cv1/conv/Conv_output_0_Conv")
#loc169 = loc("load_/model.6/m.0/m/m.1/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc170 = loc("load_/model.6/m.0/m/m.1/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc171 = loc("/model.6/m.0/m/m.1/cv1/act/Mul_output_0_Mul")
#loc172 = loc("load_/model.6/m.0/m/m.1/cv2/act/Mul_output_0_Mul_table")
#loc173 = loc("/model.6/m.0/m/m.1/cv2/conv/Conv_output_0_Conv")
#loc174 = loc("/model.6/m.0/m/m.1/cv2/act/Mul_output_0_Mul")
#loc175 = loc("/model.6/m.0/m/m.1/Add_output_0_Add")
#loc176 = loc("load_/model.6/m.0/cv3/conv/Conv_output_0_Conv_filter_reordered")
#loc177 = loc("load_/model.6/m.0/cv3/conv/Conv_output_0_Conv_bias_packed")
#loc178 = loc("/model.6/m.0/Concat_output_0_Concat")
#loc179 = loc("/model.6/m.0/cv3/act/Mul_output_0_Mul_table")
#loc180 = loc("/model.6/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc181 = loc("/model.6/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc182 = loc("/model.6/cv2/act/Mul_output_0_Mul_table")
#loc183 = loc("/model.6/cv2/act/Mul_output_0_Mul")
#loc184 = loc("load_/model.6/m.0/cv3/conv/Conv_output_0_Conv")
#loc185 = loc("load_/model.6/m.0/cv3/act/Mul_output_0_Mul_table")
#loc186 = loc("load_/model.6/Split_output_0_Split")
#loc187 = loc("load_/model.6/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc188 = loc("/model.6/m.0/cv3/act/Mul_output_0_Mul")
#loc189 = loc("load_/model.6/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc190 = loc("/model.6/Concat_output_0_Concat")
#loc191 = loc("load_/model.6/cv2/act/Mul_output_0_Mul_table")
#loc192 = loc("/model.6/cv2/conv/Conv_output_0_Conv")
#loc193 = loc("/model.7/conv/Conv_output_0_Conv_bias_packed")
#loc194 = loc("/model.7/conv/Conv_output_0_Conv_filter_reordered")
#loc195 = loc("/model.7/conv/Conv_output_0_Conv")
#loc196 = loc("/model.7/act/Mul_output_0_Mul_table")
#loc197 = loc("/model.7/act/Mul_output_0_Mul")
#loc198 = loc("/model.8/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc199 = loc("/model.8/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc200 = loc("/model.8/cv1/conv/Conv_output_0_Conv")
#loc201 = loc("/model.8/cv1/act/Mul_output_0_Mul_table")
#loc202 = loc("/model.8/cv1/act/Mul_output_0_Mul")
#loc203 = loc("/model.8/Split_output_0_Split")
#loc204 = loc("/model.8/Split_output_1_Split")
#loc205 = loc("/model.8/m.0/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc206 = loc("/model.8/m.0/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc207 = loc("/model.8/m.0/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc208 = loc("/model.8/m.0/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc209 = loc("/model.8/m.0/cv1/act/Mul_output_0_Mul_table")
#loc210 = loc("/model.8/m.0/cv2/act/Mul_output_0_Mul_table")
#loc211 = loc("/model.8/m.0/m/m.0/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc212 = loc("/model.8/m.0/m/m.0/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc213 = loc("/model.8/m.0/cv1/act/Mul_output_0_Mul")
#loc214 = loc("/model.8/m.0/cv2/act/Mul_output_0_Mul")
#loc215 = loc("/model.8/m.0/m/m.0/cv1/conv/Conv_output_0_Conv")
#loc216 = loc("load_/model.8/Split_output_1_Split")
#loc217 = loc("load_/model.8/m.0/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc218 = loc("load_/model.8/m.0/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc219 = loc("load_/model.8/m.0/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc220 = loc("load_/model.8/m.0/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc221 = loc("/model.8/m.0/cv1/conv/Conv_output_0_Conv")
#loc222 = loc("load_/model.8/m.0/cv1/act/Mul_output_0_Mul_table")
#loc223 = loc("load_/model.8/m.0/m/m.0/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc224 = loc("/model.8/m.0/cv2/conv/Conv_output_0_Conv")
#loc225 = loc("load_/model.8/m.0/cv2/act/Mul_output_0_Mul_table")
#loc226 = loc("load_/model.8/m.0/m/m.0/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc227 = loc("/model.8/m.0/m/m.0/cv1/act/Mul_output_0_Mul_table")
#loc228 = loc("/model.8/m.0/m/m.0/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc229 = loc("/model.8/m.0/m/m.0/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc230 = loc("/model.8/m.0/m/m.0/cv2/conv/Conv_output_0_Conv")
#loc231 = loc("load_/model.8/m.0/m/m.0/cv1/conv/Conv_output_0_Conv")
#loc232 = loc("load_/model.8/m.0/m/m.0/cv1/act/Mul_output_0_Mul_table")
#loc233 = loc("load_/model.8/m.0/m/m.0/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc234 = loc("load_/model.8/m.0/m/m.0/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc235 = loc("/model.8/m.0/m/m.0/cv1/act/Mul_output_0_Mul")
#loc236 = loc("/model.8/m.0/m/m.0/cv2/act/Mul_output_0_Mul_table")
#loc237 = loc("/model.8/m.0/m/m.1/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc238 = loc("/model.8/m.0/m/m.1/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc239 = loc("/model.8/m.0/m/m.0/Add_output_0_Add")
#loc240 = loc("/model.8/m.0/m/m.1/cv1/conv/Conv_output_0_Conv")
#loc241 = loc("load_/model.8/m.0/m/m.0/cv2/conv/Conv_output_0_Conv")
#loc242 = loc("load_/model.8/m.0/m/m.0/cv2/act/Mul_output_0_Mul_table")
#loc243 = loc("load_/model.8/m.0/cv1/act/Mul_output_0_Mul")
#loc244 = loc("load_/model.8/m.0/m/m.1/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc245 = loc("/model.8/m.0/m/m.0/cv2/act/Mul_output_0_Mul")
#loc246 = loc("load_/model.8/m.0/m/m.1/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc247 = loc("/model.8/m.0/m/m.1/cv1/act/Mul_output_0_Mul_table")
#loc248 = loc("/model.8/m.0/m/m.1/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc249 = loc("/model.8/m.0/m/m.1/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc250 = loc("/model.8/m.0/m/m.1/cv2/act/Mul_output_0_Mul_table")
#loc251 = loc("/model.8/m.0/cv3/conv/Conv_output_0_Conv_bias_packed")
#loc252 = loc("/model.8/m.0/cv3/conv/Conv_output_0_Conv_filter_reordered")
#loc253 = loc("/model.8/m.0/cv3/act/Mul_output_0_Mul_table")
#loc254 = loc("/model.8/m.0/cv3/act/Mul_output_0_Mul")
#loc255 = loc("load_/model.8/m.0/m/m.1/cv1/conv/Conv_output_0_Conv")
#loc256 = loc("load_/model.8/m.0/m/m.1/cv1/act/Mul_output_0_Mul_table")
#loc257 = loc("load_/model.8/m.0/m/m.1/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc258 = loc("load_/model.8/m.0/m/m.1/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc259 = loc("/model.8/m.0/m/m.1/cv1/act/Mul_output_0_Mul")
#loc260 = loc("load_/model.8/m.0/m/m.1/cv2/act/Mul_output_0_Mul_table")
#loc261 = loc("load_/model.8/m.0/cv3/conv/Conv_output_0_Conv_filter_reordered")
#loc262 = loc("/model.8/m.0/m/m.1/cv2/conv/Conv_output_0_Conv")
#loc263 = loc("load_/model.8/m.0/m/m.0/Add_output_0_Add")
#loc264 = loc("/model.8/m.0/m/m.1/cv2/act/Mul_output_0_Mul")
#loc265 = loc("load_/model.8/m.0/cv2/act/Mul_output_0_Mul")
#loc266 = loc("/model.8/m.0/m/m.1/Add_output_0_Add")
#loc267 = loc("load_/model.8/m.0/cv3/conv/Conv_output_0_Conv_bias_packed")
#loc268 = loc("/model.8/m.0/Concat_output_0_Concat")
#loc269 = loc("load_/model.8/m.0/cv3/act/Mul_output_0_Mul_table")
#loc270 = loc("/model.8/m.0/cv3/conv/Conv_output_0_Conv")
#loc271 = loc("/model.8/Concat_output_0_Concat")
#loc272 = loc("/model.8/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc273 = loc("/model.8/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc274 = loc("/model.8/cv2/conv/Conv_output_0_Conv")
#loc275 = loc("/model.8/cv2/act/Mul_output_0_Mul_table")
#loc276 = loc("/model.9/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc277 = loc("/model.9/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc278 = loc("/model.9/cv1/act/Mul_output_0_Mul_table")
#loc279 = loc("/model.9/cv1/act/Mul_output_0_Mul")
#loc280 = loc("/model.9/m/MaxPool_output_0_MaxPool")
#loc281 = loc("/model.9/m_1/MaxPool_output_0_MaxPool")
#loc282 = loc("/model.9/m_2/MaxPool_output_0_MaxPool")
#loc283 = loc("load_/model.8/cv2/conv/Conv_output_0_Conv")
#loc284 = loc("load_/model.8/cv2/act/Mul_output_0_Mul_table")
#loc285 = loc("load_/model.9/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc286 = loc("load_/model.9/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc287 = loc("/model.8/cv2/act/Mul_output_0_Mul")
#loc288 = loc("load_/model.9/cv1/act/Mul_output_0_Mul_table")
#loc289 = loc("/model.9/cv1/conv/Conv_output_0_Conv")
#loc290 = loc("/model.9/Concat_output_0_Concat")
#loc291 = loc("/model.9/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc292 = loc("/model.9/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc293 = loc("/model.9/cv2/conv/Conv_output_0_Conv")
#loc294 = loc("/model.9/cv2/act/Mul_output_0_Mul_table")
#loc295 = loc("/model.9/cv2/act/Mul_output_0_Mul")
#loc296 = loc("/model.10/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc297 = loc("/model.10/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc298 = loc("/model.10/cv1/conv/Conv_output_0_Conv")
#loc299 = loc("/model.10/cv1/act/Mul_output_0_Mul_table")
#loc300 = loc("/model.10/cv1/act/Mul_output_0_Mul")
#loc301 = loc("/model.10/Split_output_0_Split")
#loc302 = loc("/model.10/Split_output_1_Split")
#loc303 = loc("/model.10/m/m.0/attn/qkv/conv/Conv_output_0_Conv_bias_packed")
#loc304 = loc("/model.10/m/m.0/attn/qkv/conv/Conv_output_0_Conv_filter_reordered")
#loc305 = loc("/model.10/m/m.0/attn/qkv/conv/Conv_output_0_Conv")
#loc306 = loc("/model.10/m/m.0/attn/Reshape_output_0_Reshape")
#loc307 = loc("/model.10/m/m.0/attn/Split_output_0_Split")
#loc308 = loc("/model.10/m/m.0/attn/Split_output_1_Split")
#loc309 = loc("/model.10/m/m.0/attn/Split_output_1_Split/model.10/m/m.0/attn/MatMul_output_0_MatMul_bf16")
#loc310 = loc("/model.10/m/m.0/attn/Split_output_2_Split")
#loc311 = loc("/model.10/m/m.0/attn/Split_output_2_Split/model.10/m/m.0/attn/MatMul_1_output_0_MatMul_bf16")
#loc312 = loc("/model.10/m/m.0/attn/Transpose_output_0_Transpose")
#loc313 = loc("/model.10/m/m.0/attn/Transpose_output_0_Transpose/model.10/m/m.0/attn/MatMul_output_0_MatMul_bf16")
#loc314 = loc("/model.10/m/m.0/attn/Reshape_2_output_0_Reshape")
#loc315 = loc("/model.10/m/m.0/attn/MatMul_output_0_MatMul")
#loc316 = loc("/model.10/m/m.0/attn/MatMul_output_0_MatMul/model.10/m/m.0/attn/Mul_output_0_Mul_si8")
#loc317 = loc("/model.10/m/m.0/attn/pe/conv/Conv_output_0_Conv_bias_packed")
#loc318 = loc("/model.10/m/m.0/attn/pe/conv/Conv_output_0_Conv_filter_reordered")
#loc319 = loc("/model.10/m/m.0/attn/pe/conv/Conv_output_0_Conv")
#loc320 = loc("/model.10/m/m.0/attn/Mul_output_0_Mul_table")
#loc321 = loc("/model.10/m/m.0/attn/Mul_output_0_Mul")
#loc322 = loc("/model.10/m/m.0/attn/Mul_output_0_Mul/model.10/m/m.0/attn/Softmax_output_0_Softmax_bf16")
#loc323 = loc("/model.10/m/m.0/attn/Softmax_output_0_Softmax/model.10/m/m.0/attn/Softmax_output_0_Softmax_slope_table_bf16")
#loc324 = loc("/model.10/m/m.0/attn/Softmax_output_0_Softmax/model.10/m/m.0/attn/Softmax_output_0_Softmax_slope_slope_table_bf16")
#loc325 = loc("/model.10/m/m.0/attn/Softmax_output_0_Softmax/model.10/m/m.0/attn/Softmax_output_0_Softmax_pow_table_bf16")
#loc326 = loc("/model.10/m/m.0/attn/Softmax_output_0_Softmax/model.10/m/m.0/attn/Softmax_output_0_Softmax_pow_mantissa_table_bf16")
#loc327 = loc("/model.10/m/m.0/attn/Softmax_output_0_Softmax")
#loc328 = loc("/model.10/m/m.0/attn/Softmax_output_0_Softmax/model.10/m/m.0/attn/Transpose_1_output_0_Transpose_si8")
#loc329 = loc("/model.10/m/m.0/attn/Transpose_1_output_0_Transpose")
#loc330 = loc("/model.10/m/m.0/attn/Transpose_1_output_0_Transpose/model.10/m/m.0/attn/MatMul_1_output_0_MatMul_bf16")
#loc331 = loc("/model.10/m/m.0/attn/MatMul_1_output_0_MatMul")
#loc332 = loc("/model.10/m/m.0/attn/Reshape_1_output_0_Reshape")
#loc333 = loc("/model.10/m/m.0/attn/Reshape_1_output_0_Reshape/model.10/m/m.0/attn/Add_output_0_Add_si8")
#loc334 = loc("/model.10/m/m.0/attn/proj/conv/Conv_output_0_Conv_bias_packed")
#loc335 = loc("/model.10/m/m.0/attn/proj/conv/Conv_output_0_Conv_filter_reordered")
#loc336 = loc("/model.10/m/m.0/Add_output_0_Add")
#loc337 = loc("load_/model.10/m/m.0/attn/Reshape_1_output_0_Reshape/model.10/m/m.0/attn/Add_output_0_Add_si8")
#loc338 = loc("load_/model.10/m/m.0/attn/pe/conv/Conv_output_0_Conv")
#loc339 = loc("load_/model.10/m/m.0/attn/proj/conv/Conv_output_0_Conv_filter_reordered")
#loc340 = loc("load_/model.10/m/m.0/attn/proj/conv/Conv_output_0_Conv_bias_packed")
#loc341 = loc("/model.10/m/m.0/attn/Add_output_0_Add")
#loc342 = loc("load_/model.10/Split_output_1_Split")
#loc343 = loc("/model.10/m/m.0/attn/proj/conv/Conv_output_0_Conv")
#loc344 = loc("/model.10/m/m.0/ffn/ffn.0/conv/Conv_output_0_Conv_bias_packed")
#loc345 = loc("/model.10/m/m.0/ffn/ffn.0/conv/Conv_output_0_Conv_filter_reordered")
#loc346 = loc("/model.10/m/m.0/ffn/ffn.0/conv/Conv_output_0_Conv")
#loc347 = loc("/model.10/m/m.0/ffn/ffn.0/act/Mul_output_0_Mul_table")
#loc348 = loc("/model.10/m/m.0/ffn/ffn.1/conv/Conv_output_0_Conv_bias_packed")
#loc349 = loc("/model.10/m/m.0/ffn/ffn.1/conv/Conv_output_0_Conv_filter_reordered")
#loc350 = loc("/model.10/m/m.0/ffn/ffn.1/conv/Conv_output_0_Conv")
#loc351 = loc("load_/model.10/m/m.0/ffn/ffn.0/conv/Conv_output_0_Conv")
#loc352 = loc("load_/model.10/m/m.0/ffn/ffn.0/act/Mul_output_0_Mul_table")
#loc353 = loc("load_/model.10/m/m.0/ffn/ffn.1/conv/Conv_output_0_Conv_filter_reordered")
#loc354 = loc("load_/model.10/m/m.0/ffn/ffn.1/conv/Conv_output_0_Conv_bias_packed")
#loc355 = loc("/model.10/m/m.0/ffn/ffn.0/act/Mul_output_0_Mul")
#loc356 = loc("/model.10/Concat_output_0_Concat")
#loc357 = loc("load_/model.10/m/m.0/Add_output_0_Add")
#loc358 = loc("load_/model.10/m/m.0/ffn/ffn.1/conv/Conv_output_0_Conv")
#loc359 = loc("load_/model.10/Split_output_0_Split")
#loc360 = loc("/model.10/m/m.0/Add_1_output_0_Add")
#loc361 = loc("/model.10/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc362 = loc("/model.10/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc363 = loc("/model.10/cv2/conv/Conv_output_0_Conv")
#loc364 = loc("/model.10/cv2/act/Mul_output_0_Mul_table")
#loc365 = loc("/model.10/cv2/act/Mul_output_0_Mul")
#loc366 = loc("/model.11/Resize_output_0_Resize_filter_reordered")
#loc367 = loc("/model.11/Resize_output_0_Resize_bias_packed")
#loc368 = loc("/model.12/Concat_output_0_Concat")
#loc369 = loc("load_/model.10/cv2/act/Mul_output_0_Mul")
#loc370 = loc("load_/model.11/Resize_output_0_Resize_filter_reordered")
#loc371 = loc("load_/model.11/Resize_output_0_Resize_bias_packed")
#loc372 = loc("load_/model.6/cv2/act/Mul_output_0_Mul")
#loc373 = loc("/model.11/Resize_output_0_Resize")
#loc374 = loc("/model.13/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc375 = loc("/model.13/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc376 = loc("/model.13/cv1/conv/Conv_output_0_Conv")
#loc377 = loc("/model.13/cv1/act/Mul_output_0_Mul_table")
#loc378 = loc("/model.13/cv1/act/Mul_output_0_Mul")
#loc379 = loc("/model.13/Split_output_0_Split")
#loc380 = loc("/model.13/Split_output_1_Split")
#loc381 = loc("/model.13/m.0/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc382 = loc("/model.13/m.0/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc383 = loc("/model.13/m.0/cv1/conv/Conv_output_0_Conv")
#loc384 = loc("/model.13/m.0/cv1/act/Mul_output_0_Mul_table")
#loc385 = loc("/model.13/m.0/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc386 = loc("/model.13/m.0/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc387 = loc("/model.13/m.0/cv2/act/Mul_output_0_Mul_table")
#loc388 = loc("/model.13/Concat_output_0_Concat")
#loc389 = loc("load_/model.13/m.0/cv1/conv/Conv_output_0_Conv")
#loc390 = loc("load_/model.13/m.0/cv1/act/Mul_output_0_Mul_table")
#loc391 = loc("load_/model.13/m.0/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc392 = loc("load_/model.13/m.0/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc393 = loc("/model.13/m.0/cv1/act/Mul_output_0_Mul")
#loc394 = loc("load_/model.13/m.0/cv2/act/Mul_output_0_Mul_table")
#loc395 = loc("/model.13/m.0/cv2/conv/Conv_output_0_Conv")
#loc396 = loc("load_/model.13/Split_output_1_Split")
#loc397 = loc("/model.13/m.0/cv2/act/Mul_output_0_Mul")
#loc398 = loc("load_/model.13/Split_output_0_Split")
#loc399 = loc("/model.13/m.0/Add_output_0_Add")
#loc400 = loc("/model.13/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc401 = loc("/model.13/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc402 = loc("/model.13/cv2/act/Mul_output_0_Mul_table")
#loc403 = loc("/model.13/cv2/act/Mul_output_0_Mul")
#loc404 = loc("load_/model.13/Concat_output_0_Concat")
#loc405 = loc("load_/model.13/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc406 = loc("load_/model.13/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc407 = loc("load_/model.13/cv2/act/Mul_output_0_Mul_table")
#loc408 = loc("/model.13/cv2/conv/Conv_output_0_Conv")
#loc409 = loc("/model.14/Resize_output_0_Resize_filter_reordered")
#loc410 = loc("/model.14/Resize_output_0_Resize_bias_packed")
#loc411 = loc("/model.16/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc412 = loc("/model.16/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc413 = loc("/model.16/cv1/act/Mul_output_0_Mul_table")
#loc414 = loc("/model.16/cv1/act/Mul_output_0_Mul")
#loc415 = loc("load_/model.13/cv2/act/Mul_output_0_Mul")
#loc416 = loc("load_/model.14/Resize_output_0_Resize_filter_reordered")
#loc417 = loc("load_/model.14/Resize_output_0_Resize_bias_packed")
#loc418 = loc("load_/model.4/cv2/act/Mul_output_0_Mul")
#loc419 = loc("/model.14/Resize_output_0_Resize")
#loc420 = loc("load_/model.16/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc421 = loc("load_/model.16/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc422 = loc("/model.15/Concat_output_0_Concat")
#loc423 = loc("load_/model.16/cv1/act/Mul_output_0_Mul_table")
#loc424 = loc("/model.16/cv1/conv/Conv_output_0_Conv")
#loc425 = loc("/model.16/Split_output_0_Split")
#loc426 = loc("/model.16/Split_output_1_Split")
#loc427 = loc("/model.16/m.0/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc428 = loc("/model.16/m.0/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc429 = loc("/model.16/m.0/cv1/conv/Conv_output_0_Conv")
#loc430 = loc("/model.16/m.0/cv1/act/Mul_output_0_Mul_table")
#loc431 = loc("/model.16/m.0/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc432 = loc("/model.16/m.0/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc433 = loc("/model.16/m.0/cv2/act/Mul_output_0_Mul_table")
#loc434 = loc("/model.16/m.0/Add_output_0_Add")
#loc435 = loc("load_/model.16/m.0/cv1/conv/Conv_output_0_Conv")
#loc436 = loc("load_/model.16/m.0/cv1/act/Mul_output_0_Mul_table")
#loc437 = loc("load_/model.16/m.0/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc438 = loc("load_/model.16/m.0/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc439 = loc("/model.16/m.0/cv1/act/Mul_output_0_Mul")
#loc440 = loc("load_/model.16/m.0/cv2/act/Mul_output_0_Mul_table")
#loc441 = loc("/model.16/m.0/cv2/conv/Conv_output_0_Conv")
#loc442 = loc("load_/model.16/Split_output_1_Split")
#loc443 = loc("/model.16/m.0/cv2/act/Mul_output_0_Mul")
#loc444 = loc("/model.16/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc445 = loc("/model.16/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc446 = loc("/model.16/cv2/act/Mul_output_0_Mul_table")
#loc447 = loc("/model.16/cv2/act/Mul_output_0_Mul")
#loc448 = loc("load_/model.16/Split_output_0_Split")
#loc449 = loc("load_/model.16/m.0/Add_output_0_Add")
#loc450 = loc("load_/model.16/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc451 = loc("load_/model.16/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc452 = loc("/model.16/Concat_output_0_Concat")
#loc453 = loc("load_/model.16/cv2/act/Mul_output_0_Mul_table")
#loc454 = loc("/model.16/cv2/conv/Conv_output_0_Conv")
#loc455 = loc("/model.17/conv/Conv_output_0_Conv_bias_packed")
#loc456 = loc("/model.17/conv/Conv_output_0_Conv_filter_reordered")
#loc457 = loc("/model.17/conv/Conv_output_0_Conv")
#loc458 = loc("/model.23/cv2.0/cv2.0.0/conv/Conv_output_0_Conv_bias_packed")
#loc459 = loc("/model.23/cv2.0/cv2.0.0/conv/Conv_output_0_Conv_filter_reordered")
#loc460 = loc("/model.23/cv2.0/cv2.0.0/conv/Conv_output_0_Conv")
#loc461 = loc("/model.23/cv3.0/cv3.0.0/cv3.0.0.0/conv/Conv_output_0_Conv_bias_packed")
#loc462 = loc("/model.23/cv3.0/cv3.0.0/cv3.0.0.0/conv/Conv_output_0_Conv_filter_reordered")
#loc463 = loc("/model.17/act/Mul_output_0_Mul_table")
#loc464 = loc("/model.23/cv3.0/cv3.0.0/cv3.0.0.0/conv/Conv_output_0_Conv")
#loc465 = loc("/model.17/act/Mul_output_0_Mul")
#loc466 = loc("load_/model.16/cv2/act/Mul_output_0_Mul")
#loc467 = loc("load_/model.23/cv3.0/cv3.0.0/cv3.0.0.0/conv/Conv_output_0_Conv_filter_reordered")
#loc468 = loc("load_/model.23/cv3.0/cv3.0.0/cv3.0.0.0/conv/Conv_output_0_Conv_bias_packed")
#loc469 = loc("load_/model.17/conv/Conv_output_0_Conv")
#loc470 = loc("load_/model.17/act/Mul_output_0_Mul_table")
#loc471 = loc("/model.23/cv2.0/cv2.0.0/act/Mul_output_0_Mul_table")
#loc472 = loc("/model.23/cv3.0/cv3.0.0/cv3.0.0.0/act/Mul_output_0_Mul_table")
#loc473 = loc("/model.23/cv2.0/cv2.0.0/act/Mul_output_0_Mul")
#loc474 = loc("/model.23/cv3.0/cv3.0.0/cv3.0.0.0/act/Mul_output_0_Mul")
#loc475 = loc("load_/model.23/cv2.0/cv2.0.0/conv/Conv_output_0_Conv")
#loc476 = loc("load_/model.23/cv2.0/cv2.0.0/act/Mul_output_0_Mul_table")
#loc477 = loc("load_/model.23/cv3.0/cv3.0.0/cv3.0.0.0/conv/Conv_output_0_Conv")
#loc478 = loc("load_/model.23/cv3.0/cv3.0.0/cv3.0.0.0/act/Mul_output_0_Mul_table")
#loc479 = loc("/model.23/cv2.0/cv2.0.1/conv/Conv_output_0_Conv_bias_packed")
#loc480 = loc("/model.23/cv2.0/cv2.0.1/conv/Conv_output_0_Conv_filter_reordered")
#loc481 = loc("/model.23/cv3.0/cv3.0.0/cv3.0.0.1/conv/Conv_output_0_Conv_bias_packed")
#loc482 = loc("/model.23/cv3.0/cv3.0.0/cv3.0.0.1/conv/Conv_output_0_Conv_filter_reordered")
#loc483 = loc("/model.19/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc484 = loc("/model.19/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc485 = loc("/model.23/cv2.0/cv2.0.1/act/Mul_output_0_Mul_table")
#loc486 = loc("/model.23/cv3.0/cv3.0.0/cv3.0.0.1/act/Mul_output_0_Mul_table")
#loc487 = loc("/model.19/cv1/conv/Conv_output_0_Conv")
#loc488 = loc("/model.23/cv2.0/cv2.0.1/act/Mul_output_0_Mul")
#loc489 = loc("/model.23/cv3.0/cv3.0.0/cv3.0.0.1/act/Mul_output_0_Mul")
#loc490 = loc("load_/model.17/act/Mul_output_0_Mul")
#loc491 = loc("load_/model.23/cv2.0/cv2.0.0/act/Mul_output_0_Mul")
#loc492 = loc("load_/model.23/cv2.0/cv2.0.1/conv/Conv_output_0_Conv_filter_reordered")
#loc493 = loc("load_/model.23/cv2.0/cv2.0.1/conv/Conv_output_0_Conv_bias_packed")
#loc494 = loc("/model.18/Concat_output_0_Concat")
#loc495 = loc("load_/model.23/cv3.0/cv3.0.0/cv3.0.0.0/act/Mul_output_0_Mul")
#loc496 = loc("load_/model.23/cv3.0/cv3.0.0/cv3.0.0.1/conv/Conv_output_0_Conv_filter_reordered")
#loc497 = loc("load_/model.23/cv3.0/cv3.0.0/cv3.0.0.1/conv/Conv_output_0_Conv_bias_packed")
#loc498 = loc("/model.23/cv2.0/cv2.0.1/conv/Conv_output_0_Conv")
#loc499 = loc("load_/model.19/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc500 = loc("load_/model.19/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc501 = loc("/model.23/cv3.0/cv3.0.0/cv3.0.0.1/conv/Conv_output_0_Conv")
#loc502 = loc("load_/model.23/cv2.0/cv2.0.1/act/Mul_output_0_Mul_table")
#loc503 = loc("load_/model.23/cv3.0/cv3.0.0/cv3.0.0.1/act/Mul_output_0_Mul_table")
#loc504 = loc("/model.19/cv1/act/Mul_output_0_Mul_table")
#loc505 = loc("/model.23/cv2.0/cv2.0.2/Conv_output_0_Conv_bias_packed")
#loc506 = loc("/model.23/cv2.0/cv2.0.2/Conv_output_0_Conv_filter_reordered")
#loc507 = loc("/model.23/cv3.0/cv3.0.1/cv3.0.1.0/conv/Conv_output_0_Conv_bias_packed")
#loc508 = loc("/model.23/cv3.0/cv3.0.1/cv3.0.1.0/conv/Conv_output_0_Conv_filter_reordered")
#loc509 = loc("/model.19/cv1/act/Mul_output_0_Mul")
#loc510 = loc("/model.23/cv2.0/cv2.0.2/Conv_output_0_Conv")
#loc511 = loc("/model.23/cv3.0/cv3.0.1/cv3.0.1.0/conv/Conv_output_0_Conv")
#loc512 = loc("load_/model.19/cv1/conv/Conv_output_0_Conv")
#loc513 = loc("load_/model.19/cv1/act/Mul_output_0_Mul_table")
#loc514 = loc("load_/model.23/cv2.0/cv2.0.1/act/Mul_output_0_Mul")
#loc515 = loc("load_/model.23/cv2.0/cv2.0.2/Conv_output_0_Conv_filter_reordered")
#loc516 = loc("load_/model.23/cv2.0/cv2.0.2/Conv_output_0_Conv_bias_packed")
#loc517 = loc("load_/model.23/cv3.0/cv3.0.0/cv3.0.0.1/act/Mul_output_0_Mul")
#loc518 = loc("load_/model.23/cv3.0/cv3.0.1/cv3.0.1.0/conv/Conv_output_0_Conv_filter_reordered")
#loc519 = loc("load_/model.23/cv3.0/cv3.0.1/cv3.0.1.0/conv/Conv_output_0_Conv_bias_packed")
#loc520 = loc("/model.19/Split_output_0_Split")
#loc521 = loc("/model.19/Split_output_1_Split")
#loc522 = loc("/model.19/m.0/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc523 = loc("/model.19/m.0/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc524 = loc("/model.23/cv3.0/cv3.0.1/cv3.0.1.0/act/Mul_output_0_Mul_table")
#loc525 = loc("/model.23/cv3.0/cv3.0.1/cv3.0.1.1/conv/Conv_output_0_Conv_bias_packed")
#loc526 = loc("/model.23/cv3.0/cv3.0.1/cv3.0.1.1/conv/Conv_output_0_Conv_filter_reordered")
#loc527 = loc("/model.19/m.0/cv1/act/Mul_output_0_Mul_table")
#loc528 = loc("/model.23/cv3.0/cv3.0.1/cv3.0.1.1/conv/Conv_output_0_Conv")
#loc529 = loc("/model.19/m.0/cv1/act/Mul_output_0_Mul")
#loc530 = loc("load_/model.19/m.0/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc531 = loc("load_/model.19/Split_output_1_Split")
#loc532 = loc("load_/model.19/m.0/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc533 = loc("load_/model.23/cv3.0/cv3.0.1/cv3.0.1.0/conv/Conv_output_0_Conv")
#loc534 = loc("load_/model.23/cv3.0/cv3.0.1/cv3.0.1.0/act/Mul_output_0_Mul_table")
#loc535 = loc("/model.19/m.0/cv1/conv/Conv_output_0_Conv")
#loc536 = loc("load_/model.23/cv3.0/cv3.0.1/cv3.0.1.1/conv/Conv_output_0_Conv_filter_reordered")
#loc537 = loc("load_/model.23/cv3.0/cv3.0.1/cv3.0.1.1/conv/Conv_output_0_Conv_bias_packed")
#loc538 = loc("/model.23/cv3.0/cv3.0.1/cv3.0.1.0/act/Mul_output_0_Mul")
#loc539 = loc("load_/model.19/m.0/cv1/act/Mul_output_0_Mul_table")
#loc540 = loc("/model.19/m.0/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc541 = loc("/model.19/m.0/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc542 = loc("/model.23/cv3.0/cv3.0.1/cv3.0.1.1/act/Mul_output_0_Mul_table")
#loc543 = loc("/model.23/cv3.0/cv3.0.2/Conv_output_0_Conv_bias_packed")
#loc544 = loc("/model.23/cv3.0/cv3.0.2/Conv_output_0_Conv_filter_reordered")
#loc545 = loc("/model.19/m.0/cv2/conv/Conv_output_0_Conv")
#loc546 = loc("/model.23/cv3.0/cv3.0.2/Conv_output_0_Conv")
#loc547 = loc("load_/model.19/m.0/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc548 = loc("load_/model.19/m.0/cv1/act/Mul_output_0_Mul")
#loc549 = loc("load_/model.19/m.0/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc550 = loc("load_/model.23/cv3.0/cv3.0.1/cv3.0.1.1/conv/Conv_output_0_Conv")
#loc551 = loc("load_/model.23/cv3.0/cv3.0.1/cv3.0.1.1/act/Mul_output_0_Mul_table")
#loc552 = loc("load_/model.23/cv3.0/cv3.0.2/Conv_output_0_Conv_filter_reordered")
#loc553 = loc("load_/model.23/cv3.0/cv3.0.2/Conv_output_0_Conv_bias_packed")
#loc554 = loc("/model.23/cv3.0/cv3.0.1/cv3.0.1.1/act/Mul_output_0_Mul")
#loc555 = loc("/model.19/m.0/cv2/act/Mul_output_0_Mul_table")
#loc556 = loc("/model.23/Concat_output_0_Concat")
#loc557 = loc("/model.19/m.0/Add_output_0_Add")
#loc558 = loc("load_/model.19/m.0/cv2/conv/Conv_output_0_Conv")
#loc559 = loc("load_/model.19/m.0/cv2/act/Mul_output_0_Mul_table")
#loc560 = loc("load_/model.23/cv2.0/cv2.0.2/Conv_output_0_Conv")
#loc561 = loc("load_/model.23/cv3.0/cv3.0.2/Conv_output_0_Conv")
#loc562 = loc("/model.19/m.0/cv2/act/Mul_output_0_Mul")
#loc563 = loc("/model.23/Reshape_output_0_Reshape")
#loc564 = loc("/model.19/Concat_output_0_Concat")
#loc565 = loc("/model.19/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc566 = loc("/model.19/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc567 = loc("/model.19/cv2/act/Mul_output_0_Mul_table")
#loc568 = loc("/model.19/cv2/act/Mul_output_0_Mul")
#loc569 = loc("load_/model.19/Concat_output_0_Concat")
#loc570 = loc("load_/model.19/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc571 = loc("load_/model.19/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc572 = loc("load_/model.19/cv2/act/Mul_output_0_Mul_table")
#loc573 = loc("/model.19/cv2/conv/Conv_output_0_Conv")
#loc574 = loc("/model.20/conv/Conv_output_0_Conv_bias_packed")
#loc575 = loc("/model.20/conv/Conv_output_0_Conv_filter_reordered")
#loc576 = loc("/model.20/conv/Conv_output_0_Conv")
#loc577 = loc("/model.23/cv2.1/cv2.1.0/conv/Conv_output_0_Conv_bias_packed")
#loc578 = loc("/model.23/cv2.1/cv2.1.0/conv/Conv_output_0_Conv_filter_reordered")
#loc579 = loc("/model.23/cv3.1/cv3.1.0/cv3.1.0.0/conv/Conv_output_0_Conv_bias_packed")
#loc580 = loc("/model.23/cv3.1/cv3.1.0/cv3.1.0.0/conv/Conv_output_0_Conv_filter_reordered")
#loc581 = loc("/model.23/cv2.1/cv2.1.0/conv/Conv_output_0_Conv")
#loc582 = loc("/model.23/cv3.1/cv3.1.0/cv3.1.0.0/conv/Conv_output_0_Conv")
#loc583 = loc("load_/model.23/cv2.1/cv2.1.0/conv/Conv_output_0_Conv_filter_reordered")
#loc584 = loc("load_/model.23/cv2.1/cv2.1.0/conv/Conv_output_0_Conv_bias_packed")
#loc585 = loc("load_/model.19/cv2/act/Mul_output_0_Mul")
#loc586 = loc("load_/model.23/cv3.1/cv3.1.0/cv3.1.0.0/conv/Conv_output_0_Conv_filter_reordered")
#loc587 = loc("load_/model.23/cv3.1/cv3.1.0/cv3.1.0.0/conv/Conv_output_0_Conv_bias_packed")
#loc588 = loc("/model.20/act/Mul_output_0_Mul_table")
#loc589 = loc("/model.23/cv2.1/cv2.1.0/act/Mul_output_0_Mul_table")
#loc590 = loc("/model.23/cv3.1/cv3.1.0/cv3.1.0.0/act/Mul_output_0_Mul_table")
#loc591 = loc("/model.23/cv2.1/cv2.1.1/conv/Conv_output_0_Conv_bias_packed")
#loc592 = loc("/model.23/cv2.1/cv2.1.1/conv/Conv_output_0_Conv_filter_reordered")
#loc593 = loc("/model.23/cv3.1/cv3.1.0/cv3.1.0.1/conv/Conv_output_0_Conv_bias_packed")
#loc594 = loc("/model.23/cv3.1/cv3.1.0/cv3.1.0.1/conv/Conv_output_0_Conv_filter_reordered")
#loc595 = loc("/model.21/Concat_output_0_Concat")
#loc596 = loc("/model.23/cv2.1/cv2.1.1/conv/Conv_output_0_Conv")
#loc597 = loc("/model.23/cv3.1/cv3.1.0/cv3.1.0.1/conv/Conv_output_0_Conv")
#loc598 = loc("load_/model.20/conv/Conv_output_0_Conv")
#loc599 = loc("load_/model.20/act/Mul_output_0_Mul_table")
#loc600 = loc("load_/model.23/cv2.1/cv2.1.0/conv/Conv_output_0_Conv")
#loc601 = loc("load_/model.23/cv2.1/cv2.1.0/act/Mul_output_0_Mul_table")
#loc602 = loc("/model.20/act/Mul_output_0_Mul")
#loc603 = loc("load_/model.23/cv3.1/cv3.1.0/cv3.1.0.0/conv/Conv_output_0_Conv")
#loc604 = loc("load_/model.23/cv3.1/cv3.1.0/cv3.1.0.0/act/Mul_output_0_Mul_table")
#loc605 = loc("/model.23/cv2.1/cv2.1.0/act/Mul_output_0_Mul")
#loc606 = loc("/model.23/cv3.1/cv3.1.0/cv3.1.0.0/act/Mul_output_0_Mul")
#loc607 = loc("load_/model.23/cv2.1/cv2.1.1/conv/Conv_output_0_Conv_filter_reordered")
#loc608 = loc("load_/model.23/cv2.1/cv2.1.1/conv/Conv_output_0_Conv_bias_packed")
#loc609 = loc("load_/model.23/cv3.1/cv3.1.0/cv3.1.0.1/conv/Conv_output_0_Conv_filter_reordered")
#loc610 = loc("load_/model.23/cv3.1/cv3.1.0/cv3.1.0.1/conv/Conv_output_0_Conv_bias_packed")
#loc611 = loc("/model.22/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc612 = loc("/model.22/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc613 = loc("/model.22/cv1/conv/Conv_output_0_Conv")
#loc614 = loc("/model.23/cv2.1/cv2.1.1/act/Mul_output_0_Mul_table")
#loc615 = loc("/model.23/cv3.1/cv3.1.0/cv3.1.0.1/act/Mul_output_0_Mul_table")
#loc616 = loc("/model.22/cv1/act/Mul_output_0_Mul_table")
#loc617 = loc("/model.23/cv2.1/cv2.1.2/Conv_output_0_Conv_bias_packed")
#loc618 = loc("/model.23/cv2.1/cv2.1.2/Conv_output_0_Conv_filter_reordered")
#loc619 = loc("/model.23/cv3.1/cv3.1.1/cv3.1.1.0/conv/Conv_output_0_Conv_bias_packed")
#loc620 = loc("/model.23/cv3.1/cv3.1.1/cv3.1.1.0/conv/Conv_output_0_Conv_filter_reordered")
#loc621 = loc("/model.22/cv1/act/Mul_output_0_Mul")
#loc622 = loc("/model.23/cv2.1/cv2.1.2/Conv_output_0_Conv")
#loc623 = loc("/model.23/cv3.1/cv3.1.1/cv3.1.1.0/conv/Conv_output_0_Conv")
#loc624 = loc("load_/model.23/cv2.1/cv2.1.1/conv/Conv_output_0_Conv")
#loc625 = loc("load_/model.23/cv2.1/cv2.1.1/act/Mul_output_0_Mul_table")
#loc626 = loc("load_/model.23/cv3.1/cv3.1.0/cv3.1.0.1/conv/Conv_output_0_Conv")
#loc627 = loc("load_/model.23/cv3.1/cv3.1.0/cv3.1.0.1/act/Mul_output_0_Mul_table")
#loc628 = loc("/model.23/cv2.1/cv2.1.1/act/Mul_output_0_Mul")
#loc629 = loc("load_/model.22/cv1/conv/Conv_output_0_Conv")
#loc630 = loc("load_/model.22/cv1/act/Mul_output_0_Mul_table")
#loc631 = loc("/model.23/cv3.1/cv3.1.0/cv3.1.0.1/act/Mul_output_0_Mul")
#loc632 = loc("load_/model.23/cv2.1/cv2.1.2/Conv_output_0_Conv_filter_reordered")
#loc633 = loc("load_/model.23/cv2.1/cv2.1.2/Conv_output_0_Conv_bias_packed")
#loc634 = loc("load_/model.23/cv3.1/cv3.1.1/cv3.1.1.0/conv/Conv_output_0_Conv_filter_reordered")
#loc635 = loc("load_/model.23/cv3.1/cv3.1.1/cv3.1.1.0/conv/Conv_output_0_Conv_bias_packed")
#loc636 = loc("/model.22/Split_output_0_Split")
#loc637 = loc("/model.22/Split_output_1_Split")
#loc638 = loc("/model.22/m.0/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc639 = loc("/model.22/m.0/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc640 = loc("/model.22/m.0/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc641 = loc("/model.22/m.0/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc642 = loc("/model.22/m.0/cv1/conv/Conv_output_0_Conv")
#loc643 = loc("/model.22/m.0/cv2/conv/Conv_output_0_Conv")
#loc644 = loc("load_/model.22/m.0/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc645 = loc("load_/model.22/m.0/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc646 = loc("load_/model.22/Split_output_1_Split")
#loc647 = loc("load_/model.22/m.0/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc648 = loc("load_/model.22/m.0/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc649 = loc("/model.23/cv3.1/cv3.1.1/cv3.1.1.0/act/Mul_output_0_Mul_table")
#loc650 = loc("/model.23/cv3.1/cv3.1.1/cv3.1.1.1/conv/Conv_output_0_Conv_bias_packed")
#loc651 = loc("/model.23/cv3.1/cv3.1.1/cv3.1.1.1/conv/Conv_output_0_Conv_filter_reordered")
#loc652 = loc("/model.22/m.0/cv1/act/Mul_output_0_Mul_table")
#loc653 = loc("/model.22/m.0/cv2/act/Mul_output_0_Mul_table")
#loc654 = loc("/model.22/m.0/m/m.0/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc655 = loc("/model.22/m.0/m/m.0/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc656 = loc("/model.23/cv3.1/cv3.1.1/cv3.1.1.1/act/Mul_output_0_Mul_table")
#loc657 = loc("/model.23/cv3.1/cv3.1.2/Conv_output_0_Conv_bias_packed")
#loc658 = loc("/model.23/cv3.1/cv3.1.2/Conv_output_0_Conv_filter_reordered")
#loc659 = loc("/model.22/m.0/cv1/act/Mul_output_0_Mul")
#loc660 = loc("/model.22/m.0/cv2/act/Mul_output_0_Mul")
#loc661 = loc("/model.22/m.0/m/m.0/cv1/conv/Conv_output_0_Conv")
#loc662 = loc("/model.23/cv3.1/cv3.1.2/Conv_output_0_Conv")
#loc663 = loc("load_/model.23/cv3.1/cv3.1.1/cv3.1.1.0/conv/Conv_output_0_Conv")
#loc664 = loc("load_/model.23/cv3.1/cv3.1.1/cv3.1.1.0/act/Mul_output_0_Mul_table")
#loc665 = loc("load_/model.23/cv3.1/cv3.1.1/cv3.1.1.1/conv/Conv_output_0_Conv_filter_reordered")
#loc666 = loc("load_/model.23/cv3.1/cv3.1.1/cv3.1.1.1/conv/Conv_output_0_Conv_bias_packed")
#loc667 = loc("/model.23/cv3.1/cv3.1.1/cv3.1.1.0/act/Mul_output_0_Mul")
#loc668 = loc("load_/model.22/m.0/cv1/conv/Conv_output_0_Conv")
#loc669 = loc("load_/model.22/m.0/cv1/act/Mul_output_0_Mul_table")
#loc670 = loc("load_/model.22/m.0/m/m.0/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc671 = loc("/model.23/cv3.1/cv3.1.1/cv3.1.1.1/conv/Conv_output_0_Conv")
#loc672 = loc("load_/model.22/m.0/cv2/conv/Conv_output_0_Conv")
#loc673 = loc("load_/model.22/m.0/cv2/act/Mul_output_0_Mul_table")
#loc674 = loc("load_/model.22/m.0/m/m.0/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc675 = loc("load_/model.23/cv3.1/cv3.1.1/cv3.1.1.1/act/Mul_output_0_Mul_table")
#loc676 = loc("load_/model.23/cv3.1/cv3.1.2/Conv_output_0_Conv_filter_reordered")
#loc677 = loc("load_/model.23/cv3.1/cv3.1.2/Conv_output_0_Conv_bias_packed")
#loc678 = loc("/model.23/cv3.1/cv3.1.1/cv3.1.1.1/act/Mul_output_0_Mul")
#loc679 = loc("/model.22/m.0/m/m.0/cv1/act/Mul_output_0_Mul_table")
#loc680 = loc("/model.22/m.0/m/m.0/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc681 = loc("/model.22/m.0/m/m.0/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc682 = loc("/model.23/Concat_1_output_0_Concat")
#loc683 = loc("/model.22/m.0/m/m.0/cv2/conv/Conv_output_0_Conv")
#loc684 = loc("load_/model.22/m.0/m/m.0/cv1/conv/Conv_output_0_Conv")
#loc685 = loc("load_/model.22/m.0/m/m.0/cv1/act/Mul_output_0_Mul_table")
#loc686 = loc("load_/model.23/cv2.1/cv2.1.2/Conv_output_0_Conv")
#loc687 = loc("load_/model.23/cv3.1/cv3.1.2/Conv_output_0_Conv")
#loc688 = loc("/model.22/m.0/m/m.0/cv1/act/Mul_output_0_Mul")
#loc689 = loc("load_/model.22/m.0/m/m.0/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc690 = loc("load_/model.22/m.0/m/m.0/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc691 = loc("/model.23/Reshape_1_output_0_Reshape")
#loc692 = loc("/model.22/m.0/m/m.0/cv2/act/Mul_output_0_Mul_table")
#loc693 = loc("/model.22/m.0/m/m.1/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc694 = loc("/model.22/m.0/m/m.1/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc695 = loc("/model.22/m.0/m/m.0/Add_output_0_Add")
#loc696 = loc("/model.22/m.0/m/m.1/cv1/conv/Conv_output_0_Conv")
#loc697 = loc("load_/model.22/m.0/m/m.0/cv2/conv/Conv_output_0_Conv")
#loc698 = loc("load_/model.22/m.0/m/m.0/cv2/act/Mul_output_0_Mul_table")
#loc699 = loc("load_/model.22/m.0/cv1/act/Mul_output_0_Mul")
#loc700 = loc("load_/model.22/m.0/m/m.1/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc701 = loc("/model.22/m.0/m/m.0/cv2/act/Mul_output_0_Mul")
#loc702 = loc("load_/model.22/m.0/m/m.1/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc703 = loc("/model.22/m.0/m/m.1/cv1/act/Mul_output_0_Mul_table")
#loc704 = loc("/model.22/m.0/m/m.1/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc705 = loc("/model.22/m.0/m/m.1/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc706 = loc("/model.22/m.0/m/m.1/cv2/act/Mul_output_0_Mul_table")
#loc707 = loc("/model.22/m.0/cv3/conv/Conv_output_0_Conv_bias_packed")
#loc708 = loc("/model.22/m.0/cv3/conv/Conv_output_0_Conv_filter_reordered")
#loc709 = loc("/model.22/m.0/cv3/act/Mul_output_0_Mul_table")
#loc710 = loc("/model.22/m.0/cv3/act/Mul_output_0_Mul")
#loc711 = loc("load_/model.22/m.0/m/m.1/cv1/conv/Conv_output_0_Conv")
#loc712 = loc("load_/model.22/m.0/m/m.1/cv1/act/Mul_output_0_Mul_table")
#loc713 = loc("load_/model.22/m.0/m/m.1/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc714 = loc("load_/model.22/m.0/m/m.1/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc715 = loc("/model.22/m.0/m/m.1/cv1/act/Mul_output_0_Mul")
#loc716 = loc("load_/model.22/m.0/m/m.1/cv2/act/Mul_output_0_Mul_table")
#loc717 = loc("load_/model.22/m.0/cv3/conv/Conv_output_0_Conv_filter_reordered")
#loc718 = loc("/model.22/m.0/m/m.1/cv2/conv/Conv_output_0_Conv")
#loc719 = loc("load_/model.22/m.0/m/m.0/Add_output_0_Add")
#loc720 = loc("/model.22/m.0/m/m.1/cv2/act/Mul_output_0_Mul")
#loc721 = loc("load_/model.22/m.0/cv2/act/Mul_output_0_Mul")
#loc722 = loc("/model.22/m.0/m/m.1/Add_output_0_Add")
#loc723 = loc("load_/model.22/m.0/cv3/conv/Conv_output_0_Conv_bias_packed")
#loc724 = loc("/model.22/m.0/Concat_output_0_Concat")
#loc725 = loc("load_/model.22/m.0/cv3/act/Mul_output_0_Mul_table")
#loc726 = loc("/model.22/m.0/cv3/conv/Conv_output_0_Conv")
#loc727 = loc("/model.22/Concat_output_0_Concat")
#loc728 = loc("/model.22/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc729 = loc("/model.22/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc730 = loc("/model.22/cv2/conv/Conv_output_0_Conv")
#loc731 = loc("/model.22/cv2/act/Mul_output_0_Mul_table")
#loc732 = loc("/model.22/cv2/act/Mul_output_0_Mul")
#loc733 = loc("/model.23/cv2.2/cv2.2.0/conv/Conv_output_0_Conv_bias_packed")
#loc734 = loc("/model.23/cv2.2/cv2.2.0/conv/Conv_output_0_Conv_filter_reordered")
#loc735 = loc("/model.23/cv2.2/cv2.2.0/conv/Conv_output_0_Conv")
#loc736 = loc("/model.23/cv3.2/cv3.2.0/cv3.2.0.0/conv/Conv_output_0_Conv_bias_packed")
#loc737 = loc("/model.23/cv3.2/cv3.2.0/cv3.2.0.0/conv/Conv_output_0_Conv_filter_reordered")
#loc738 = loc("/model.23/cv2.2/cv2.2.0/act/Mul_output_0_Mul_table")
#loc739 = loc("/model.23/cv3.2/cv3.2.0/cv3.2.0.0/act/Mul_output_0_Mul_table")
#loc740 = loc("/model.23/cv2.2/cv2.2.1/conv/Conv_output_0_Conv_bias_packed")
#loc741 = loc("/model.23/cv2.2/cv2.2.1/conv/Conv_output_0_Conv_filter_reordered")
#loc742 = loc("/model.23/cv3.2/cv3.2.0/cv3.2.0.1/conv/Conv_output_0_Conv_bias_packed")
#loc743 = loc("/model.23/cv3.2/cv3.2.0/cv3.2.0.1/conv/Conv_output_0_Conv_filter_reordered")
#loc744 = loc("/model.23/cv2.2/cv2.2.1/act/Mul_output_0_Mul_table")
#loc745 = loc("/model.23/cv3.2/cv3.2.0/cv3.2.0.1/act/Mul_output_0_Mul_table")
#loc746 = loc("/model.23/cv2.2/cv2.2.2/Conv_output_0_Conv_bias_packed")
#loc747 = loc("/model.23/cv2.2/cv2.2.2/Conv_output_0_Conv_filter_reordered")
#loc748 = loc("/model.23/cv3.2/cv3.2.1/cv3.2.1.0/conv/Conv_output_0_Conv_bias_packed")
#loc749 = loc("/model.23/cv3.2/cv3.2.1/cv3.2.1.0/conv/Conv_output_0_Conv_filter_reordered")
#loc750 = loc("/model.23/cv3.2/cv3.2.1/cv3.2.1.0/act/Mul_output_0_Mul_table")
#loc751 = loc("/model.23/cv3.2/cv3.2.1/cv3.2.1.1/conv/Conv_output_0_Conv_bias_packed")
#loc752 = loc("/model.23/cv3.2/cv3.2.1/cv3.2.1.1/conv/Conv_output_0_Conv_filter_reordered")
#loc753 = loc("/model.23/cv3.2/cv3.2.1/cv3.2.1.1/act/Mul_output_0_Mul_table")
#loc754 = loc("/model.23/cv3.2/cv3.2.2/Conv_output_0_Conv_bias_packed")
#loc755 = loc("/model.23/cv3.2/cv3.2.2/Conv_output_0_Conv_filter_reordered")
#loc756 = loc("/model.23/cv2.2/cv2.2.2/Conv_output_0_Conv")
#loc757 = loc("/model.23/cv3.2/cv3.2.2/Conv_output_0_Conv")
#loc758 = loc("load_/model.22/cv2/act/Mul_output_0_Mul")
#loc759 = loc("load_/model.23/cv3.2/cv3.2.0/cv3.2.0.0/conv/Conv_output_0_Conv_bias_packed")
#loc760 = loc("load_/model.23/cv3.2/cv3.2.0/cv3.2.0.0/conv/Conv_output_0_Conv_filter_reordered")
#loc761 = loc("load_/model.23/cv2.2/cv2.2.0/conv/Conv_output_0_Conv")
#loc762 = loc("load_/model.23/cv2.2/cv2.2.0/act/Mul_output_0_Mul_table")
#loc763 = loc("/model.23/cv3.2/cv3.2.0/cv3.2.0.0/conv/Conv_output_0_Conv")
#loc764 = loc("load_/model.23/cv3.2/cv3.2.0/cv3.2.0.0/act/Mul_output_0_Mul_table")
#loc765 = loc("/model.23/cv2.2/cv2.2.0/act/Mul_output_0_Mul")
#loc766 = loc("load_/model.23/cv2.2/cv2.2.1/conv/Conv_output_0_Conv_filter_reordered")
#loc767 = loc("load_/model.23/cv2.2/cv2.2.1/conv/Conv_output_0_Conv_bias_packed")
#loc768 = loc("/model.23/cv3.2/cv3.2.0/cv3.2.0.0/act/Mul_output_0_Mul")
#loc769 = loc("load_/model.23/cv3.2/cv3.2.0/cv3.2.0.1/conv/Conv_output_0_Conv_filter_reordered")
#loc770 = loc("load_/model.23/cv3.2/cv3.2.0/cv3.2.0.1/conv/Conv_output_0_Conv_bias_packed")
#loc771 = loc("/model.23/cv2.2/cv2.2.1/conv/Conv_output_0_Conv")
#loc772 = loc("load_/model.23/cv2.2/cv2.2.1/act/Mul_output_0_Mul_table")
#loc773 = loc("/model.23/cv3.2/cv3.2.0/cv3.2.0.1/conv/Conv_output_0_Conv")
#loc774 = loc("load_/model.23/cv3.2/cv3.2.0/cv3.2.0.1/act/Mul_output_0_Mul_table")
#loc775 = loc("/model.23/cv2.2/cv2.2.1/act/Mul_output_0_Mul")
#loc776 = loc("load_/model.23/cv2.2/cv2.2.2/Conv_output_0_Conv_filter_reordered")
#loc777 = loc("load_/model.23/cv2.2/cv2.2.2/Conv_output_0_Conv_bias_packed")
#loc778 = loc("/model.23/cv3.2/cv3.2.0/cv3.2.0.1/act/Mul_output_0_Mul")
#loc779 = loc("load_/model.23/cv3.2/cv3.2.1/cv3.2.1.0/conv/Conv_output_0_Conv_filter_reordered")
#loc780 = loc("load_/model.23/cv3.2/cv3.2.1/cv3.2.1.0/conv/Conv_output_0_Conv_bias_packed")
#loc781 = loc("load_/model.23/cv3.2/cv3.2.1/cv3.2.1.0/act/Mul_output_0_Mul_table")
#loc782 = loc("/model.23/cv3.2/cv3.2.1/cv3.2.1.0/conv/Conv_output_0_Conv")
#loc783 = loc("load_/model.23/cv3.2/cv3.2.1/cv3.2.1.1/conv/Conv_output_0_Conv_filter_reordered")
#loc784 = loc("load_/model.23/cv3.2/cv3.2.1/cv3.2.1.1/conv/Conv_output_0_Conv_bias_packed")
#loc785 = loc("/model.23/cv3.2/cv3.2.1/cv3.2.1.0/act/Mul_output_0_Mul")
#loc786 = loc("load_/model.23/cv3.2/cv3.2.1/cv3.2.1.1/act/Mul_output_0_Mul_table")
#loc787 = loc("/model.23/cv3.2/cv3.2.1/cv3.2.1.1/conv/Conv_output_0_Conv")
#loc788 = loc("load_/model.23/cv3.2/cv3.2.2/Conv_output_0_Conv_filter_reordered")
#loc789 = loc("load_/model.23/cv3.2/cv3.2.2/Conv_output_0_Conv_bias_packed")
#loc790 = loc("/model.23/cv3.2/cv3.2.1/cv3.2.1.1/act/Mul_output_0_Mul")
#loc791 = loc("/model.23/Concat_2_output_0_Concat")
#loc792 = loc("/model.23/Reshape_2_output_0_Reshape")
#loc793 = loc("/model.23/Concat_3_output_0_Concat")
#loc794 = loc("/model.23/Split_output_0_Split")
#loc795 = loc("/model.23/Split_output_1_Split")
#loc796 = loc("/model.23/dfl/Reshape_output_0_Reshape")
#loc797 = loc("/model.23/Sigmoid_output_0_Sigmoid_table")
#loc798 = loc("/model.23/Sigmoid_output_0_Sigmoid")
#loc799 = loc("/model.23/dfl/Transpose_output_0_Transpose/model.23/dfl/Softmax_output_0_Softmax_bf16_/model.23/dfl/Transpose_output_0_Transpose")
#loc800 = loc("/model.23/dfl/Softmax_output_0_Softmax/model.23/dfl/Softmax_output_0_Softmax_slope_table_bf16")
#loc801 = loc("/model.23/dfl/Softmax_output_0_Softmax/model.23/dfl/Softmax_output_0_Softmax_slope_slope_table_bf16")
#loc802 = loc("/model.23/dfl/Softmax_output_0_Softmax/model.23/dfl/Softmax_output_0_Softmax_pow_table_bf16")
#loc803 = loc("/model.23/dfl/Softmax_output_0_Softmax/model.23/dfl/Softmax_output_0_Softmax_pow_mantissa_table_bf16")
#loc804 = loc("/model.23/dfl/Softmax_output_0_Softmax_/model.23/dfl/Transpose_output_0_Transpose_/model.23/dfl/Transpose_output_0_Transpose/model.23/dfl/Softmax_output_0_Softmax_bf16")
#loc805 = loc("/model.23/dfl/Softmax_output_0_Softmax/model.23/dfl/conv/Conv_output_0_Conv_si8_/model.23/dfl/Transpose_output_0_Transpose_/model.23/dfl/Transpose_output_0_Transpose/model.23/dfl/Softmax_output_0_Softmax_bf16_/model.23/dfl/Softmax_output_0_Softmax")
#loc806 = loc("/model.23/dfl/Transpose_output_0_Transpose_/model.23/dfl/Transpose_output_0_Transpose/model.23/dfl/Softmax_output_0_Softmax_bf16_/model.23/dfl/Softmax_output_0_Softmax_/model.23/dfl/Softmax_output_0_Softmax/model.23/dfl/conv/Conv_output_0_Conv_si8")
#loc807 = loc("/model.23/dfl/conv/Conv_output_0_Conv_bias_packed")
#loc808 = loc("/model.23/dfl/conv/Conv_output_0_Conv_filter_reordered")
#loc809 = loc("/model.23/dfl/conv/Conv_output_0_Conv")
#loc810 = loc("/model.23/Sigmoid_output_0_Sigmoid_f32")
#loc811 = loc("/model.23/dfl/conv/Conv_output_0_Conv_f32")
#loc812 = loc(fused[#loc213, #loc214, #loc215])
#loc813 = loc(fused[#loc239, #loc240])
#loc814 = loc(fused[#loc279, #loc280, #loc281, #loc282])
#loc815 = loc(fused[#loc464, #loc465])
#loc816 = loc(fused[#loc473, #loc474])
#loc817 = loc(fused[#loc487, #loc488, #loc489])
#loc818 = loc(fused[#loc509, #loc510, #loc511])
#loc819 = loc(fused[#loc528, #loc529])
#loc820 = loc(fused[#loc545, #loc546])
#loc821 = loc(fused[#loc556, #loc557])
#loc822 = loc(fused[#loc581, #loc582])
#loc823 = loc(fused[#loc595, #loc596, #loc597])
#loc824 = loc(fused[#loc621, #loc622, #loc623])
#loc825 = loc(fused[#loc642, #loc643])
#loc826 = loc(fused[#loc659, #loc660, #loc661, #loc662])
#loc827 = loc(fused[#loc682, #loc683])
#loc828 = loc(fused[#loc695, #loc696])
#loc829 = loc(fused[#loc756, #loc757])

