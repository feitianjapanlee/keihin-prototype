#loc = loc(unknown)
#loc1 = loc("images")
module @yolov8n attributes {module.FLOPs = 1440546520 : i64, module.addr_mode = "basic", module.asymmetric = false, module.chip = "cv181x", module.cores = 1 : i64, module.devices = 1 : i64, module.high_precision = false, module.inputs = ["images"], module.mode = "INT8", module.outputs = ["/model.22/dfl/conv/Conv_output_0_Conv_f32", "/model.22/Sigmoid_output_0_Sigmoid_f32"], module.platform = "ONNX", module.q_group_size = 0 : i64, module.state = "TPU_ADDRESSED", module.top_run_mode = "STATIC", module.weight_file = "yolov8n_tpu_addressed_cv181x_int8_sym_weight.npz"} {
  module @yolov8n attributes {module.coeff_addr = 1099511627776 : i64, module.coeff_size = 3070208 : i64, module.device_id = 0 : i64, module.neuron_size = 427008 : i64, module.private_size = 0 : i64, module.step = 0 : i64} {
    func.func @main(%arg0: tensor<1x3x224x320xf32> loc(unknown)) -> (tensor<1x1x4x1470xf32, 5497558138880 : i64>, tensor<1x4x1470xf32, 4398046511104 : i64>) {
      %0 = "top.Input"(%arg0) {channel_format = "nchw", do_preprocess = true, keep_aspect_ratio = true, keep_ratio_mode = "letterbox", mean = [0.000000e+00, 0.000000e+00, 0.000000e+00], pad_type = "center", pad_value = 0 : i64, pixel_format = "rgb", resize_dims = [224, 320], scale = [0.0039215688593685627, 0.0039215688593685627, 0.0039215688593685627]} : (tensor<1x3x224x320xf32>) -> tensor<1x3x224x320x!quant.uniform<i8:f32, 0.0078740157480314959>, 3298534883328 : i64> loc(#loc1)
      %1:2 = call @subfunc_0(%0) : (tensor<1x3x224x320x!quant.uniform<i8:f32, 0.0078740157480314959>, 3298534883328 : i64>) -> (tensor<1x1x4x1470xf32, 5497558138880 : i64>, tensor<1x4x1470xf32, 4398046511104 : i64>) loc(#loc)
      return %1#0, %1#1 : tensor<1x1x4x1470xf32, 5497558138880 : i64>, tensor<1x4x1470xf32, 4398046511104 : i64> loc(#loc)
    } loc(#loc)
    func.func @subfunc_0(%arg0: tensor<1x3x224x320x!quant.uniform<i8:f32, 0.0078740157480314959>, 3298534883328 : i64> loc("images")) -> (tensor<1x1x4x1470xf32, 5497558138880 : i64>, tensor<1x4x1470xf32, 4398046511104 : i64>) attributes {id = 0 : i64, mode = #tpu<run_mode TPU_STATIC>, next_index = array<i32: -1>} {
      %0 = "top.None"() : () -> none loc(#loc)
      %1 = "top.Weight"() : () -> tensor<1x16x1x9xi8, 1099513056272 : i64> loc(#loc2)
      %2 = "top.Weight"() : () -> tensor<1x16x9x4xsi8, 1099513023776 : i64> loc(#loc3)
      %3 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099512746352 : i64> loc(#loc4)
      %4 = "top.Weight"() : () -> tensor<1x32x1x9xi8, 1099513145184 : i64> loc(#loc5)
      %5 = "top.Weight"() : () -> tensor<1x32x9x16xsi8, 1099514647552 : i64> loc(#loc6)
      %6 = "tpu.Group"(%arg0) ({
        %266 = "tpu.Load"(%arg0) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 11200, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [3], d_idx = [0], d_slice = [1], h_idx = [0, 29, 61, 93, 125, 157, 189], h_slice = [32, 35, 35, 35, 35, 35, 35], w_idx = [0], w_slice = [320], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 4 : i64} : (tensor<1x3x224x320x!quant.uniform<i8:f32, 0.0078740157480314959>, 3298534883328 : i64>) -> tensor<1x3x224x320x!quant.uniform<i8:f32, 0.0078740157480314959>> loc(#loc8)
        %267 = "tpu.Load"(%2) {do_bcast = false, ginfo = #tpu.lg<out_addr = 11776, out_size = 72, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [16], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [4], id = 1, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x16x9x4xsi8, 1099513023776 : i64>) -> tensor<1x16x9x4xsi8> loc(#loc9)
        %268 = "tpu.Load"(%1) {do_bcast = false, ginfo = #tpu.lg<out_addr = 11904, out_size = 18, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [16], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 2, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x16x1x9xi8, 1099513056272 : i64>) -> tensor<1x16x1x9xi8> loc(#loc10)
        %269 = "tpu.Load"(%3) {do_bcast = true, ginfo = #tpu.lg<out_addr = 31232, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 3, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099512746352 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc11)
        %270 = "tpu.Conv2D"(%266, %267, %268) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 5440, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [16], d_idx = [0], d_slice = [1], h_idx = [0, 15, 31, 47, 63, 79, 95], h_slice = [16, 17, 17, 17, 17, 17, 17], w_idx = [0], w_slice = [160], id = 4, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [2, 2], support_compress = true, use_3ic_optimize = 4 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x3x224x320x!quant.uniform<i8:f32, 0.0078740157480314959>>, tensor<1x16x9x4xsi8>, tensor<1x16x1x9xi8>) -> tensor<1x16x112x160x!quant.uniform<i8:f32, 0.30922601732283467>> loc(#loc12)
        %271 = "tpu.Load"(%5) {do_bcast = false, ginfo = #tpu.lg<out_addr = 11200, out_size = 576, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [16], id = 5, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x32x9x16xsi8, 1099514647552 : i64>) -> tensor<1x32x9x16xsi8> loc(#loc13)
        %272 = "tpu.Load"(%4) {do_bcast = false, ginfo = #tpu.lg<out_addr = 11856, out_size = 36, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 6, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x32x1x9xi8, 1099513145184 : i64>) -> tensor<1x32x1x9xi8> loc(#loc14)
        %273 = "tpu.Lut"(%270, %269) {ginfo = #tpu.lg<out_addr = 20480, out_size = 5440, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [16], d_idx = [0], d_slice = [1], h_idx = [0, 15, 31, 47, 63, 79, 95], h_slice = [16, 17, 17, 17, 17, 17, 17], w_idx = [0], w_slice = [160], id = 7, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x16x112x160x!quant.uniform<i8:f32, 0.30922601732283467>>, tensor<1x1x1x256xsi8>) -> tensor<1x16x112x160x!quant.uniform<i8:f32, 0.25628297322834642>> loc(#loc15)
        %274 = "tpu.Conv2D"(%273, %271, %272) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 28672, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0, 8, 16, 24, 32, 40, 48], h_slice = [8, 8, 8, 8, 8, 8, 8], w_idx = [0], w_slice = [80], id = 8, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [2, 2], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x16x112x160x!quant.uniform<i8:f32, 0.25628297322834642>>, tensor<1x32x9x16xsi8>, tensor<1x32x1x9xi8>) -> tensor<1x32x56x80x!quant.uniform<i8:f32, 0.96238692440944873>> loc(#loc7)
        %275 = "tpu.Store"(%274, %0) {ginfo = #tpu.lg<out_addr = 28672, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0, 8, 16, 24, 32, 40, 48], h_slice = [8, 8, 8, 8, 8, 8, 8], w_idx = [0], w_slice = [80], id = 9, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x32x56x80x!quant.uniform<i8:f32, 0.96238692440944873>>, none) -> tensor<1x32x56x80x!quant.uniform<i8:f32, 0.96238692440944873>, 0 : i64> loc(#loc7)
        "tpu.Yield"(%275) : (tensor<1x32x56x80x!quant.uniform<i8:f32, 0.96238692440944873>, 0 : i64>) -> () loc(#loc7)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 4, 3, 9, -2, 7, 5, 6, -3, 8, 0, 1, 2], group_type = 0 : i64, hsecs = 7 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [-2, 1], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x3x224x320x!quant.uniform<i8:f32, 0.0078740157480314959>, 3298534883328 : i64>) -> tensor<1x32x56x80x!quant.uniform<i8:f32, 0.96238692440944873>, 0 : i64> loc(#loc7)
      %7 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099514347904 : i64> loc(#loc16)
      %8 = "top.Weight"() : () -> tensor<1x32x1x9xi8, 1099513031408 : i64> loc(#loc17)
      %9 = "top.Weight"() : () -> tensor<1x32x1x32xsi8, 1099512708464 : i64> loc(#loc18)
      %10 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099512886464 : i64> loc(#loc19)
      %11 = "tpu.Group"(%6) ({
        %266 = "tpu.Load"(%6) {do_bcast = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 6080, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0, 19, 38], h_slice = [19, 19, 18], w_idx = [0], w_slice = [80], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x32x56x80x!quant.uniform<i8:f32, 0.96238692440944873>, 0 : i64>) -> tensor<1x32x56x80x!quant.uniform<i8:f32, 0.96238692440944873>> loc(#loc21)
        %267 = "tpu.Load"(%7) {do_bcast = true, ginfo = #tpu.lg<out_addr = 6336, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 1, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099514347904 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc22)
        %268 = "tpu.Load"(%9) {do_bcast = false, ginfo = #tpu.lg<out_addr = 30656, out_size = 128, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [32], id = 2, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x32x1x32xsi8, 1099512708464 : i64>) -> tensor<1x32x1x32xsi8> loc(#loc23)
        %269 = "tpu.Load"(%8) {do_bcast = false, ginfo = #tpu.lg<out_addr = 6592, out_size = 36, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 3, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x32x1x9xi8, 1099513031408 : i64>) -> tensor<1x32x1x9xi8> loc(#loc24)
        %270 = "tpu.Lut"(%266, %267) {ginfo = #tpu.lg<out_addr = 0, out_size = 6080, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0, 19, 38], h_slice = [19, 19, 18], w_idx = [0], w_slice = [80], id = 4, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x32x56x80x!quant.uniform<i8:f32, 0.96238692440944873>>, tensor<1x1x1x256xsi8>) -> tensor<1x32x56x80x!quant.uniform<i8:f32, 0.87199718267716531>> loc(#loc25)
        %271 = "tpu.Load"(%10) {do_bcast = true, ginfo = #tpu.lg<out_addr = 6080, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 5, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099512886464 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc26)
        %272 = "tpu.Conv2D"(%270, %268, %269) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 8192, out_size = 6080, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0, 19, 38], h_slice = [19, 19, 18], w_idx = [0], w_slice = [80], id = 6, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x32x56x80x!quant.uniform<i8:f32, 0.87199718267716531>>, tensor<1x32x1x32xsi8>, tensor<1x32x1x9xi8>) -> tensor<1x32x56x80x!quant.uniform<i8:f32, 0.43888809921259841>> loc(#loc27)
        %273 = "tpu.Lut"(%272, %271) {ginfo = #tpu.lg<out_addr = 24576, out_size = 6080, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0, 19, 38], h_slice = [19, 19, 18], w_idx = [0], w_slice = [80], id = 7, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x32x56x80x!quant.uniform<i8:f32, 0.43888809921259841>>, tensor<1x1x1x256xsi8>) -> tensor<1x32x56x80x!quant.uniform<i8:f32, 0.25703029606299216>> loc(#loc20)
        %274 = "tpu.Store"(%273, %0) {ginfo = #tpu.lg<out_addr = 24576, out_size = 6080, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0, 19, 38], h_slice = [19, 19, 18], w_idx = [0], w_slice = [80], id = 8, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x32x56x80x!quant.uniform<i8:f32, 0.25703029606299216>>, none) -> tensor<1x32x56x80x!quant.uniform<i8:f32, 0.25703029606299216>, 143360 : i64> loc(#loc20)
        "tpu.Yield"(%274) : (tensor<1x32x56x80x!quant.uniform<i8:f32, 0.25703029606299216>, 143360 : i64>) -> () loc(#loc20)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 4, 2, 3, 8, -2, 6, 5, -3, 7, 0, 1], group_type = 0 : i64, hsecs = 3 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [1], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x32x56x80x!quant.uniform<i8:f32, 0.96238692440944873>, 0 : i64>) -> tensor<1x32x56x80x!quant.uniform<i8:f32, 0.25703029606299216>, 143360 : i64> loc(#loc20)
      %12 = "tpu.Slice"(%11, %0, %0, %0, %0) {axes = [], ends = [9223372036854775807, 16, 9223372036854775807, 9223372036854775807], hasparamConvert_axes = [1], offset = [0, 0, 0, 0], steps = [1, 1, 1, 1]} : (tensor<1x32x56x80x!quant.uniform<i8:f32, 0.25703029606299216>, 143360 : i64>, none, none, none, none) -> tensor<1x16x56x80x!quant.uniform<i8:f32, 0.25703029606299216>, 143360 : i64> loc(#loc28)
      %13 = "tpu.Slice"(%11, %0, %0, %0, %0) {axes = [], ends = [9223372036854775807, 32, 9223372036854775807, 9223372036854775807], hasparamConvert_axes = [1], offset = [0, 16, 0, 0], steps = [1, 1, 1, 1]} : (tensor<1x32x56x80x!quant.uniform<i8:f32, 0.25703029606299216>, 143360 : i64>, none, none, none, none) -> tensor<1x16x56x80x!quant.uniform<i8:f32, 0.25703029606299216>, 215040 : i64> loc(#loc29)
      %14 = "top.Weight"() : () -> tensor<1x16x1x9xi8, 1099512831472 : i64> loc(#loc30)
      %15 = "top.Weight"() : () -> tensor<1x16x9x16xsi8, 1099512703664 : i64> loc(#loc31)
      %16 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099512887008 : i64> loc(#loc32)
      %17 = "top.Weight"() : () -> tensor<1x16x1x9xi8, 1099513024352 : i64> loc(#loc33)
      %18 = "top.Weight"() : () -> tensor<1x16x9x16xsi8, 1099513024496 : i64> loc(#loc34)
      %19 = "tpu.Group"(%13) ({
        %266 = "tpu.Load"(%13) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 4800, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [16], d_idx = [0], d_slice = [1], h_idx = [0, 26], h_slice = [30, 30], w_idx = [0], w_slice = [80], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x16x56x80x!quant.uniform<i8:f32, 0.25703029606299216>, 215040 : i64>) -> tensor<1x16x56x80x!quant.uniform<i8:f32, 0.25703029606299216>> loc(#loc36)
        %267 = "tpu.Load"(%15) {do_bcast = false, ginfo = #tpu.lg<out_addr = 5088, out_size = 288, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [16], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [16], id = 1, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x16x9x16xsi8, 1099512703664 : i64>) -> tensor<1x16x9x16xsi8> loc(#loc37)
        %268 = "tpu.Load"(%14) {do_bcast = false, ginfo = #tpu.lg<out_addr = 5408, out_size = 18, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [16], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 2, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x16x1x9xi8, 1099512831472 : i64>) -> tensor<1x16x1x9xi8> loc(#loc38)
        %269 = "tpu.Load"(%16) {do_bcast = true, ginfo = #tpu.lg<out_addr = 29056, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 3, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099512887008 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc39)
        %270 = "tpu.Conv2D"(%266, %267, %268) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 8192, out_size = 4640, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [16], d_idx = [0], d_slice = [1], h_idx = [0, 27], h_slice = [29, 29], w_idx = [0], w_slice = [80], id = 4, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x16x56x80x!quant.uniform<i8:f32, 0.25703029606299216>>, tensor<1x16x9x16xsi8>, tensor<1x16x1x9xi8>) -> tensor<1x16x56x80x!quant.uniform<i8:f32, 0.25260705354330709>> loc(#loc40)
        %271 = "tpu.Load"(%18) {do_bcast = false, ginfo = #tpu.lg<out_addr = 4800, out_size = 288, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [16], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [16], id = 5, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x16x9x16xsi8, 1099513024496 : i64>) -> tensor<1x16x9x16xsi8> loc(#loc41)
        %272 = "tpu.Load"(%17) {do_bcast = false, ginfo = #tpu.lg<out_addr = 5376, out_size = 18, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [16], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 6, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x16x1x9xi8, 1099513024352 : i64>) -> tensor<1x16x1x9xi8> loc(#loc42)
        %273 = "tpu.Lut"(%270, %269) {ginfo = #tpu.lg<out_addr = 16384, out_size = 4640, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [16], d_idx = [0], d_slice = [1], h_idx = [0, 27], h_slice = [29, 29], w_idx = [0], w_slice = [80], id = 7, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x16x56x80x!quant.uniform<i8:f32, 0.25260705354330709>>, tensor<1x1x1x256xsi8>) -> tensor<1x16x56x80x!quant.uniform<i8:f32, 0.21204552283464567>> loc(#loc43)
        %274 = "tpu.Conv2D"(%273, %271, %272) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 4480, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [16], d_idx = [0], d_slice = [1], h_idx = [0, 28], h_slice = [28, 28], w_idx = [0], w_slice = [80], id = 8, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x16x56x80x!quant.uniform<i8:f32, 0.21204552283464567>>, tensor<1x16x9x16xsi8>, tensor<1x16x1x9xi8>) -> tensor<1x16x56x80x!quant.uniform<i8:f32, 0.17999419842519684>> loc(#loc35)
        %275 = "tpu.Store"(%274, %0) {ginfo = #tpu.lg<out_addr = 24576, out_size = 4480, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [16], d_idx = [0], d_slice = [1], h_idx = [0, 28], h_slice = [28, 28], w_idx = [0], w_slice = [80], id = 9, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x16x56x80x!quant.uniform<i8:f32, 0.17999419842519684>>, none) -> tensor<1x16x56x80x!quant.uniform<i8:f32, 0.17999419842519684>, 286720 : i64> loc(#loc35)
        "tpu.Yield"(%275) : (tensor<1x16x56x80x!quant.uniform<i8:f32, 0.17999419842519684>, 286720 : i64>) -> () loc(#loc35)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 4, 3, 9, -2, 7, 5, 6, -3, 8, 0, 1, 2], group_type = 0 : i64, hsecs = 2 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x16x56x80x!quant.uniform<i8:f32, 0.25703029606299216>, 215040 : i64>) -> tensor<1x16x56x80x!quant.uniform<i8:f32, 0.17999419842519684>, 286720 : i64> loc(#loc35)
      %20 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099513026800 : i64> loc(#loc44)
      %21 = "top.Weight"() : () -> tensor<1x32x1x9xi8, 1099512886720 : i64> loc(#loc45)
      %22 = "top.Weight"() : () -> tensor<1x32x1x48xsi8, 1099514606912 : i64> loc(#loc46)
      %23 = "tpu.Group"(%19, %13, %12) ({
        %266 = "tpu.Load"(%19) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 3040, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [16], d_idx = [0], d_slice = [1], h_idx = [0, 19, 38], h_slice = [19, 19, 18], w_idx = [0], w_slice = [80], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x16x56x80x!quant.uniform<i8:f32, 0.17999419842519684>, 286720 : i64>) -> tensor<1x16x56x80x!quant.uniform<i8:f32, 0.17999419842519684>> loc(#loc48)
        %267 = "tpu.Load"(%20) {do_bcast = true, ginfo = #tpu.lg<out_addr = 28672, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 1, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099513026800 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc49)
        %268 = "tpu.Load"(%13) {do_bcast = false, ginfo = #tpu.lg<out_addr = 20480, out_size = 3040, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [16], d_idx = [0], d_slice = [1], h_idx = [0, 19, 38], h_slice = [19, 19, 18], w_idx = [0], w_slice = [80], id = 2, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x16x56x80x!quant.uniform<i8:f32, 0.25703029606299216>, 215040 : i64>) -> tensor<1x16x56x80x!quant.uniform<i8:f32, 0.25703029606299216>> loc(#loc36)
        %269 = "tpu.Lut"(%266, %267) {ginfo = #tpu.lg<out_addr = 0, out_size = 3040, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [16], d_idx = [0], d_slice = [1], h_idx = [0, 19, 38], h_slice = [19, 19, 18], w_idx = [0], w_slice = [80], id = 3, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x16x56x80x!quant.uniform<i8:f32, 0.17999419842519684>>, tensor<1x1x1x256xsi8>) -> tensor<1x16x56x80x!quant.uniform<i8:f32, 0.19860304173228346>> loc(#loc50)
        %270 = "tpu.Load"(%12) {do_bcast = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 3040, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [16], d_idx = [0], d_slice = [1], h_idx = [0, 19, 38], h_slice = [19, 19, 18], w_idx = [0], w_slice = [80], id = 4, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x16x56x80x!quant.uniform<i8:f32, 0.25703029606299216>, 143360 : i64>) -> tensor<1x16x56x80x!quant.uniform<i8:f32, 0.25703029606299216>> loc(#loc51)
        %271 = "tpu.Add"(%268, %269) {do_relu = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 3040, buffer_addr = 4096, buffer_size = 3040, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [16], d_idx = [0], d_slice = [1], h_idx = [0, 19, 38], h_slice = [19, 19, 18], w_idx = [0], w_slice = [80], id = 5, stage = 1, slice_idx = 0, group_type = 0>, multipliers = [70, 54], relu_limit = -1.000000e+00 : f64, rshifts = [6]} : (tensor<1x16x56x80x!quant.uniform<i8:f32, 0.25703029606299216>>, tensor<1x16x56x80x!quant.uniform<i8:f32, 0.19860304173228346>>) -> tensor<1x16x56x80x!quant.uniform<i8:f32, 0.23443995984251967>> loc(#loc52)
        %272 = "tpu.Load"(%22) {do_bcast = false, ginfo = #tpu.lg<out_addr = 9120, out_size = 192, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [48], id = 6, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x32x1x48xsi8, 1099514606912 : i64>) -> tensor<1x32x1x48xsi8> loc(#loc53)
        %273 = "tpu.Load"(%21) {do_bcast = false, ginfo = #tpu.lg<out_addr = 9312, out_size = 36, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 7, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x32x1x9xi8, 1099512886720 : i64>) -> tensor<1x32x1x9xi8> loc(#loc54)
        %274 = "tpu.Concat"(%270, %268, %271) {axis = 1 : si32, do_relu = false, ginfo = #tpu.lg<out_addr = 0, out_size = 9120, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [48], d_idx = [0], d_slice = [1], h_idx = [0, 19, 38], h_slice = [19, 19, 18], w_idx = [0], w_slice = [80], id = 8, stage = 1, slice_idx = 0, group_type = 0>, multipliers = [72, 72, 65], only_merge = false, relu_limit = -1.000000e+00 : f64, rshifts = [6, 6, 6]} : (tensor<1x16x56x80x!quant.uniform<i8:f32, 0.25703029606299216>>, tensor<1x16x56x80x!quant.uniform<i8:f32, 0.25703029606299216>>, tensor<1x16x56x80x!quant.uniform<i8:f32, 0.23443995984251967>>) -> tensor<1x48x56x80x!quant.uniform<i8:f32, 0.22740728267716537>> loc(#loc55)
        %275 = "tpu.Conv2D"(%274, %272, %273) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 6080, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0, 19, 38], h_slice = [19, 19, 18], w_idx = [0], w_slice = [80], id = 9, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x48x56x80x!quant.uniform<i8:f32, 0.22740728267716537>>, tensor<1x32x1x48xsi8>, tensor<1x32x1x9xi8>) -> tensor<1x32x56x80x!quant.uniform<i8:f32, 0.14162470629921259>> loc(#loc47)
        %276 = "tpu.Store"(%275, %0) {ginfo = #tpu.lg<out_addr = 12288, out_size = 6080, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0, 19, 38], h_slice = [19, 19, 18], w_idx = [0], w_slice = [80], id = 10, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x32x56x80x!quant.uniform<i8:f32, 0.14162470629921259>>, none) -> tensor<1x32x56x80x!quant.uniform<i8:f32, 0.14162470629921259>, 0 : i64> loc(#loc47)
        "tpu.Yield"(%276) : (tensor<1x32x56x80x!quant.uniform<i8:f32, 0.14162470629921259>, 0 : i64>) -> () loc(#loc47)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 3, 2, 10, -2, 5, 4, -3, 8, 6, 7, -4, 9, 0, 1], group_type = 0 : i64, hsecs = 3 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [-1, 1], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x16x56x80x!quant.uniform<i8:f32, 0.17999419842519684>, 286720 : i64>, tensor<1x16x56x80x!quant.uniform<i8:f32, 0.25703029606299216>, 215040 : i64>, tensor<1x16x56x80x!quant.uniform<i8:f32, 0.25703029606299216>, 143360 : i64>) -> tensor<1x32x56x80x!quant.uniform<i8:f32, 0.14162470629921259>, 0 : i64> loc(#loc47)
      %24 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099513105824 : i64> loc(#loc56)
      %25 = "top.Weight"() : () -> tensor<1x64x1x9xi8, 1099513107232 : i64> loc(#loc57)
      %26 = "top.Weight"() : () -> tensor<1x64x9x32xsi8, 1099513223344 : i64> loc(#loc58)
      %27 = "tpu.Group"(%23) ({
        %266 = "tpu.Load"(%23) {do_bcast = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 9280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0, 27], h_slice = [28, 29], w_idx = [0], w_slice = [80], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x32x56x80x!quant.uniform<i8:f32, 0.14162470629921259>, 0 : i64>) -> tensor<1x32x56x80x!quant.uniform<i8:f32, 0.14162470629921259>> loc(#loc60)
        %267 = "tpu.Load"(%24) {do_bcast = true, ginfo = #tpu.lg<out_addr = 11584, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 1, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099513105824 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc61)
        %268 = "tpu.Load"(%26) {do_bcast = false, ginfo = #tpu.lg<out_addr = 9280, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [32], id = 2, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x9x32xsi8, 1099513223344 : i64>) -> tensor<1x64x9x32xsi8> loc(#loc62)
        %269 = "tpu.Load"(%25) {do_bcast = false, ginfo = #tpu.lg<out_addr = 11840, out_size = 72, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 3, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x9xi8, 1099513107232 : i64>) -> tensor<1x64x1x9xi8> loc(#loc63)
        %270 = "tpu.Lut"(%266, %267) {ginfo = #tpu.lg<out_addr = 0, out_size = 9280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0, 27], h_slice = [28, 29], w_idx = [0], w_slice = [80], id = 4, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x32x56x80x!quant.uniform<i8:f32, 0.14162470629921259>>, tensor<1x1x1x256xsi8>) -> tensor<1x32x56x80x!quant.uniform<i8:f32, 0.085063231496062999>> loc(#loc64)
        %271 = "tpu.Conv2D"(%270, %268, %269) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 4480, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 14], h_slice = [14, 14], w_idx = [0], w_slice = [40], id = 5, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [2, 2], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x32x56x80x!quant.uniform<i8:f32, 0.085063231496062999>>, tensor<1x64x9x32xsi8>, tensor<1x64x1x9xi8>) -> tensor<1x64x28x40x!quant.uniform<i8:f32, 0.048336753543307083>> loc(#loc59)
        %272 = "tpu.Store"(%271, %0) {ginfo = #tpu.lg<out_addr = 24576, out_size = 4480, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 14], h_slice = [14, 14], w_idx = [0], w_slice = [40], id = 6, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.048336753543307083>>, none) -> tensor<1x64x28x40x!quant.uniform<i8:f32, 0.048336753543307083>, 143360 : i64> loc(#loc59)
        "tpu.Yield"(%272) : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.048336753543307083>, 143360 : i64>) -> () loc(#loc59)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 4, 2, 3, 6, -2, 5, 0, 1], group_type = 0 : i64, hsecs = 2 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [1], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x32x56x80x!quant.uniform<i8:f32, 0.14162470629921259>, 0 : i64>) -> tensor<1x64x28x40x!quant.uniform<i8:f32, 0.048336753543307083>, 143360 : i64> loc(#loc59)
      %28 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099512848256 : i64> loc(#loc65)
      %29 = "top.Weight"() : () -> tensor<1x64x1x9xi8, 1099513291952 : i64> loc(#loc66)
      %30 = "top.Weight"() : () -> tensor<1x64x1x64xsi8, 1099513027312 : i64> loc(#loc67)
      %31 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099513302032 : i64> loc(#loc68)
      %32 = "tpu.Group"(%27) ({
        %266 = "tpu.Load"(%27) {do_bcast = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 4480, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 14], h_slice = [14, 14], w_idx = [0], w_slice = [40], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.048336753543307083>, 143360 : i64>) -> tensor<1x64x28x40x!quant.uniform<i8:f32, 0.048336753543307083>> loc(#loc70)
        %267 = "tpu.Load"(%28) {do_bcast = true, ginfo = #tpu.lg<out_addr = 4736, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 1, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099512848256 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc71)
        %268 = "tpu.Load"(%30) {do_bcast = false, ginfo = #tpu.lg<out_addr = 29056, out_size = 512, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [64], id = 2, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x64xsi8, 1099513027312 : i64>) -> tensor<1x64x1x64xsi8> loc(#loc72)
        %269 = "tpu.Load"(%29) {do_bcast = false, ginfo = #tpu.lg<out_addr = 4992, out_size = 72, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 3, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x9xi8, 1099513291952 : i64>) -> tensor<1x64x1x9xi8> loc(#loc73)
        %270 = "tpu.Lut"(%266, %267) {ginfo = #tpu.lg<out_addr = 0, out_size = 4480, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 14], h_slice = [14, 14], w_idx = [0], w_slice = [40], id = 4, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.048336753543307083>>, tensor<1x1x1x256xsi8>) -> tensor<1x64x28x40x!quant.uniform<i8:f32, 0.056522050393700791>> loc(#loc74)
        %271 = "tpu.Load"(%31) {do_bcast = true, ginfo = #tpu.lg<out_addr = 4480, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 5, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099513302032 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc75)
        %272 = "tpu.Conv2D"(%270, %268, %269) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 8192, out_size = 4480, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 14], h_slice = [14, 14], w_idx = [0], w_slice = [40], id = 6, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.056522050393700791>>, tensor<1x64x1x64xsi8>, tensor<1x64x1x9xi8>) -> tensor<1x64x28x40x!quant.uniform<i8:f32, 0.059017308661417321>> loc(#loc76)
        %273 = "tpu.Lut"(%272, %271) {ginfo = #tpu.lg<out_addr = 24576, out_size = 4480, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 14], h_slice = [14, 14], w_idx = [0], w_slice = [40], id = 7, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.059017308661417321>>, tensor<1x1x1x256xsi8>) -> tensor<1x64x28x40x!quant.uniform<i8:f32, 0.053309097637795275>> loc(#loc69)
        %274 = "tpu.Store"(%273, %0) {ginfo = #tpu.lg<out_addr = 24576, out_size = 4480, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 14], h_slice = [14, 14], w_idx = [0], w_slice = [40], id = 8, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.053309097637795275>>, none) -> tensor<1x64x28x40x!quant.uniform<i8:f32, 0.053309097637795275>, 71680 : i64> loc(#loc69)
        "tpu.Yield"(%274) : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.053309097637795275>, 71680 : i64>) -> () loc(#loc69)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 4, 2, 3, 8, -2, 6, 5, -3, 7, 0, 1], group_type = 0 : i64, hsecs = 2 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.048336753543307083>, 143360 : i64>) -> tensor<1x64x28x40x!quant.uniform<i8:f32, 0.053309097637795275>, 71680 : i64> loc(#loc69)
      %33 = "tpu.Slice"(%32, %0, %0, %0, %0) {axes = [], ends = [9223372036854775807, 32, 9223372036854775807, 9223372036854775807], hasparamConvert_axes = [1], offset = [0, 0, 0, 0], steps = [1, 1, 1, 1]} : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.053309097637795275>, 71680 : i64>, none, none, none, none) -> tensor<1x32x28x40x!quant.uniform<i8:f32, 0.053309097637795275>, 71680 : i64> loc(#loc77)
      %34 = "tpu.Slice"(%32, %0, %0, %0, %0) {axes = [], ends = [9223372036854775807, 64, 9223372036854775807, 9223372036854775807], hasparamConvert_axes = [1], offset = [0, 32, 0, 0], steps = [1, 1, 1, 1]} : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.053309097637795275>, 71680 : i64>, none, none, none, none) -> tensor<1x32x28x40x!quant.uniform<i8:f32, 0.053309097637795275>, 107520 : i64> loc(#loc78)
      %35 = "top.Weight"() : () -> tensor<1x32x1x9xi8, 1099514022800 : i64> loc(#loc79)
      %36 = "top.Weight"() : () -> tensor<1x32x9x32xsi8, 1099513302544 : i64> loc(#loc80)
      %37 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099513311760 : i64> loc(#loc81)
      %38 = "top.Weight"() : () -> tensor<1x32x1x9xi8, 1099513301744 : i64> loc(#loc82)
      %39 = "top.Weight"() : () -> tensor<1x32x9x32xsi8, 1099513282736 : i64> loc(#loc83)
      %40 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099512848000 : i64> loc(#loc84)
      %41 = "top.Weight"() : () -> tensor<1x32x1x9xi8, 1099512707120 : i64> loc(#loc85)
      %42 = "top.Weight"() : () -> tensor<1x32x9x32xsi8, 1099512820336 : i64> loc(#loc86)
      %43 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099513023520 : i64> loc(#loc87)
      %44 = "top.Weight"() : () -> tensor<1x32x1x9xi8, 1099512707664 : i64> loc(#loc88)
      %45 = "top.Weight"() : () -> tensor<1x32x9x32xsi8, 1099513056416 : i64> loc(#loc89)
      %46:2 = "tpu.Group"(%34) ({
        %266 = "tpu.Load"(%34) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 4480, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [28], w_idx = [0], w_slice = [40], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x32x28x40x!quant.uniform<i8:f32, 0.053309097637795275>, 107520 : i64>) -> tensor<1x32x28x40x!quant.uniform<i8:f32, 0.053309097637795275>> loc(#loc92)
        %267 = "tpu.Load"(%36) {do_bcast = false, ginfo = #tpu.lg<out_addr = 28672, out_size = 1152, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [32], id = 1, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x32x9x32xsi8, 1099513302544 : i64>) -> tensor<1x32x9x32xsi8> loc(#loc93)
        %268 = "tpu.Load"(%35) {do_bcast = false, ginfo = #tpu.lg<out_addr = 4528, out_size = 36, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 2, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x32x1x9xi8, 1099514022800 : i64>) -> tensor<1x32x1x9xi8> loc(#loc94)
        %269 = "tpu.Load"(%37) {do_bcast = true, ginfo = #tpu.lg<out_addr = 8960, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 3, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099513311760 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc95)
        %270 = "tpu.Conv2D"(%266, %267, %268) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 4480, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [28], w_idx = [0], w_slice = [40], id = 4, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x32x28x40x!quant.uniform<i8:f32, 0.053309097637795275>>, tensor<1x32x9x32xsi8>, tensor<1x32x1x9xi8>) -> tensor<1x32x28x40x!quant.uniform<i8:f32, 0.050650926771653539>> loc(#loc96)
        %271 = "tpu.Load"(%39) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 1152, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [32], id = 5, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x32x9x32xsi8, 1099513282736 : i64>) -> tensor<1x32x9x32xsi8> loc(#loc97)
        %272 = "tpu.Load"(%38) {do_bcast = false, ginfo = #tpu.lg<out_addr = 9216, out_size = 36, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 6, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x32x1x9xi8, 1099513301744 : i64>) -> tensor<1x32x1x9xi8> loc(#loc98)
        %273 = "tpu.Lut"(%270, %269) {ginfo = #tpu.lg<out_addr = 4480, out_size = 4480, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [28], w_idx = [0], w_slice = [40], id = 7, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x32x28x40x!quant.uniform<i8:f32, 0.050650926771653539>>, tensor<1x1x1x256xsi8>) -> tensor<1x32x28x40x!quant.uniform<i8:f32, 0.038292336220472435>> loc(#loc99)
        %274 = "tpu.Load"(%40) {do_bcast = true, ginfo = #tpu.lg<out_addr = 28672, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 8, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099512848000 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc100)
        %275 = "tpu.Conv2D"(%273, %271, %272) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 4480, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [28], w_idx = [0], w_slice = [40], id = 9, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x32x28x40x!quant.uniform<i8:f32, 0.038292336220472435>>, tensor<1x32x9x32xsi8>, tensor<1x32x1x9xi8>) -> tensor<1x32x28x40x!quant.uniform<i8:f32, 0.056342346456692907>> loc(#loc101)
        %276 = "tpu.Lut"(%275, %274) {ginfo = #tpu.lg<out_addr = 8192, out_size = 4480, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [28], w_idx = [0], w_slice = [40], id = 10, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x32x28x40x!quant.uniform<i8:f32, 0.056342346456692907>>, tensor<1x1x1x256xsi8>) -> tensor<1x32x28x40x!quant.uniform<i8:f32, 0.04134017086614173>> loc(#loc102)
        %277 = "tpu.Load"(%42) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 1152, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [32], id = 11, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x32x9x32xsi8, 1099512820336 : i64>) -> tensor<1x32x9x32xsi8> loc(#loc103)
        %278 = "tpu.Load"(%41) {do_bcast = false, ginfo = #tpu.lg<out_addr = 4480, out_size = 36, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 12, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x32x1x9xi8, 1099512707120 : i64>) -> tensor<1x32x1x9xi8> loc(#loc104)
        %279 = "tpu.Add"(%266, %276) {do_relu = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 4480, buffer_addr = 25728, buffer_size = 4480, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [28], w_idx = [0], w_slice = [40], id = 13, stage = 1, slice_idx = 0, group_type = 0>, multipliers = [63, 49], relu_limit = -1.000000e+00 : f64, rshifts = [6]} : (tensor<1x32x28x40x!quant.uniform<i8:f32, 0.053309097637795275>>, tensor<1x32x28x40x!quant.uniform<i8:f32, 0.04134017086614173>>) -> tensor<1x32x28x40x!quant.uniform<i8:f32, 0.053568592125984249>> loc(#loc90)
        %280 = "tpu.Load"(%43) {do_bcast = true, ginfo = #tpu.lg<out_addr = 28672, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 14, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099513023520 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc105)
        %281 = "tpu.Store"(%279, %0) {ginfo = #tpu.lg<out_addr = 16384, out_size = 4480, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [28], w_idx = [0], w_slice = [40], id = 15, stage = 1, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x32x28x40x!quant.uniform<i8:f32, 0.053568592125984249>>, none) -> tensor<1x32x28x40x!quant.uniform<i8:f32, 0.053568592125984249>, 143360 : i64> loc(#loc90)
        %282 = "tpu.Conv2D"(%279, %277, %278) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 0, out_size = 4480, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [28], w_idx = [0], w_slice = [40], id = 16, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x32x28x40x!quant.uniform<i8:f32, 0.053568592125984249>>, tensor<1x32x9x32xsi8>, tensor<1x32x1x9xi8>) -> tensor<1x32x28x40x!quant.uniform<i8:f32, 0.049326625984251964>> loc(#loc106)
        %283 = "tpu.Load"(%45) {do_bcast = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 1152, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [32], id = 17, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x32x9x32xsi8, 1099513056416 : i64>) -> tensor<1x32x9x32xsi8> loc(#loc107)
        %284 = "tpu.Load"(%44) {do_bcast = false, ginfo = #tpu.lg<out_addr = 4480, out_size = 36, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 18, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x32x1x9xi8, 1099512707664 : i64>) -> tensor<1x32x1x9xi8> loc(#loc108)
        %285 = "tpu.Lut"(%282, %280) {ginfo = #tpu.lg<out_addr = 8192, out_size = 4480, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [28], w_idx = [0], w_slice = [40], id = 19, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x32x28x40x!quant.uniform<i8:f32, 0.049326625984251964>>, tensor<1x1x1x256xsi8>) -> tensor<1x32x28x40x!quant.uniform<i8:f32, 0.027419133070866141>> loc(#loc109)
        %286 = "tpu.Conv2D"(%285, %283, %284) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 20480, out_size = 4480, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [28], w_idx = [0], w_slice = [40], id = 20, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x32x28x40x!quant.uniform<i8:f32, 0.027419133070866141>>, tensor<1x32x9x32xsi8>, tensor<1x32x1x9xi8>) -> tensor<1x32x28x40x!quant.uniform<i8:f32, 0.079176850393700787>> loc(#loc91)
        %287 = "tpu.Store"(%286, %0) {ginfo = #tpu.lg<out_addr = 20480, out_size = 4480, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [28], w_idx = [0], w_slice = [40], id = 21, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x32x28x40x!quant.uniform<i8:f32, 0.079176850393700787>>, none) -> tensor<1x32x28x40x!quant.uniform<i8:f32, 0.079176850393700787>, 179200 : i64> loc(#loc91)
        "tpu.Yield"(%281, %287) : (tensor<1x32x28x40x!quant.uniform<i8:f32, 0.053568592125984249>, 143360 : i64>, tensor<1x32x28x40x!quant.uniform<i8:f32, 0.079176850393700787>, 179200 : i64>) -> () loc(#loc608)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 4, 3, 21, -2, 7, 5, 6, -3, 9, 8, -4, 10, -5, 13, 11, 12, -6, 16, 14, 15, -7, 19, 17, 18, -8, 20, 0, 1, 2], group_type = 0 : i64, hsecs = 1 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x32x28x40x!quant.uniform<i8:f32, 0.053309097637795275>, 107520 : i64>) -> (tensor<1x32x28x40x!quant.uniform<i8:f32, 0.053568592125984249>, 143360 : i64>, tensor<1x32x28x40x!quant.uniform<i8:f32, 0.079176850393700787>, 179200 : i64>) loc(#loc608)
      %47 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099512707408 : i64> loc(#loc110)
      %48 = "top.Weight"() : () -> tensor<1x64x1x9xi8, 1099512848512 : i64> loc(#loc111)
      %49 = "top.Weight"() : () -> tensor<1x64x1x128xsi8, 1099513274544 : i64> loc(#loc112)
      %50 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099514311936 : i64> loc(#loc113)
      %51 = "tpu.Group"(%46#1, %46#0, %33, %34) ({
        %266 = "tpu.Load"(%46#1) {do_bcast = false, ginfo = #tpu.lg<out_addr = 21504, out_size = 2240, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0, 14], h_slice = [14, 14], w_idx = [0], w_slice = [40], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x32x28x40x!quant.uniform<i8:f32, 0.079176850393700787>, 179200 : i64>) -> tensor<1x32x28x40x!quant.uniform<i8:f32, 0.079176850393700787>> loc(#loc115)
        %267 = "tpu.Load"(%47) {do_bcast = true, ginfo = #tpu.lg<out_addr = 30912, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 1, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099512707408 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc116)
        %268 = "tpu.Load"(%46#0) {do_bcast = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 2240, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0, 14], h_slice = [14, 14], w_idx = [0], w_slice = [40], id = 2, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x32x28x40x!quant.uniform<i8:f32, 0.053568592125984249>, 143360 : i64>) -> tensor<1x32x28x40x!quant.uniform<i8:f32, 0.053568592125984249>> loc(#loc117)
        %269 = "tpu.Lut"(%266, %267) {ginfo = #tpu.lg<out_addr = 8192, out_size = 2240, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0, 14], h_slice = [14, 14], w_idx = [0], w_slice = [40], id = 3, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x32x28x40x!quant.uniform<i8:f32, 0.079176850393700787>>, tensor<1x1x1x256xsi8>) -> tensor<1x32x28x40x!quant.uniform<i8:f32, 0.096142340944881887>> loc(#loc118)
        %270 = "tpu.Load"(%33) {do_bcast = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 2240, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0, 14], h_slice = [14, 14], w_idx = [0], w_slice = [40], id = 4, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x32x28x40x!quant.uniform<i8:f32, 0.053309097637795275>, 71680 : i64>) -> tensor<1x32x28x40x!quant.uniform<i8:f32, 0.053309097637795275>> loc(#loc119)
        %271 = "tpu.Load"(%34) {do_bcast = false, ginfo = #tpu.lg<out_addr = 28672, out_size = 2240, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0, 14], h_slice = [14, 14], w_idx = [0], w_slice = [40], id = 5, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x32x28x40x!quant.uniform<i8:f32, 0.053309097637795275>, 107520 : i64>) -> tensor<1x32x28x40x!quant.uniform<i8:f32, 0.053309097637795275>> loc(#loc92)
        %272 = "tpu.Add"(%268, %269) {do_relu = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 2240, buffer_addr = 0, buffer_size = 2240, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0, 14], h_slice = [14, 14], w_idx = [0], w_slice = [40], id = 6, stage = 1, slice_idx = 0, group_type = 0>, multipliers = [47, 84], relu_limit = -1.000000e+00 : f64, rshifts = [6]} : (tensor<1x32x28x40x!quant.uniform<i8:f32, 0.053568592125984249>>, tensor<1x32x28x40x!quant.uniform<i8:f32, 0.096142340944881887>>) -> tensor<1x32x28x40x!quant.uniform<i8:f32, 0.072487838582677169>> loc(#loc120)
        %273 = "tpu.Load"(%49) {do_bcast = false, ginfo = #tpu.lg<out_addr = 20480, out_size = 1024, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [128], id = 7, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x128xsi8, 1099513274544 : i64>) -> tensor<1x64x1x128xsi8> loc(#loc121)
        %274 = "tpu.Load"(%48) {do_bcast = false, ginfo = #tpu.lg<out_addr = 10432, out_size = 72, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 8, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x9xi8, 1099512848512 : i64>) -> tensor<1x64x1x9xi8> loc(#loc122)
        %275 = "tpu.Concat"(%270, %271, %268, %272) {axis = 1 : si32, do_relu = false, ginfo = #tpu.lg<out_addr = 0, out_size = 8960, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 14], h_slice = [14, 14], w_idx = [0], w_slice = [40], id = 9, stage = 1, slice_idx = 0, group_type = 0>, multipliers = [94, 94, 94, 1], only_merge = false, relu_limit = -1.000000e+00 : f64, rshifts = [7, 7, 7, 0]} : (tensor<1x32x28x40x!quant.uniform<i8:f32, 0.053309097637795275>>, tensor<1x32x28x40x!quant.uniform<i8:f32, 0.053309097637795275>>, tensor<1x32x28x40x!quant.uniform<i8:f32, 0.053568592125984249>>, tensor<1x32x28x40x!quant.uniform<i8:f32, 0.072487838582677169>>) -> tensor<1x128x28x40x!quant.uniform<i8:f32, 0.072487838582677169>> loc(#loc123)
        %276 = "tpu.Load"(%50) {do_bcast = true, ginfo = #tpu.lg<out_addr = 26816, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 10, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099514311936 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc124)
        %277 = "tpu.Conv2D"(%275, %273, %274) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 4480, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 14], h_slice = [14, 14], w_idx = [0], w_slice = [40], id = 11, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x28x40x!quant.uniform<i8:f32, 0.072487838582677169>>, tensor<1x64x1x128xsi8>, tensor<1x64x1x9xi8>) -> tensor<1x64x28x40x!quant.uniform<i8:f32, 0.060370201574803148>> loc(#loc125)
        %278 = "tpu.Lut"(%277, %276) {ginfo = #tpu.lg<out_addr = 0, out_size = 4480, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 14], h_slice = [14, 14], w_idx = [0], w_slice = [40], id = 12, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.060370201574803148>>, tensor<1x1x1x256xsi8>) -> tensor<1x64x28x40x!quant.uniform<i8:f32, 0.045550125984251968>> loc(#loc114)
        %279 = "tpu.Store"(%278, %0) {ginfo = #tpu.lg<out_addr = 0, out_size = 4480, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 14], h_slice = [14, 14], w_idx = [0], w_slice = [40], id = 13, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.045550125984251968>>, none) -> tensor<1x64x28x40x!quant.uniform<i8:f32, 0.045550125984251968>, 0 : i64> loc(#loc114)
        "tpu.Yield"(%279) : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.045550125984251968>, 0 : i64>) -> () loc(#loc114)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 3, 2, 13, -2, 6, 4, 5, -3, 9, 7, 8, -4, 11, 10, -5, 12, 0, 1], group_type = 0 : i64, hsecs = 2 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x32x28x40x!quant.uniform<i8:f32, 0.079176850393700787>, 179200 : i64>, tensor<1x32x28x40x!quant.uniform<i8:f32, 0.053568592125984249>, 143360 : i64>, tensor<1x32x28x40x!quant.uniform<i8:f32, 0.053309097637795275>, 71680 : i64>, tensor<1x32x28x40x!quant.uniform<i8:f32, 0.053309097637795275>, 107520 : i64>) -> tensor<1x64x28x40x!quant.uniform<i8:f32, 0.045550125984251968>, 0 : i64> loc(#loc114)
      %52 = "top.Weight"() : () -> tensor<1x128x1x9xi8, 1099514690176 : i64> loc(#loc126)
      %53 = "top.Weight"() : () -> tensor<1x128x9x64xsi8, 1099512924128 : i64> loc(#loc127)
      %54 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099512703408 : i64> loc(#loc128)
      %55 = "top.Weight"() : () -> tensor<1x128x1x9xi8, 1099513065888 : i64> loc(#loc129)
      %56 = "top.Weight"() : () -> tensor<1x128x1x128xsi8, 1099512831616 : i64> loc(#loc130)
      %57 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099512829552 : i64> loc(#loc131)
      %58 = "tpu.Group"(%51) ({
        %266 = "tpu.Load"(%53) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 9216, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [64], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x9x64xsi8, 1099512924128 : i64>) -> tensor<1x128x9x64xsi8> loc(#loc133)
        %267 = "tpu.Load"(%51) {do_bcast = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 4864, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 13], h_slice = [14, 15], w_idx = [0], w_slice = [40], id = 1, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.045550125984251968>, 0 : i64>) -> tensor<1x64x28x40x!quant.uniform<i8:f32, 0.045550125984251968>> loc(#loc134)
        %268 = "tpu.Load"(%52) {do_bcast = false, ginfo = #tpu.lg<out_addr = 28816, out_size = 144, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 2, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x9xi8, 1099514690176 : i64>) -> tensor<1x128x1x9xi8> loc(#loc135)
        %269 = "tpu.Load"(%54) {do_bcast = true, ginfo = #tpu.lg<out_addr = 26880, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 3, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099512703408 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc136)
        %270 = "tpu.Conv2D"(%267, %266, %268) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 20480, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 7], h_slice = [7, 7], w_idx = [0], w_slice = [20], id = 4, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [2, 2], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.045550125984251968>>, tensor<1x128x9x64xsi8>, tensor<1x128x1x9xi8>) -> tensor<1x128x14x20x!quant.uniform<i8:f32, 0.076320753543307085>> loc(#loc137)
        %271 = "tpu.Load"(%56) {do_bcast = false, ginfo = #tpu.lg<out_addr = 17152, out_size = 2048, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [128], id = 5, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x128xsi8, 1099512831616 : i64>) -> tensor<1x128x1x128xsi8> loc(#loc138)
        %272 = "tpu.Load"(%55) {do_bcast = false, ginfo = #tpu.lg<out_addr = 28672, out_size = 144, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 6, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x9xi8, 1099513065888 : i64>) -> tensor<1x128x1x9xi8> loc(#loc139)
        %273 = "tpu.Lut"(%270, %269) {ginfo = #tpu.lg<out_addr = 12288, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 7], h_slice = [7, 7], w_idx = [0], w_slice = [20], id = 7, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.076320753543307085>>, tensor<1x1x1x256xsi8>) -> tensor<1x128x14x20x!quant.uniform<i8:f32, 0.044790566141732283>> loc(#loc140)
        %274 = "tpu.Load"(%57) {do_bcast = true, ginfo = #tpu.lg<out_addr = 9216, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 8, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099512829552 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc141)
        %275 = "tpu.Conv2D"(%273, %271, %272) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 20480, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 7], h_slice = [7, 7], w_idx = [0], w_slice = [20], id = 9, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.044790566141732283>>, tensor<1x128x1x128xsi8>, tensor<1x128x1x9xi8>) -> tensor<1x128x14x20x!quant.uniform<i8:f32, 0.096875926771653547>> loc(#loc142)
        %276 = "tpu.Lut"(%275, %274) {ginfo = #tpu.lg<out_addr = 24576, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 7], h_slice = [7, 7], w_idx = [0], w_slice = [20], id = 10, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.096875926771653547>>, tensor<1x1x1x256xsi8>) -> tensor<1x128x14x20x!quant.uniform<i8:f32, 0.071136143307086605>> loc(#loc132)
        %277 = "tpu.Store"(%276, %0) {ginfo = #tpu.lg<out_addr = 24576, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 7], h_slice = [7, 7], w_idx = [0], w_slice = [20], id = 11, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.071136143307086605>>, none) -> tensor<1x128x14x20x!quant.uniform<i8:f32, 0.071136143307086605>, 71680 : i64> loc(#loc132)
        "tpu.Yield"(%277) : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.071136143307086605>, 71680 : i64>) -> () loc(#loc132)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 4, 3, 11, -2, 7, 5, 6, -3, 9, 8, 0, -4, 10, 1, 2], group_type = 0 : i64, hsecs = 2 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.045550125984251968>, 0 : i64>) -> tensor<1x128x14x20x!quant.uniform<i8:f32, 0.071136143307086605>, 71680 : i64> loc(#loc132)
      %59 = "tpu.Slice"(%58, %0, %0, %0, %0) {axes = [], ends = [9223372036854775807, 64, 9223372036854775807, 9223372036854775807], hasparamConvert_axes = [1], offset = [0, 0, 0, 0], steps = [1, 1, 1, 1]} : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.071136143307086605>, 71680 : i64>, none, none, none, none) -> tensor<1x64x14x20x!quant.uniform<i8:f32, 0.071136143307086605>, 71680 : i64> loc(#loc143)
      %60 = "tpu.Slice"(%58, %0, %0, %0, %0) {axes = [], ends = [9223372036854775807, 128, 9223372036854775807, 9223372036854775807], hasparamConvert_axes = [1], offset = [0, 64, 0, 0], steps = [1, 1, 1, 1]} : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.071136143307086605>, 71680 : i64>, none, none, none, none) -> tensor<1x64x14x20x!quant.uniform<i8:f32, 0.071136143307086605>, 89600 : i64> loc(#loc144)
      %61 = "top.Weight"() : () -> tensor<1x64x1x9xi8, 1099514646976 : i64> loc(#loc145)
      %62 = "top.Weight"() : () -> tensor<1x64x9x64xsi8, 1099512629680 : i64> loc(#loc146)
      %63 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099512628336 : i64> loc(#loc147)
      %64 = "top.Weight"() : () -> tensor<1x64x1x9xi8, 1099512629104 : i64> loc(#loc148)
      %65 = "top.Weight"() : () -> tensor<1x64x9x64xsi8, 1099512591472 : i64> loc(#loc149)
      %66 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099512591216 : i64> loc(#loc150)
      %67 = "top.Weight"() : () -> tensor<1x64x1x9xi8, 1099512590640 : i64> loc(#loc151)
      %68 = "top.Weight"() : () -> tensor<1x64x9x64xsi8, 1099513068704 : i64> loc(#loc152)
      %69 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099513107808 : i64> loc(#loc153)
      %70 = "top.Weight"() : () -> tensor<1x64x1x9xi8, 1099513067616 : i64> loc(#loc154)
      %71 = "top.Weight"() : () -> tensor<1x64x9x64xsi8, 1099512709488 : i64> loc(#loc155)
      %72 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099514398976 : i64> loc(#loc156)
      %73:2 = "tpu.Group"(%60) ({
        %266 = "tpu.Load"(%60) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [14], w_idx = [0], w_slice = [20], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.071136143307086605>, 89600 : i64>) -> tensor<1x64x14x20x!quant.uniform<i8:f32, 0.071136143307086605>> loc(#loc159)
        %267 = "tpu.Load"(%62) {do_bcast = false, ginfo = #tpu.lg<out_addr = 8192, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [64], id = 1, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x9x64xsi8, 1099512629680 : i64>) -> tensor<1x64x9x64xsi8> loc(#loc160)
        %268 = "tpu.Load"(%61) {do_bcast = false, ginfo = #tpu.lg<out_addr = 28672, out_size = 72, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 2, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x9xi8, 1099514646976 : i64>) -> tensor<1x64x1x9xi8> loc(#loc161)
        %269 = "tpu.Load"(%63) {do_bcast = true, ginfo = #tpu.lg<out_addr = 20480, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 3, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099512628336 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc162)
        %270 = "tpu.Conv2D"(%266, %267, %268) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [14], w_idx = [0], w_slice = [20], id = 4, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.071136143307086605>>, tensor<1x64x9x64xsi8>, tensor<1x64x1x9xi8>) -> tensor<1x64x14x20x!quant.uniform<i8:f32, 0.088245286614173232>> loc(#loc163)
        %271 = "tpu.Load"(%65) {do_bcast = false, ginfo = #tpu.lg<out_addr = 2304, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [64], id = 5, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x9x64xsi8, 1099512591472 : i64>) -> tensor<1x64x9x64xsi8> loc(#loc164)
        %272 = "tpu.Load"(%64) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 72, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 6, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x9xi8, 1099512629104 : i64>) -> tensor<1x64x1x9xi8> loc(#loc165)
        %273 = "tpu.Lut"(%270, %269) {ginfo = #tpu.lg<out_addr = 8192, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [14], w_idx = [0], w_slice = [20], id = 7, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.088245286614173232>>, tensor<1x1x1x256xsi8>) -> tensor<1x64x14x20x!quant.uniform<i8:f32, 0.046618206299212601>> loc(#loc166)
        %274 = "tpu.Load"(%66) {do_bcast = true, ginfo = #tpu.lg<out_addr = 16384, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 8, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099512591216 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc167)
        %275 = "tpu.Conv2D"(%273, %271, %272) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [14], w_idx = [0], w_slice = [20], id = 9, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.046618206299212601>>, tensor<1x64x9x64xsi8>, tensor<1x64x1x9xi8>) -> tensor<1x64x14x20x!quant.uniform<i8:f32, 0.071490182677165351>> loc(#loc168)
        %276 = "tpu.Lut"(%275, %274) {ginfo = #tpu.lg<out_addr = 20480, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [14], w_idx = [0], w_slice = [20], id = 10, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.071490182677165351>>, tensor<1x1x1x256xsi8>) -> tensor<1x64x14x20x!quant.uniform<i8:f32, 0.066449313385826769>> loc(#loc169)
        %277 = "tpu.Load"(%68) {do_bcast = false, ginfo = #tpu.lg<out_addr = 4096, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [64], id = 11, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x9x64xsi8, 1099513068704 : i64>) -> tensor<1x64x9x64xsi8> loc(#loc170)
        %278 = "tpu.Load"(%67) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 72, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 12, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x9xi8, 1099512590640 : i64>) -> tensor<1x64x1x9xi8> loc(#loc171)
        %279 = "tpu.Add"(%266, %276) {do_relu = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 2304, buffer_addr = 12288, buffer_size = 2304, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [14], w_idx = [0], w_slice = [20], id = 13, stage = 1, slice_idx = 0, group_type = 0>, multipliers = [74, 69], relu_limit = -1.000000e+00 : f64, rshifts = [6]} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.071136143307086605>>, tensor<1x64x14x20x!quant.uniform<i8:f32, 0.066449313385826769>>) -> tensor<1x64x14x20x!quant.uniform<i8:f32, 0.061521239370078737>> loc(#loc157)
        %280 = "tpu.Load"(%69) {do_bcast = true, ginfo = #tpu.lg<out_addr = 20480, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 14, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099513107808 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc172)
        %281 = "tpu.Store"(%279, %0) {ginfo = #tpu.lg<out_addr = 16384, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [14], w_idx = [0], w_slice = [20], id = 15, stage = 1, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.061521239370078737>>, none) -> tensor<1x64x14x20x!quant.uniform<i8:f32, 0.061521239370078737>, 143360 : i64> loc(#loc157)
        %282 = "tpu.Conv2D"(%279, %277, %278) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [14], w_idx = [0], w_slice = [20], id = 16, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.061521239370078737>>, tensor<1x64x9x64xsi8>, tensor<1x64x1x9xi8>) -> tensor<1x64x14x20x!quant.uniform<i8:f32, 0.060301919685039368>> loc(#loc173)
        %283 = "tpu.Load"(%71) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [64], id = 17, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x9x64xsi8, 1099512709488 : i64>) -> tensor<1x64x9x64xsi8> loc(#loc174)
        %284 = "tpu.Load"(%70) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 72, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 18, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x9xi8, 1099513067616 : i64>) -> tensor<1x64x1x9xi8> loc(#loc175)
        %285 = "tpu.Lut"(%282, %280) {ginfo = #tpu.lg<out_addr = 8192, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [14], w_idx = [0], w_slice = [20], id = 19, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.060301919685039368>>, tensor<1x1x1x256xsi8>) -> tensor<1x64x14x20x!quant.uniform<i8:f32, 0.034922958267716533>> loc(#loc176)
        %286 = "tpu.Load"(%72) {do_bcast = true, ginfo = #tpu.lg<out_addr = 18688, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 20, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099514398976 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc177)
        %287 = "tpu.Conv2D"(%285, %283, %284) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [14], w_idx = [0], w_slice = [20], id = 21, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.034922958267716533>>, tensor<1x64x9x64xsi8>, tensor<1x64x1x9xi8>) -> tensor<1x64x14x20x!quant.uniform<i8:f32, 0.14659774960629923>> loc(#loc178)
        %288 = "tpu.Lut"(%287, %286) {ginfo = #tpu.lg<out_addr = 20480, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [14], w_idx = [0], w_slice = [20], id = 22, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.14659774960629923>>, tensor<1x1x1x256xsi8>) -> tensor<1x64x14x20x!quant.uniform<i8:f32, 0.06446008031496063>> loc(#loc179)
        %289 = "tpu.Add"(%279, %288) {do_relu = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 2304, buffer_addr = 2304, buffer_size = 2304, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [14], w_idx = [0], w_slice = [20], id = 23, stage = 1, slice_idx = 0, group_type = 0>, multipliers = [105, 110], relu_limit = -1.000000e+00 : f64, rshifts = [7]} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.061521239370078737>>, tensor<1x64x14x20x!quant.uniform<i8:f32, 0.06446008031496063>>) -> tensor<1x64x14x20x!quant.uniform<i8:f32, 0.074925970866141736>> loc(#loc158)
        %290 = "tpu.Store"(%289, %0) {ginfo = #tpu.lg<out_addr = 24576, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [14], w_idx = [0], w_slice = [20], id = 24, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.074925970866141736>>, none) -> tensor<1x64x14x20x!quant.uniform<i8:f32, 0.074925970866141736>, 161280 : i64> loc(#loc158)
        "tpu.Yield"(%281, %290) : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.061521239370078737>, 143360 : i64>, tensor<1x64x14x20x!quant.uniform<i8:f32, 0.074925970866141736>, 161280 : i64>) -> () loc(#loc609)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 4, 3, 24, -2, 7, 5, 6, -3, 9, 8, -4, 10, -5, 13, 11, 12, -6, 16, 14, 15, -7, 19, 17, 18, -8, 21, 20, -9, 22, 0, -10, 23, 1, 2], group_type = 0 : i64, hsecs = 1 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.071136143307086605>, 89600 : i64>) -> (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.061521239370078737>, 143360 : i64>, tensor<1x64x14x20x!quant.uniform<i8:f32, 0.074925970866141736>, 161280 : i64>) loc(#loc609)
      %74 = "top.Weight"() : () -> tensor<1x128x1x9xi8, 1099512589488 : i64> loc(#loc180)
      %75 = "top.Weight"() : () -> tensor<1x128x1x256xsi8, 1099513241776 : i64> loc(#loc181)
      %76 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099513302288 : i64> loc(#loc182)
      %77 = "tpu.Group"(%59, %60, %73#0, %73#1) ({
        %266 = "tpu.Load"(%59) {do_bcast = false, ginfo = #tpu.lg<out_addr = 8704, out_size = 1152, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 7], h_slice = [7, 7], w_idx = [0], w_slice = [20], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.071136143307086605>, 71680 : i64>) -> tensor<1x64x14x20x!quant.uniform<i8:f32, 0.071136143307086605>> loc(#loc184)
        %267 = "tpu.Load"(%60) {do_bcast = false, ginfo = #tpu.lg<out_addr = 28672, out_size = 1152, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 7], h_slice = [7, 7], w_idx = [0], w_slice = [20], id = 1, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.071136143307086605>, 89600 : i64>) -> tensor<1x64x14x20x!quant.uniform<i8:f32, 0.071136143307086605>> loc(#loc159)
        %268 = "tpu.Load"(%73#0) {do_bcast = false, ginfo = #tpu.lg<out_addr = 20480, out_size = 1152, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 7], h_slice = [7, 7], w_idx = [0], w_slice = [20], id = 2, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.061521239370078737>, 143360 : i64>) -> tensor<1x64x14x20x!quant.uniform<i8:f32, 0.061521239370078737>> loc(#loc185)
        %269 = "tpu.Load"(%73#1) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 1152, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 7], h_slice = [7, 7], w_idx = [0], w_slice = [20], id = 3, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.074925970866141736>, 161280 : i64>) -> tensor<1x64x14x20x!quant.uniform<i8:f32, 0.074925970866141736>> loc(#loc186)
        %270 = "tpu.Load"(%75) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 4096, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 4, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x256xsi8, 1099513241776 : i64>) -> tensor<1x128x1x256xsi8> loc(#loc187)
        %271 = "tpu.Load"(%74) {do_bcast = false, ginfo = #tpu.lg<out_addr = 18688, out_size = 144, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 5, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x9xi8, 1099512589488 : i64>) -> tensor<1x128x1x9xi8> loc(#loc188)
        %272 = "tpu.Concat"(%266, %267, %268, %269) {axis = 1 : si32, do_relu = false, ginfo = #tpu.lg<out_addr = 4096, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0, 7], h_slice = [7, 7], w_idx = [0], w_slice = [20], id = 6, stage = 1, slice_idx = 0, group_type = 0>, multipliers = [1, 1, 110, 67], only_merge = false, relu_limit = -1.000000e+00 : f64, rshifts = [0, 0, 7, 6]} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.071136143307086605>>, tensor<1x64x14x20x!quant.uniform<i8:f32, 0.071136143307086605>>, tensor<1x64x14x20x!quant.uniform<i8:f32, 0.061521239370078737>>, tensor<1x64x14x20x!quant.uniform<i8:f32, 0.074925970866141736>>) -> tensor<1x256x14x20x!quant.uniform<i8:f32, 0.071136143307086605>> loc(#loc189)
        %273 = "tpu.Load"(%76) {do_bcast = true, ginfo = #tpu.lg<out_addr = 9856, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 7, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099513302288 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc190)
        %274 = "tpu.Conv2D"(%272, %270, %271) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 7], h_slice = [7, 7], w_idx = [0], w_slice = [20], id = 8, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x256x14x20x!quant.uniform<i8:f32, 0.071136143307086605>>, tensor<1x128x1x256xsi8>, tensor<1x128x1x9xi8>) -> tensor<1x128x14x20x!quant.uniform<i8:f32, 0.084550354330708663>> loc(#loc191)
        %275 = "tpu.Lut"(%274, %273) {ginfo = #tpu.lg<out_addr = 16384, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 7], h_slice = [7, 7], w_idx = [0], w_slice = [20], id = 9, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.084550354330708663>>, tensor<1x1x1x256xsi8>) -> tensor<1x128x14x20x!quant.uniform<i8:f32, 0.045643936220472442>> loc(#loc183)
        %276 = "tpu.Store"(%275, %0) {ginfo = #tpu.lg<out_addr = 16384, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 7], h_slice = [7, 7], w_idx = [0], w_slice = [20], id = 10, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.045643936220472442>>, none) -> tensor<1x128x14x20x!quant.uniform<i8:f32, 0.045643936220472442>, 107520 : i64> loc(#loc183)
        "tpu.Yield"(%276) : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.045643936220472442>, 107520 : i64>) -> () loc(#loc183)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 6, 4, 5, 10, -2, 8, 7, -3, 9, 0, 1, 2, 3], group_type = 0 : i64, hsecs = 2 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.071136143307086605>, 71680 : i64>, tensor<1x64x14x20x!quant.uniform<i8:f32, 0.071136143307086605>, 89600 : i64>, tensor<1x64x14x20x!quant.uniform<i8:f32, 0.061521239370078737>, 143360 : i64>, tensor<1x64x14x20x!quant.uniform<i8:f32, 0.074925970866141736>, 161280 : i64>) -> tensor<1x128x14x20x!quant.uniform<i8:f32, 0.045643936220472442>, 107520 : i64> loc(#loc183)
      %78 = "top.Weight"() {do_compress = true} : () -> tensor<1x256x1x9xi8, 1099513473168 : i64> loc(#loc192)
      %79 = "top.Weight"() {do_compress = true} : () -> tensor<1x256x9x128xsi8, 1099513475472 : i64> loc(#loc193)
      %80 = "tpu.Conv2D"(%77, %79, %78) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [2, 2], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.045643936220472442>, 107520 : i64>, tensor<1x256x9x128xsi8, 1099513475472 : i64>, tensor<1x256x1x9xi8, 1099513473168 : i64>) -> tensor<1x256x7x10x!quant.uniform<i8:f32, 0.076289011811023627>, 89600 : i64> loc(#loc194)
      %81 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099514023360 : i64> loc(#loc195)
      %82 = "top.Weight"() : () -> tensor<1x256x1x9xi8, 1099513770384 : i64> loc(#loc196)
      %83 = "top.Weight"() : () -> tensor<1x256x1x256xsi8, 1099513809552 : i64> loc(#loc197)
      %84 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099513023264 : i64> loc(#loc198)
      %85 = "tpu.Group"(%80) ({
        %266 = "tpu.Load"(%80) {do_bcast = false, ginfo = #tpu.lg<out_addr = 20480, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x7x10x!quant.uniform<i8:f32, 0.076289011811023627>, 89600 : i64>) -> tensor<1x256x7x10x!quant.uniform<i8:f32, 0.076289011811023627>> loc(#loc200)
        %267 = "tpu.Load"(%81) {do_bcast = true, ginfo = #tpu.lg<out_addr = 10752, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 1, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099514023360 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc201)
        %268 = "tpu.Load"(%83) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 8192, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 2, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x1x256xsi8, 1099513809552 : i64>) -> tensor<1x256x1x256xsi8> loc(#loc202)
        %269 = "tpu.Load"(%82) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 288, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 3, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x1x9xi8, 1099513770384 : i64>) -> tensor<1x256x1x9xi8> loc(#loc203)
        %270 = "tpu.Lut"(%266, %267) {ginfo = #tpu.lg<out_addr = 12288, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 4, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x256x7x10x!quant.uniform<i8:f32, 0.076289011811023627>>, tensor<1x1x1x256xsi8>) -> tensor<1x256x7x10x!quant.uniform<i8:f32, 0.066281227559055106>> loc(#loc204)
        %271 = "tpu.Load"(%84) {do_bcast = true, ginfo = #tpu.lg<out_addr = 28672, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 5, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099513023264 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc205)
        %272 = "tpu.Conv2D"(%270, %268, %269) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 6, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x256x7x10x!quant.uniform<i8:f32, 0.066281227559055106>>, tensor<1x256x1x256xsi8>, tensor<1x256x1x9xi8>) -> tensor<1x256x7x10x!quant.uniform<i8:f32, 0.099623086614173226>> loc(#loc206)
        %273 = "tpu.Lut"(%272, %271) {ginfo = #tpu.lg<out_addr = 8192, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 7, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x256x7x10x!quant.uniform<i8:f32, 0.099623086614173226>>, tensor<1x1x1x256xsi8>) -> tensor<1x256x7x10x!quant.uniform<i8:f32, 0.086719380314960625>> loc(#loc199)
        %274 = "tpu.Store"(%273, %0) {ginfo = #tpu.lg<out_addr = 8192, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 8, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x256x7x10x!quant.uniform<i8:f32, 0.086719380314960625>>, none) -> tensor<1x256x7x10x!quant.uniform<i8:f32, 0.086719380314960625>, 71680 : i64> loc(#loc199)
        "tpu.Yield"(%274) : (tensor<1x256x7x10x!quant.uniform<i8:f32, 0.086719380314960625>, 71680 : i64>) -> () loc(#loc199)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 4, 2, 3, -2, 6, 5, 8, -3, 7, 0, 1], group_type = 0 : i64, hsecs = 1 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x256x7x10x!quant.uniform<i8:f32, 0.076289011811023627>, 89600 : i64>) -> tensor<1x256x7x10x!quant.uniform<i8:f32, 0.086719380314960625>, 71680 : i64> loc(#loc199)
      %86 = "tpu.Slice"(%85, %0, %0, %0, %0) {axes = [], ends = [9223372036854775807, 128, 9223372036854775807, 9223372036854775807], hasparamConvert_axes = [1], offset = [0, 0, 0, 0], steps = [1, 1, 1, 1]} : (tensor<1x256x7x10x!quant.uniform<i8:f32, 0.086719380314960625>, 71680 : i64>, none, none, none, none) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.086719380314960625>, 71680 : i64> loc(#loc207)
      %87 = "tpu.Slice"(%85, %0, %0, %0, %0) {axes = [], ends = [9223372036854775807, 256, 9223372036854775807, 9223372036854775807], hasparamConvert_axes = [1], offset = [0, 128, 0, 0], steps = [1, 1, 1, 1]} : (tensor<1x256x7x10x!quant.uniform<i8:f32, 0.086719380314960625>, 71680 : i64>, none, none, none, none) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.086719380314960625>, 80640 : i64> loc(#loc208)
      %88 = "top.Weight"() {do_compress = true} : () -> tensor<1x128x1x9xi8, 1099514474688 : i64> loc(#loc209)
      %89 = "top.Weight"() {do_compress = true} : () -> tensor<1x128x9x128xsi8, 1099513875344 : i64> loc(#loc210)
      %90 = "tpu.Conv2D"(%87, %89, %88) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.086719380314960625>, 80640 : i64>, tensor<1x128x9x128xsi8, 1099513875344 : i64>, tensor<1x128x1x9xi8, 1099514474688 : i64>) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.10349535748031495>, 98560 : i64> loc(#loc211)
      %91 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099514023616 : i64> loc(#loc212)
      %92 = "top.Weight"() : () -> tensor<1x128x1x9xi8, 1099512705968 : i64> loc(#loc213)
      %93 = "top.Weight"() : () -> tensor<1x128x9x128xsi8, 1099514061312 : i64> loc(#loc214)
      %94 = "tpu.Group"(%90) ({
        %266 = "tpu.Load"(%90) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.10349535748031495>, 98560 : i64>) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.10349535748031495>> loc(#loc216)
        %267 = "tpu.Load"(%91) {do_bcast = true, ginfo = #tpu.lg<out_addr = 18432, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 1, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099514023616 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc217)
        %268 = "tpu.Load"(%93) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 18432, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [128], id = 2, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x9x128xsi8, 1099514061312 : i64>) -> tensor<1x128x9x128xsi8> loc(#loc218)
        %269 = "tpu.Load"(%92) {do_bcast = false, ginfo = #tpu.lg<out_addr = 18688, out_size = 144, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 3, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x9xi8, 1099512705968 : i64>) -> tensor<1x128x1x9xi8> loc(#loc219)
        %270 = "tpu.Lut"(%266, %267) {ginfo = #tpu.lg<out_addr = 20480, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 4, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.10349535748031495>>, tensor<1x1x1x256xsi8>) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.066988670866141728>> loc(#loc220)
        %271 = "tpu.Conv2D"(%270, %268, %269) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 28672, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 5, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.066988670866141728>>, tensor<1x128x9x128xsi8>, tensor<1x128x1x9xi8>) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.13391847716535435>> loc(#loc215)
        %272 = "tpu.Store"(%271, %0) {ginfo = #tpu.lg<out_addr = 28672, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 6, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.13391847716535435>>, none) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.13391847716535435>, 89600 : i64> loc(#loc215)
        "tpu.Yield"(%272) : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.13391847716535435>, 89600 : i64>) -> () loc(#loc215)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 4, 2, 3, 6, -2, 5, 0, 1], group_type = 0 : i64, hsecs = 1 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.10349535748031495>, 98560 : i64>) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.13391847716535435>, 89600 : i64> loc(#loc215)
      %95 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099514023088 : i64> loc(#loc221)
      %96 = "top.Weight"() : () -> tensor<1x256x1x9xi8, 1099514211328 : i64> loc(#loc222)
      %97 = "top.Weight"() : () -> tensor<1x256x1x384xsi8, 1099514213632 : i64> loc(#loc223)
      %98 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099512628848 : i64> loc(#loc224)
      %99 = "top.Weight"() : () -> tensor<1x128x1x9xi8, 1099514312192 : i64> loc(#loc225)
      %100 = "top.Weight"() : () -> tensor<1x128x1x256xsi8, 1099514313344 : i64> loc(#loc226)
      %101 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099514346368 : i64> loc(#loc227)
      %102 = "top.Weight"() : () -> tensor<1x256x1x9xi8, 1099514209024 : i64> loc(#loc228)
      %103 = "top.Weight"() : () -> tensor<1x256x1x512xsi8, 1099514475840 : i64> loc(#loc229)
      %104 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099514346112 : i64> loc(#loc230)
      %105 = "tpu.Group"(%94, %87, %86) ({
        %266 = "tpu.Load"(%94) {do_bcast = false, ginfo = #tpu.lg<out_addr = 8192, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.13391847716535435>, 89600 : i64>) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.13391847716535435>> loc(#loc232)
        %267 = "tpu.Load"(%95) {do_bcast = true, ginfo = #tpu.lg<out_addr = 12288, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 1, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099514023088 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc233)
        %268 = "tpu.Load"(%87) {do_bcast = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 2, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.086719380314960625>, 80640 : i64>) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.086719380314960625>> loc(#loc234)
        %269 = "tpu.Load"(%96) {do_bcast = false, ginfo = #tpu.lg<out_addr = 17664, out_size = 288, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 3, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x1x9xi8, 1099514211328 : i64>) -> tensor<1x256x1x9xi8> loc(#loc235)
        %270 = "tpu.Lut"(%266, %267) {ginfo = #tpu.lg<out_addr = 4096, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 4, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.13391847716535435>>, tensor<1x1x1x256xsi8>) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.096878862992125989>> loc(#loc236)
        %271 = "tpu.Load"(%86) {do_bcast = false, ginfo = #tpu.lg<out_addr = 20480, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 5, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.086719380314960625>, 71680 : i64>) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.086719380314960625>> loc(#loc237)
        %272 = "tpu.Add"(%268, %270) {do_relu = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 1280, buffer_addr = 8192, buffer_size = 1280, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 6, stage = 1, slice_idx = 0, group_type = 0>, multipliers = [93, 104], relu_limit = -1.000000e+00 : f64, rshifts = [7]} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.086719380314960625>>, tensor<1x128x7x10x!quant.uniform<i8:f32, 0.096878862992125989>>) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.11874968818897638>> loc(#loc238)
        %273 = "tpu.Load"(%97) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 12288, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [384], id = 7, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x1x384xsi8, 1099514213632 : i64>) -> tensor<1x256x1x384xsi8> loc(#loc239)
        %274 = "tpu.Concat"(%271, %268, %272) {axis = 1 : si32, do_relu = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 3840, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [384], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 8, stage = 1, slice_idx = 0, group_type = 0>, multipliers = [64, 64, 88], only_merge = false, relu_limit = -1.000000e+00 : f64, rshifts = [6, 6, 6]} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.086719380314960625>>, tensor<1x128x7x10x!quant.uniform<i8:f32, 0.086719380314960625>>, tensor<1x128x7x10x!quant.uniform<i8:f32, 0.11874968818897638>>) -> tensor<1x384x7x10x!quant.uniform<i8:f32, 0.086197319685039367>> loc(#loc240)
        %275 = "tpu.Load"(%98) {do_bcast = true, ginfo = #tpu.lg<out_addr = 28672, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 9, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099512628848 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc241)
        %276 = "tpu.Conv2D"(%274, %273, %269) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 20480, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 10, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x384x7x10x!quant.uniform<i8:f32, 0.086197319685039367>>, tensor<1x256x1x384xsi8>, tensor<1x256x1x9xi8>) -> tensor<1x256x7x10x!quant.uniform<i8:f32, 0.13678595748031494>> loc(#loc242)
        %277 = "tpu.Load"(%100) {do_bcast = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 4096, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 11, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x256xsi8, 1099514313344 : i64>) -> tensor<1x128x1x256xsi8> loc(#loc243)
        %278 = "tpu.Load"(%99) {do_bcast = false, ginfo = #tpu.lg<out_addr = 23040, out_size = 144, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 12, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x9xi8, 1099514312192 : i64>) -> tensor<1x128x1x9xi8> loc(#loc244)
        %279 = "tpu.Lut"(%276, %275) {ginfo = #tpu.lg<out_addr = 24576, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 13, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x256x7x10x!quant.uniform<i8:f32, 0.13678595748031494>>, tensor<1x1x1x256xsi8>) -> tensor<1x256x7x10x!quant.uniform<i8:f32, 0.088524759055118112>> loc(#loc245)
        %280 = "tpu.Load"(%101) {do_bcast = true, ginfo = #tpu.lg<out_addr = 28672, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 14, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099514346368 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc246)
        %281 = "tpu.Load"(%103) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 16384, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [512], id = 15, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x1x512xsi8, 1099514475840 : i64>) -> tensor<1x256x1x512xsi8> loc(#loc247)
        %282 = "tpu.Conv2D"(%279, %277, %278) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 20480, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 16, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x256x7x10x!quant.uniform<i8:f32, 0.088524759055118112>>, tensor<1x128x1x256xsi8>, tensor<1x128x1x9xi8>) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.09800595590551181>> loc(#loc248)
        %283 = "tpu.Lut"(%282, %280) {ginfo = #tpu.lg<out_addr = 24576, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 17, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.09800595590551181>>, tensor<1x1x1x256xsi8>) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.099224616535433074>> loc(#loc249)
        %284 = "tpu.Pool2D"(%283) {count_include_pad = false, do_relu = false, first_round_mode = #tpu<round_mode HalfAwayFromZero>, ginfo = #tpu.lg<out_addr = 28672, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 18, stage = 1, slice_idx = 0, group_type = 0>, is_adaptive = false, keepdims = true, kernel_shape = [5, 5], pad_value = 0 : i64, pads = [2, 2, 2, 2], pool_mode = #tpu<pool_mode Max>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfAwayFromZero>, strides = [1, 1]} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.099224616535433074>>) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.099224616535433074>> loc(#loc250)
        %285 = "tpu.Pool2D"(%284) {count_include_pad = false, do_relu = false, first_round_mode = #tpu<round_mode HalfAwayFromZero>, ginfo = #tpu.lg<out_addr = 21504, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 19, stage = 1, slice_idx = 0, group_type = 0>, is_adaptive = false, keepdims = true, kernel_shape = [5, 5], pad_value = 0 : i64, pads = [2, 2, 2, 2], pool_mode = #tpu<pool_mode Max>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfAwayFromZero>, strides = [1, 1]} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.099224616535433074>>) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.099224616535433074>> loc(#loc251)
        %286 = "tpu.Pool2D"(%285) {count_include_pad = false, do_relu = false, first_round_mode = #tpu<round_mode HalfAwayFromZero>, ginfo = #tpu.lg<out_addr = 22784, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 20, stage = 1, slice_idx = 0, group_type = 0>, is_adaptive = false, keepdims = true, kernel_shape = [5, 5], pad_value = 0 : i64, pads = [2, 2, 2, 2], pool_mode = #tpu<pool_mode Max>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfAwayFromZero>, strides = [1, 1]} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.099224616535433074>>) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.099224616535433074>> loc(#loc252)
        %287 = "tpu.Load"(%102) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24064, out_size = 288, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 21, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x1x9xi8, 1099514209024 : i64>) -> tensor<1x256x1x9xi8> loc(#loc253)
        %288 = "tpu.Concat"(%283, %284, %285, %286) {axis = 1 : si32, do_relu = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 5120, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [512], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 22, stage = 1, slice_idx = 0, group_type = 0>, multipliers = [1, 1, 1, 1], only_merge = true, relu_limit = -1.000000e+00 : f64, rshifts = [0, 0, 0, 0]} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.099224616535433074>>, tensor<1x128x7x10x!quant.uniform<i8:f32, 0.099224616535433074>>, tensor<1x128x7x10x!quant.uniform<i8:f32, 0.099224616535433074>>, tensor<1x128x7x10x!quant.uniform<i8:f32, 0.099224616535433074>>) -> tensor<1x512x7x10x!quant.uniform<i8:f32, 0.099224616535433074>> loc(#loc254)
        %289 = "tpu.Load"(%104) {do_bcast = true, ginfo = #tpu.lg<out_addr = 28672, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 23, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099514346112 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc255)
        %290 = "tpu.Conv2D"(%288, %281, %287) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 24, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x512x7x10x!quant.uniform<i8:f32, 0.099224616535433074>>, tensor<1x256x1x512xsi8>, tensor<1x256x1x9xi8>) -> tensor<1x256x7x10x!quant.uniform<i8:f32, 0.07386082204724409>> loc(#loc256)
        %291 = "tpu.Lut"(%290, %289) {ginfo = #tpu.lg<out_addr = 0, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 25, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x256x7x10x!quant.uniform<i8:f32, 0.07386082204724409>>, tensor<1x1x1x256xsi8>) -> tensor<1x256x7x10x!quant.uniform<i8:f32, 0.018032498425196851>> loc(#loc231)
        %292 = "tpu.Store"(%291, %0) {ginfo = #tpu.lg<out_addr = 0, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 26, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x256x7x10x!quant.uniform<i8:f32, 0.018032498425196851>>, none) -> tensor<1x256x7x10x!quant.uniform<i8:f32, 0.018032498425196851>, 409088 : i64> loc(#loc231)
        "tpu.Yield"(%292) : (tensor<1x256x7x10x!quant.uniform<i8:f32, 0.018032498425196851>, 409088 : i64>) -> () loc(#loc231)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 4, 2, 3, -2, 6, 5, 26, -3, 8, 7, -4, 10, 9, -5, 13, 11, 12, -6, 16, 14, 15, -7, 17, -8, 18, -9, 19, -10, 20, -11, 22, 21, -12, 24, 23, -13, 25, 0, 1], group_type = 0 : i64, hsecs = 1 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.13391847716535435>, 89600 : i64>, tensor<1x128x7x10x!quant.uniform<i8:f32, 0.086719380314960625>, 80640 : i64>, tensor<1x128x7x10x!quant.uniform<i8:f32, 0.086719380314960625>, 71680 : i64>) -> tensor<1x256x7x10x!quant.uniform<i8:f32, 0.018032498425196851>, 409088 : i64> loc(#loc231)
      %106 = "top.Weight"() : () -> tensor<1x256x2x2xsi8, 1099514346624 : i64> loc(#loc257)
      %107 = "top.Weight"() : () -> tensor<1x256x1x5xi8, 1099513145472 : i64> loc(#loc258)
      %108 = "top.Weight"() : () -> tensor<1x128x1x9xi8, 1099513312016 : i64> loc(#loc259)
      %109 = "top.Weight"() : () -> tensor<1x128x1x384xsi8, 1099514348160 : i64> loc(#loc260)
      %110 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099514397312 : i64> loc(#loc261)
      %111 = "tpu.Group"(%105, %77) ({
        %266 = "tpu.Load"(%105) {do_bcast = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 1536, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0, 3], h_slice = [4, 4], w_idx = [0], w_slice = [10], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x7x10x!quant.uniform<i8:f32, 0.018032498425196851>, 409088 : i64>) -> tensor<1x256x7x10x!quant.uniform<i8:f32, 0.018032498425196851>> loc(#loc263)
        %267 = "tpu.Load"(%106) {do_bcast = false, ginfo = #tpu.lg<out_addr = 28672, out_size = 512, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [2], w_idx = [0], w_slice = [2], id = 1, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x2x2xsi8, 1099514346624 : i64>) -> tensor<1x256x2x2xsi8> loc(#loc264)
        %268 = "tpu.Load"(%107) {do_bcast = false, ginfo = #tpu.lg<out_addr = 15104, out_size = 160, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [5], id = 2, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x1x5xi8, 1099513145472 : i64>) -> tensor<1x256x1x5xi8> loc(#loc265)
        %269 = "tpu.Load"(%77) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 7], h_slice = [7, 7], w_idx = [0], w_slice = [20], id = 3, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.045643936220472442>, 107520 : i64>) -> tensor<1x128x14x20x!quant.uniform<i8:f32, 0.045643936220472442>> loc(#loc266)
        %270 = "tpu.Deconv"(%266, %267, %268) {dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0, 7], h_slice = [7, 7], w_idx = [0], w_slice = [20], id = 4, stage = 1, slice_idx = 0, group_type = 0>, group = 256 : i64, inserts = [0, 0], kernel_shape = [2, 2], pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, strides = [2, 2], with_bias = false} : (tensor<1x256x7x10x!quant.uniform<i8:f32, 0.018032498425196851>>, tensor<1x256x2x2xsi8>, tensor<1x256x1x5xi8>) -> tensor<1x256x14x20x!quant.uniform<i8:f32, 0.033822777165354326>> loc(#loc267)
        %271 = "tpu.Load"(%109) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 6144, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [384], id = 5, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x384xsi8, 1099514348160 : i64>) -> tensor<1x128x1x384xsi8> loc(#loc268)
        %272 = "tpu.Load"(%108) {do_bcast = false, ginfo = #tpu.lg<out_addr = 29184, out_size = 144, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 6, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x9xi8, 1099513312016 : i64>) -> tensor<1x128x1x9xi8> loc(#loc269)
        %273 = "tpu.Concat"(%270, %269) {axis = 1 : si32, do_relu = false, ginfo = #tpu.lg<out_addr = 8192, out_size = 6912, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [384], d_idx = [0], d_slice = [1], h_idx = [0, 7], h_slice = [7, 7], w_idx = [0], w_slice = [20], id = 7, stage = 1, slice_idx = 0, group_type = 0>, multipliers = [113, 76], only_merge = false, relu_limit = -1.000000e+00 : f64, rshifts = [7, 6]} : (tensor<1x256x14x20x!quant.uniform<i8:f32, 0.033822777165354326>>, tensor<1x128x14x20x!quant.uniform<i8:f32, 0.045643936220472442>>) -> tensor<1x384x14x20x!quant.uniform<i8:f32, 0.038282122834645667>> loc(#loc270)
        %274 = "tpu.Load"(%110) {do_bcast = true, ginfo = #tpu.lg<out_addr = 20992, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 8, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099514397312 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc271)
        %275 = "tpu.Conv2D"(%273, %271, %272) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 7], h_slice = [7, 7], w_idx = [0], w_slice = [20], id = 9, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x384x14x20x!quant.uniform<i8:f32, 0.038282122834645667>>, tensor<1x128x1x384xsi8>, tensor<1x128x1x9xi8>) -> tensor<1x128x14x20x!quant.uniform<i8:f32, 0.078892362204724412>> loc(#loc272)
        %276 = "tpu.Lut"(%275, %274) {ginfo = #tpu.lg<out_addr = 6144, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 7], h_slice = [7, 7], w_idx = [0], w_slice = [20], id = 10, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.078892362204724412>>, tensor<1x1x1x256xsi8>) -> tensor<1x128x14x20x!quant.uniform<i8:f32, 0.054354860629921259>> loc(#loc262)
        %277 = "tpu.Store"(%276, %0) {ginfo = #tpu.lg<out_addr = 6144, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 7], h_slice = [7, 7], w_idx = [0], w_slice = [20], id = 11, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.054354860629921259>>, none) -> tensor<1x128x14x20x!quant.uniform<i8:f32, 0.054354860629921259>, 71680 : i64> loc(#loc262)
        "tpu.Yield"(%277) : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.054354860629921259>, 71680 : i64>) -> () loc(#loc262)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 4, 3, 11, -2, 7, 5, 6, -3, 9, 8, -4, 10, 0, 1, 2], group_type = 0 : i64, hsecs = 2 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x256x7x10x!quant.uniform<i8:f32, 0.018032498425196851>, 409088 : i64>, tensor<1x128x14x20x!quant.uniform<i8:f32, 0.045643936220472442>, 107520 : i64>) -> tensor<1x128x14x20x!quant.uniform<i8:f32, 0.054354860629921259>, 71680 : i64> loc(#loc262)
      %112 = "tpu.Slice"(%111, %0, %0, %0, %0) {axes = [], ends = [9223372036854775807, 64, 9223372036854775807, 9223372036854775807], hasparamConvert_axes = [1], offset = [0, 0, 0, 0], steps = [1, 1, 1, 1]} : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.054354860629921259>, 71680 : i64>, none, none, none, none) -> tensor<1x64x14x20x!quant.uniform<i8:f32, 0.054354860629921259>, 71680 : i64> loc(#loc273)
      %113 = "tpu.Slice"(%111, %0, %0, %0, %0) {axes = [], ends = [9223372036854775807, 128, 9223372036854775807, 9223372036854775807], hasparamConvert_axes = [1], offset = [0, 64, 0, 0], steps = [1, 1, 1, 1]} : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.054354860629921259>, 71680 : i64>, none, none, none, none) -> tensor<1x64x14x20x!quant.uniform<i8:f32, 0.054354860629921259>, 89600 : i64> loc(#loc274)
      %114 = "top.Weight"() : () -> tensor<1x64x1x9xi8, 1099514397568 : i64> loc(#loc275)
      %115 = "top.Weight"() : () -> tensor<1x64x9x64xsi8, 1099513772688 : i64> loc(#loc276)
      %116 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099514399232 : i64> loc(#loc277)
      %117 = "top.Weight"() : () -> tensor<1x64x1x9xi8, 1099513106080 : i64> loc(#loc278)
      %118 = "top.Weight"() : () -> tensor<1x64x9x64xsi8, 1099512887264 : i64> loc(#loc279)
      %119 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099514463200 : i64> loc(#loc280)
      %120 = "top.Weight"() : () -> tensor<1x128x1x9xi8, 1099514463744 : i64> loc(#loc281)
      %121 = "top.Weight"() : () -> tensor<1x128x1x192xsi8, 1099514400064 : i64> loc(#loc282)
      %122 = "tpu.Group"(%113, %112) ({
        %266 = "tpu.Load"(%113) {do_bcast = false, ginfo = #tpu.lg<out_addr = 8192, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [14], w_idx = [0], w_slice = [20], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.054354860629921259>, 89600 : i64>) -> tensor<1x64x14x20x!quant.uniform<i8:f32, 0.054354860629921259>> loc(#loc284)
        %267 = "tpu.Load"(%115) {do_bcast = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [64], id = 1, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x9x64xsi8, 1099513772688 : i64>) -> tensor<1x64x9x64xsi8> loc(#loc285)
        %268 = "tpu.Load"(%114) {do_bcast = false, ginfo = #tpu.lg<out_addr = 7056, out_size = 72, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 2, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x9xi8, 1099514397568 : i64>) -> tensor<1x64x1x9xi8> loc(#loc286)
        %269 = "tpu.Load"(%116) {do_bcast = true, ginfo = #tpu.lg<out_addr = 25088, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 3, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099514399232 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc287)
        %270 = "tpu.Conv2D"(%266, %267, %268) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 28672, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [14], w_idx = [0], w_slice = [20], id = 4, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.054354860629921259>>, tensor<1x64x9x64xsi8>, tensor<1x64x1x9xi8>) -> tensor<1x64x14x20x!quant.uniform<i8:f32, 0.078829742519685039>> loc(#loc288)
        %271 = "tpu.Load"(%118) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [64], id = 5, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x9x64xsi8, 1099512887264 : i64>) -> tensor<1x64x9x64xsi8> loc(#loc289)
        %272 = "tpu.Load"(%117) {do_bcast = false, ginfo = #tpu.lg<out_addr = 20480, out_size = 72, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 6, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x9xi8, 1099513106080 : i64>) -> tensor<1x64x1x9xi8> loc(#loc290)
        %273 = "tpu.Lut"(%270, %269) {ginfo = #tpu.lg<out_addr = 10496, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [14], w_idx = [0], w_slice = [20], id = 7, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.078829742519685039>>, tensor<1x1x1x256xsi8>) -> tensor<1x64x14x20x!quant.uniform<i8:f32, 0.034888820472440941>> loc(#loc291)
        %274 = "tpu.Load"(%119) {do_bcast = true, ginfo = #tpu.lg<out_addr = 24576, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 8, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099514463200 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc292)
        %275 = "tpu.Conv2D"(%273, %271, %272) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [14], w_idx = [0], w_slice = [20], id = 9, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.034888820472440941>>, tensor<1x64x9x64xsi8>, tensor<1x64x1x9xi8>) -> tensor<1x64x14x20x!quant.uniform<i8:f32, 0.068479537795275588>> loc(#loc293)
        %276 = "tpu.Load"(%112) {do_bcast = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [14], w_idx = [0], w_slice = [20], id = 10, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.054354860629921259>, 71680 : i64>) -> tensor<1x64x14x20x!quant.uniform<i8:f32, 0.054354860629921259>> loc(#loc294)
        %277 = "tpu.Lut"(%275, %274) {ginfo = #tpu.lg<out_addr = 20480, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [14], w_idx = [0], w_slice = [20], id = 11, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.068479537795275588>>, tensor<1x1x1x256xsi8>) -> tensor<1x64x14x20x!quant.uniform<i8:f32, 0.053985922834645671>> loc(#loc295)
        %278 = "tpu.Load"(%121) {do_bcast = false, ginfo = #tpu.lg<out_addr = 28672, out_size = 3072, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [192], id = 12, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x192xsi8, 1099514400064 : i64>) -> tensor<1x128x1x192xsi8> loc(#loc296)
        %279 = "tpu.Load"(%120) {do_bcast = false, ginfo = #tpu.lg<out_addr = 6912, out_size = 144, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 13, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x9xi8, 1099514463744 : i64>) -> tensor<1x128x1x9xi8> loc(#loc297)
        %280 = "tpu.Concat"(%276, %266, %277) {axis = 1 : si32, do_relu = false, ginfo = #tpu.lg<out_addr = 0, out_size = 6912, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [192], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [14], w_idx = [0], w_slice = [20], id = 14, stage = 1, slice_idx = 0, group_type = 0>, multipliers = [125, 125, 124], only_merge = false, relu_limit = -1.000000e+00 : f64, rshifts = [7, 7, 7]} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.054354860629921259>>, tensor<1x64x14x20x!quant.uniform<i8:f32, 0.054354860629921259>>, tensor<1x64x14x20x!quant.uniform<i8:f32, 0.053985922834645671>>) -> tensor<1x192x14x20x!quant.uniform<i8:f32, 0.055494613385826772>> loc(#loc298)
        %281 = "tpu.Conv2D"(%280, %278, %279) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 20480, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [14], w_idx = [0], w_slice = [20], id = 15, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x192x14x20x!quant.uniform<i8:f32, 0.055494613385826772>>, tensor<1x128x1x192xsi8>, tensor<1x128x1x9xi8>) -> tensor<1x128x14x20x!quant.uniform<i8:f32, 0.079855761417322826>> loc(#loc283)
        %282 = "tpu.Store"(%281, %0) {ginfo = #tpu.lg<out_addr = 20480, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [14], w_idx = [0], w_slice = [20], id = 16, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.079855761417322826>>, none) -> tensor<1x128x14x20x!quant.uniform<i8:f32, 0.079855761417322826>, 143360 : i64> loc(#loc283)
        "tpu.Yield"(%282) : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.079855761417322826>, 143360 : i64>) -> () loc(#loc283)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 4, 3, 16, -2, 7, 5, 6, -3, 9, 8, -4, 11, 10, -5, 14, 12, 13, -6, 15, 0, 1, 2], group_type = 0 : i64, hsecs = 1 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.054354860629921259>, 89600 : i64>, tensor<1x64x14x20x!quant.uniform<i8:f32, 0.054354860629921259>, 71680 : i64>) -> tensor<1x128x14x20x!quant.uniform<i8:f32, 0.079855761417322826>, 143360 : i64> loc(#loc283)
      %123 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099514461504 : i64> loc(#loc299)
      %124 = "top.Weight"() : () -> tensor<1x128x2x2xsi8, 1099512829808 : i64> loc(#loc300)
      %125 = "top.Weight"() : () -> tensor<1x128x1x5xi8, 1099514461760 : i64> loc(#loc301)
      %126 = "top.Weight"() : () -> tensor<1x64x1x9xi8, 1099513022688 : i64> loc(#loc302)
      %127 = "top.Weight"() : () -> tensor<1x64x1x192xsi8, 1099513460624 : i64> loc(#loc303)
      %128 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099514462400 : i64> loc(#loc304)
      %129:2 = "tpu.Group"(%122, %51) ({
        %266 = "tpu.Load"(%122) {do_bcast = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 7], h_slice = [7, 7], w_idx = [0], w_slice = [20], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.079855761417322826>, 143360 : i64>) -> tensor<1x128x14x20x!quant.uniform<i8:f32, 0.079855761417322826>> loc(#loc307)
        %267 = "tpu.Load"(%123) {do_bcast = true, ginfo = #tpu.lg<out_addr = 30080, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 1, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099514461504 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc308)
        %268 = "tpu.Load"(%124) {do_bcast = false, ginfo = #tpu.lg<out_addr = 16128, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [2], w_idx = [0], w_slice = [2], id = 2, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x2x2xsi8, 1099512829808 : i64>) -> tensor<1x128x2x2xsi8> loc(#loc309)
        %269 = "tpu.Load"(%125) {do_bcast = false, ginfo = #tpu.lg<out_addr = 30336, out_size = 80, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [5], id = 3, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x5xi8, 1099514461760 : i64>) -> tensor<1x128x1x5xi8> loc(#loc310)
        %270 = "tpu.Lut"(%266, %267) {ginfo = #tpu.lg<out_addr = 8192, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 7], h_slice = [7, 7], w_idx = [0], w_slice = [20], id = 4, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.079855761417322826>>, tensor<1x1x1x256xsi8>) -> tensor<1x128x14x20x!quant.uniform<i8:f32, 0.044711233070866142>> loc(#loc305)
        %271 = "tpu.Load"(%51) {do_bcast = false, ginfo = #tpu.lg<out_addr = 25344, out_size = 4480, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 14], h_slice = [14, 14], w_idx = [0], w_slice = [40], id = 5, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.045550125984251968>, 0 : i64>) -> tensor<1x64x28x40x!quant.uniform<i8:f32, 0.045550125984251968>> loc(#loc134)
        %272 = "tpu.Store"(%270, %0) {ginfo = #tpu.lg<out_addr = 8192, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 7], h_slice = [7, 7], w_idx = [0], w_slice = [20], id = 6, stage = 1, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.044711233070866142>>, none) -> tensor<1x128x14x20x!quant.uniform<i8:f32, 0.044711233070866142>, 373248 : i64> loc(#loc305)
        %273 = "tpu.Deconv"(%270, %268, %269) {dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 8960, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 14], h_slice = [14, 14], w_idx = [0], w_slice = [40], id = 7, stage = 1, slice_idx = 0, group_type = 0>, group = 128 : i64, inserts = [0, 0], kernel_shape = [2, 2], pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, strides = [2, 2], with_bias = false} : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.044711233070866142>>, tensor<1x128x2x2xsi8>, tensor<1x128x1x5xi8>) -> tensor<1x128x28x40x!quant.uniform<i8:f32, 0.044711233070866142>> loc(#loc311)
        %274 = "tpu.Load"(%127) {do_bcast = false, ginfo = #tpu.lg<out_addr = 14592, out_size = 1536, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [192], id = 8, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x192xsi8, 1099513460624 : i64>) -> tensor<1x64x1x192xsi8> loc(#loc312)
        %275 = "tpu.Load"(%126) {do_bcast = false, ginfo = #tpu.lg<out_addr = 30416, out_size = 72, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 9, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x9xi8, 1099513022688 : i64>) -> tensor<1x64x1x9xi8> loc(#loc313)
        %276 = "tpu.Concat"(%273, %271) {axis = 1 : si32, do_relu = false, ginfo = #tpu.lg<out_addr = 0, out_size = 13440, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [192], d_idx = [0], d_slice = [1], h_idx = [0, 14], h_slice = [14, 14], w_idx = [0], w_slice = [40], id = 10, stage = 1, slice_idx = 0, group_type = 0>, multipliers = [125, 1], only_merge = false, relu_limit = -1.000000e+00 : f64, rshifts = [7, 0]} : (tensor<1x128x28x40x!quant.uniform<i8:f32, 0.044711233070866142>>, tensor<1x64x28x40x!quant.uniform<i8:f32, 0.045550125984251968>>) -> tensor<1x192x28x40x!quant.uniform<i8:f32, 0.045550125984251968>> loc(#loc314)
        %277 = "tpu.Load"(%128) {do_bcast = true, ginfo = #tpu.lg<out_addr = 29824, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 11, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099514462400 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc315)
        %278 = "tpu.Conv2D"(%276, %274, %275) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 4480, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 14], h_slice = [14, 14], w_idx = [0], w_slice = [40], id = 12, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x192x28x40x!quant.uniform<i8:f32, 0.045550125984251968>>, tensor<1x64x1x192xsi8>, tensor<1x64x1x9xi8>) -> tensor<1x64x28x40x!quant.uniform<i8:f32, 0.070900556692913397>> loc(#loc316)
        %279 = "tpu.Lut"(%278, %277) {ginfo = #tpu.lg<out_addr = 0, out_size = 4480, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 14], h_slice = [14, 14], w_idx = [0], w_slice = [40], id = 13, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.070900556692913397>>, tensor<1x1x1x256xsi8>) -> tensor<1x64x28x40x!quant.uniform<i8:f32, 0.04466412362204724>> loc(#loc306)
        %280 = "tpu.Store"(%279, %0) {ginfo = #tpu.lg<out_addr = 0, out_size = 4480, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 14], h_slice = [14, 14], w_idx = [0], w_slice = [40], id = 14, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.04466412362204724>>, none) -> tensor<1x64x28x40x!quant.uniform<i8:f32, 0.04466412362204724>, 71680 : i64> loc(#loc306)
        "tpu.Yield"(%272, %280) : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.044711233070866142>, 373248 : i64>, tensor<1x64x28x40x!quant.uniform<i8:f32, 0.04466412362204724>, 71680 : i64>) -> () loc(#loc610)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 4, 2, 3, 14, -2, 7, 5, 6, -3, 10, 8, 9, -4, 12, 11, -5, 13, 0, 1], group_type = 0 : i64, hsecs = 2 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.079855761417322826>, 143360 : i64>, tensor<1x64x28x40x!quant.uniform<i8:f32, 0.045550125984251968>, 0 : i64>) -> (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.044711233070866142>, 373248 : i64>, tensor<1x64x28x40x!quant.uniform<i8:f32, 0.04466412362204724>, 71680 : i64>) loc(#loc610)
      %130 = "tpu.Slice"(%129#1, %0, %0, %0, %0) {axes = [], ends = [9223372036854775807, 32, 9223372036854775807, 9223372036854775807], hasparamConvert_axes = [1], offset = [0, 0, 0, 0], steps = [1, 1, 1, 1]} : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.04466412362204724>, 71680 : i64>, none, none, none, none) -> tensor<1x32x28x40x!quant.uniform<i8:f32, 0.04466412362204724>, 71680 : i64> loc(#loc317)
      %131 = "tpu.Slice"(%129#1, %0, %0, %0, %0) {axes = [], ends = [9223372036854775807, 64, 9223372036854775807, 9223372036854775807], hasparamConvert_axes = [1], offset = [0, 32, 0, 0], steps = [1, 1, 1, 1]} : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.04466412362204724>, 71680 : i64>, none, none, none, none) -> tensor<1x32x28x40x!quant.uniform<i8:f32, 0.04466412362204724>, 107520 : i64> loc(#loc318)
      %132 = "top.Weight"() : () -> tensor<1x32x1x9xi8, 1099514462656 : i64> loc(#loc319)
      %133 = "top.Weight"() : () -> tensor<1x32x9x32xsi8, 1099513292528 : i64> loc(#loc320)
      %134 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099514462944 : i64> loc(#loc321)
      %135 = "top.Weight"() : () -> tensor<1x32x1x9xi8, 1099514463456 : i64> loc(#loc322)
      %136 = "top.Weight"() : () -> tensor<1x32x9x32xsi8, 1099514464896 : i64> loc(#loc323)
      %137 = "tpu.Group"(%131) ({
        %266 = "tpu.Load"(%131) {do_bcast = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 4480, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [28], w_idx = [0], w_slice = [40], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x32x28x40x!quant.uniform<i8:f32, 0.04466412362204724>, 107520 : i64>) -> tensor<1x32x28x40x!quant.uniform<i8:f32, 0.04466412362204724>> loc(#loc325)
        %267 = "tpu.Load"(%133) {do_bcast = false, ginfo = #tpu.lg<out_addr = 5632, out_size = 1152, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [32], id = 1, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x32x9x32xsi8, 1099513292528 : i64>) -> tensor<1x32x9x32xsi8> loc(#loc326)
        %268 = "tpu.Load"(%132) {do_bcast = false, ginfo = #tpu.lg<out_addr = 6832, out_size = 36, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 2, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x32x1x9xi8, 1099514462656 : i64>) -> tensor<1x32x1x9xi8> loc(#loc327)
        %269 = "tpu.Load"(%134) {do_bcast = true, ginfo = #tpu.lg<out_addr = 29056, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 3, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099514462944 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc328)
        %270 = "tpu.Conv2D"(%266, %267, %268) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 0, out_size = 4480, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [28], w_idx = [0], w_slice = [40], id = 4, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x32x28x40x!quant.uniform<i8:f32, 0.04466412362204724>>, tensor<1x32x9x32xsi8>, tensor<1x32x1x9xi8>) -> tensor<1x32x28x40x!quant.uniform<i8:f32, 0.054905933070866143>> loc(#loc329)
        %271 = "tpu.Load"(%136) {do_bcast = false, ginfo = #tpu.lg<out_addr = 4480, out_size = 1152, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [32], id = 5, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x32x9x32xsi8, 1099514464896 : i64>) -> tensor<1x32x9x32xsi8> loc(#loc330)
        %272 = "tpu.Load"(%135) {do_bcast = false, ginfo = #tpu.lg<out_addr = 6784, out_size = 36, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 6, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x32x1x9xi8, 1099514463456 : i64>) -> tensor<1x32x1x9xi8> loc(#loc331)
        %273 = "tpu.Lut"(%270, %269) {ginfo = #tpu.lg<out_addr = 8192, out_size = 4480, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [28], w_idx = [0], w_slice = [40], id = 7, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x32x28x40x!quant.uniform<i8:f32, 0.054905933070866143>>, tensor<1x1x1x256xsi8>) -> tensor<1x32x28x40x!quant.uniform<i8:f32, 0.03054977874015748>> loc(#loc332)
        %274 = "tpu.Conv2D"(%273, %271, %272) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 4480, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [28], w_idx = [0], w_slice = [40], id = 8, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x32x28x40x!quant.uniform<i8:f32, 0.03054977874015748>>, tensor<1x32x9x32xsi8>, tensor<1x32x1x9xi8>) -> tensor<1x32x28x40x!quant.uniform<i8:f32, 0.073662014173228346>> loc(#loc324)
        %275 = "tpu.Store"(%274, %0) {ginfo = #tpu.lg<out_addr = 24576, out_size = 4480, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [28], w_idx = [0], w_slice = [40], id = 9, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x32x28x40x!quant.uniform<i8:f32, 0.073662014173228346>>, none) -> tensor<1x32x28x40x!quant.uniform<i8:f32, 0.073662014173228346>, 143360 : i64> loc(#loc324)
        "tpu.Yield"(%275) : (tensor<1x32x28x40x!quant.uniform<i8:f32, 0.073662014173228346>, 143360 : i64>) -> () loc(#loc324)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 4, 3, 9, -2, 7, 5, 6, -3, 8, 0, 1, 2], group_type = 0 : i64, hsecs = 1 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x32x28x40x!quant.uniform<i8:f32, 0.04466412362204724>, 107520 : i64>) -> tensor<1x32x28x40x!quant.uniform<i8:f32, 0.073662014173228346>, 143360 : i64> loc(#loc324)
      %138 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099513027056 : i64> loc(#loc333)
      %139 = "top.Weight"() : () -> tensor<1x64x1x9xi8, 1099513067040 : i64> loc(#loc334)
      %140 = "top.Weight"() : () -> tensor<1x64x1x96xsi8, 1099514691584 : i64> loc(#loc335)
      %141 = "tpu.Group"(%137, %130, %131) ({
        %266 = "tpu.Load"(%137) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 2240, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0, 14], h_slice = [14, 14], w_idx = [0], w_slice = [40], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x32x28x40x!quant.uniform<i8:f32, 0.073662014173228346>, 143360 : i64>) -> tensor<1x32x28x40x!quant.uniform<i8:f32, 0.073662014173228346>> loc(#loc337)
        %267 = "tpu.Load"(%138) {do_bcast = true, ginfo = #tpu.lg<out_addr = 6720, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 1, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099513027056 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc338)
        %268 = "tpu.Load"(%130) {do_bcast = false, ginfo = #tpu.lg<out_addr = 12672, out_size = 2240, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0, 14], h_slice = [14, 14], w_idx = [0], w_slice = [40], id = 2, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x32x28x40x!quant.uniform<i8:f32, 0.04466412362204724>, 71680 : i64>) -> tensor<1x32x28x40x!quant.uniform<i8:f32, 0.04466412362204724>> loc(#loc339)
        %269 = "tpu.Load"(%131) {do_bcast = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 2240, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0, 14], h_slice = [14, 14], w_idx = [0], w_slice = [40], id = 3, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x32x28x40x!quant.uniform<i8:f32, 0.04466412362204724>, 107520 : i64>) -> tensor<1x32x28x40x!quant.uniform<i8:f32, 0.04466412362204724>> loc(#loc325)
        %270 = "tpu.Lut"(%266, %267) {ginfo = #tpu.lg<out_addr = 20480, out_size = 2240, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0, 14], h_slice = [14, 14], w_idx = [0], w_slice = [40], id = 4, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x32x28x40x!quant.uniform<i8:f32, 0.073662014173228346>>, tensor<1x1x1x256xsi8>) -> tensor<1x32x28x40x!quant.uniform<i8:f32, 0.057849388188976378>> loc(#loc340)
        %271 = "tpu.Load"(%140) {do_bcast = false, ginfo = #tpu.lg<out_addr = 28672, out_size = 768, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [96], id = 5, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x96xsi8, 1099514691584 : i64>) -> tensor<1x64x1x96xsi8> loc(#loc341)
        %272 = "tpu.Load"(%139) {do_bcast = false, ginfo = #tpu.lg<out_addr = 6976, out_size = 72, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 6, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x9xi8, 1099513067040 : i64>) -> tensor<1x64x1x9xi8> loc(#loc342)
        %273 = "tpu.Concat"(%268, %269, %270) {axis = 1 : si32, do_relu = false, ginfo = #tpu.lg<out_addr = 0, out_size = 6720, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [96], d_idx = [0], d_slice = [1], h_idx = [0, 14], h_slice = [14, 14], w_idx = [0], w_slice = [40], id = 7, stage = 1, slice_idx = 0, group_type = 0>, multipliers = [98, 98, 64], only_merge = false, relu_limit = -1.000000e+00 : f64, rshifts = [7, 7, 6]} : (tensor<1x32x28x40x!quant.uniform<i8:f32, 0.04466412362204724>>, tensor<1x32x28x40x!quant.uniform<i8:f32, 0.04466412362204724>>, tensor<1x32x28x40x!quant.uniform<i8:f32, 0.057849388188976378>>) -> tensor<1x96x28x40x!quant.uniform<i8:f32, 0.057848840157480312>> loc(#loc343)
        %274 = "tpu.Conv2D"(%273, %271, %272) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 8192, out_size = 4480, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 14], h_slice = [14, 14], w_idx = [0], w_slice = [40], id = 8, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x96x28x40x!quant.uniform<i8:f32, 0.057848840157480312>>, tensor<1x64x1x96xsi8>, tensor<1x64x1x9xi8>) -> tensor<1x64x28x40x!quant.uniform<i8:f32, 0.070974507874015755>> loc(#loc336)
        %275 = "tpu.Store"(%274, %0) {ginfo = #tpu.lg<out_addr = 8192, out_size = 4480, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 14], h_slice = [14, 14], w_idx = [0], w_slice = [40], id = 9, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.070974507874015755>>, none) -> tensor<1x64x28x40x!quant.uniform<i8:f32, 0.070974507874015755>, 0 : i64> loc(#loc336)
        "tpu.Yield"(%275) : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.070974507874015755>, 0 : i64>) -> () loc(#loc336)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 4, 2, 3, 9, -2, 7, 5, 6, -3, 8, 0, 1], group_type = 0 : i64, hsecs = 2 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x32x28x40x!quant.uniform<i8:f32, 0.073662014173228346>, 143360 : i64>, tensor<1x32x28x40x!quant.uniform<i8:f32, 0.04466412362204724>, 71680 : i64>, tensor<1x32x28x40x!quant.uniform<i8:f32, 0.04466412362204724>, 107520 : i64>) -> tensor<1x64x28x40x!quant.uniform<i8:f32, 0.070974507874015755>, 0 : i64> loc(#loc336)
      %142 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099514347648 : i64> loc(#loc344)
      %143 = "top.Weight"() : () -> tensor<1x64x1x9xi8, 1099514474112 : i64> loc(#loc345)
      %144 = "top.Weight"() : () -> tensor<1x64x9x64xsi8, 1099512666544 : i64> loc(#loc346)
      %145:2 = "tpu.Group"(%141) ({
        %266 = "tpu.Load"(%141) {do_bcast = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 8960, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [28], w_idx = [0], w_slice = [40], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.070974507874015755>, 0 : i64>) -> tensor<1x64x28x40x!quant.uniform<i8:f32, 0.070974507874015755>> loc(#loc349)
        %267 = "tpu.Load"(%142) {do_bcast = true, ginfo = #tpu.lg<out_addr = 11264, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 1, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099514347648 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc350)
        %268 = "tpu.Load"(%144) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [64], id = 2, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x9x64xsi8, 1099512666544 : i64>) -> tensor<1x64x9x64xsi8> loc(#loc351)
        %269 = "tpu.Load"(%143) {do_bcast = false, ginfo = #tpu.lg<out_addr = 11520, out_size = 72, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 3, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x9xi8, 1099514474112 : i64>) -> tensor<1x64x1x9xi8> loc(#loc352)
        %270 = "tpu.Lut"(%266, %267) {ginfo = #tpu.lg<out_addr = 0, out_size = 8960, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [28], w_idx = [0], w_slice = [40], id = 4, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.070974507874015755>>, tensor<1x1x1x256xsi8>) -> tensor<1x64x28x40x!quant.uniform<i8:f32, 0.05442764724409449>> loc(#loc347)
        %271 = "tpu.Store"(%270, %0) {ginfo = #tpu.lg<out_addr = 0, out_size = 8960, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [28], w_idx = [0], w_slice = [40], id = 5, stage = 1, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.05442764724409449>>, none) -> tensor<1x64x28x40x!quant.uniform<i8:f32, 0.05442764724409449>, 71680 : i64> loc(#loc347)
        %272 = "tpu.Conv2D"(%270, %268, %269) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 8960, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [14], w_idx = [0], w_slice = [20], id = 6, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [2, 2], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.05442764724409449>>, tensor<1x64x9x64xsi8>, tensor<1x64x1x9xi8>) -> tensor<1x64x14x20x!quant.uniform<i8:f32, 0.09817057322834645>> loc(#loc348)
        %273 = "tpu.Store"(%272, %0) {ginfo = #tpu.lg<out_addr = 8960, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [14], w_idx = [0], w_slice = [20], id = 7, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.09817057322834645>>, none) -> tensor<1x64x14x20x!quant.uniform<i8:f32, 0.09817057322834645>, 319488 : i64> loc(#loc348)
        "tpu.Yield"(%271, %273) : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.05442764724409449>, 71680 : i64>, tensor<1x64x14x20x!quant.uniform<i8:f32, 0.09817057322834645>, 319488 : i64>) -> () loc(#loc611)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 4, 2, 3, 7, -2, 6, 5, 0, 1], group_type = 0 : i64, hsecs = 1 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.070974507874015755>, 0 : i64>) -> (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.05442764724409449>, 71680 : i64>, tensor<1x64x14x20x!quant.uniform<i8:f32, 0.09817057322834645>, 319488 : i64>) loc(#loc611)
      %146 = "top.Weight"() : () -> tensor<1x64x1x9xi8, 1099514398400 : i64> loc(#loc353)
      %147 = "top.Weight"() : () -> tensor<1x64x9x64xsi8, 1099512849344 : i64> loc(#loc354)
      %148 = "top.Weight"() : () -> tensor<1x64x1x9xi8, 1099514608448 : i64> loc(#loc355)
      %149 = "top.Weight"() : () -> tensor<1x64x9x64xsi8, 1099514609024 : i64> loc(#loc356)
      %150 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099514645888 : i64> loc(#loc357)
      %151 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099512886208 : i64> loc(#loc358)
      %152:3 = "tpu.Group"(%145#0, %145#1) ({
        %266 = "tpu.Load"(%145#0) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24960, out_size = 4864, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 13], h_slice = [15, 15], w_idx = [0], w_slice = [40], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.05442764724409449>, 71680 : i64>) -> tensor<1x64x28x40x!quant.uniform<i8:f32, 0.05442764724409449>> loc(#loc362)
        %267 = "tpu.Load"(%147) {do_bcast = false, ginfo = #tpu.lg<out_addr = 12672, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [64], id = 1, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x9x64xsi8, 1099512849344 : i64>) -> tensor<1x64x9x64xsi8> loc(#loc363)
        %268 = "tpu.Load"(%146) {do_bcast = false, ginfo = #tpu.lg<out_addr = 19024, out_size = 72, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 2, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x9xi8, 1099514398400 : i64>) -> tensor<1x64x1x9xi8> loc(#loc364)
        %269 = "tpu.Load"(%149) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [64], id = 3, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x9x64xsi8, 1099514609024 : i64>) -> tensor<1x64x9x64xsi8> loc(#loc365)
        %270 = "tpu.Load"(%148) {do_bcast = false, ginfo = #tpu.lg<out_addr = 18944, out_size = 72, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 4, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x9xi8, 1099514608448 : i64>) -> tensor<1x64x1x9xi8> loc(#loc366)
        %271 = "tpu.Conv2D"(%266, %267, %268) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 20480, out_size = 4480, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 14], h_slice = [14, 14], w_idx = [0], w_slice = [40], id = 5, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.05442764724409449>>, tensor<1x64x9x64xsi8>, tensor<1x64x1x9xi8>) -> tensor<1x64x28x40x!quant.uniform<i8:f32, 0.14023226614173229>> loc(#loc367)
        %272 = "tpu.Load"(%145#1) {do_bcast = false, ginfo = #tpu.lg<out_addr = 17280, out_size = 1152, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 7], h_slice = [7, 7], w_idx = [0], w_slice = [20], id = 6, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.09817057322834645>, 319488 : i64>) -> tensor<1x64x14x20x!quant.uniform<i8:f32, 0.09817057322834645>> loc(#loc368)
        %273 = "tpu.Load"(%150) {do_bcast = true, ginfo = #tpu.lg<out_addr = 18432, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 7, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099514645888 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc369)
        %274 = "tpu.Conv2D"(%266, %269, %270) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 8192, out_size = 4480, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 14], h_slice = [14, 14], w_idx = [0], w_slice = [40], id = 8, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.05442764724409449>>, tensor<1x64x9x64xsi8>, tensor<1x64x1x9xi8>) -> tensor<1x64x28x40x!quant.uniform<i8:f32, 0.091178946456692916>> loc(#loc359)
        %275 = "tpu.Load"(%151) {do_bcast = true, ginfo = #tpu.lg<out_addr = 18688, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 9, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099512886208 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc370)
        %276 = "tpu.Store"(%274, %0) {ginfo = #tpu.lg<out_addr = 8192, out_size = 4480, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 14], h_slice = [14, 14], w_idx = [0], w_slice = [40], id = 10, stage = 1, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.091178946456692916>>, none) -> tensor<1x64x28x40x!quant.uniform<i8:f32, 0.091178946456692916>, 0 : i64> loc(#loc359)
        %277 = "tpu.Lut"(%272, %273) {ginfo = #tpu.lg<out_addr = 29824, out_size = 1152, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 7], h_slice = [7, 7], w_idx = [0], w_slice = [20], id = 11, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.09817057322834645>>, tensor<1x1x1x256xsi8>) -> tensor<1x64x14x20x!quant.uniform<i8:f32, 0.070121227559055116>> loc(#loc360)
        %278 = "tpu.Store"(%277, %0) {ginfo = #tpu.lg<out_addr = 29824, out_size = 1152, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 7], h_slice = [7, 7], w_idx = [0], w_slice = [20], id = 12, stage = 1, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.070121227559055116>>, none) -> tensor<1x64x14x20x!quant.uniform<i8:f32, 0.070121227559055116>, 143360 : i64> loc(#loc360)
        %279 = "tpu.Lut"(%271, %275) {ginfo = #tpu.lg<out_addr = 4608, out_size = 4480, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 14], h_slice = [14, 14], w_idx = [0], w_slice = [40], id = 13, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.14023226614173229>>, tensor<1x1x1x256xsi8>) -> tensor<1x64x28x40x!quant.uniform<i8:f32, 0.053514148818897635>> loc(#loc361)
        %280 = "tpu.Store"(%279, %0) {ginfo = #tpu.lg<out_addr = 4608, out_size = 4480, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 14], h_slice = [14, 14], w_idx = [0], w_slice = [40], id = 14, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.053514148818897635>>, none) -> tensor<1x64x28x40x!quant.uniform<i8:f32, 0.053514148818897635>, 247808 : i64> loc(#loc361)
        "tpu.Yield"(%276, %278, %280) : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.091178946456692916>, 0 : i64>, tensor<1x64x14x20x!quant.uniform<i8:f32, 0.070121227559055116>, 143360 : i64>, tensor<1x64x28x40x!quant.uniform<i8:f32, 0.053514148818897635>, 247808 : i64>) -> () loc(#loc612)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 5, 3, 4, 14, -2, 8, 6, 7, -3, 11, 9, 10, -4, 13, 12, 0, 1, 2], group_type = 0 : i64, hsecs = 2 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.05442764724409449>, 71680 : i64>, tensor<1x64x14x20x!quant.uniform<i8:f32, 0.09817057322834645>, 319488 : i64>) -> (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.091178946456692916>, 0 : i64>, tensor<1x64x14x20x!quant.uniform<i8:f32, 0.070121227559055116>, 143360 : i64>, tensor<1x64x28x40x!quant.uniform<i8:f32, 0.053514148818897635>, 247808 : i64>) loc(#loc612)
      %153 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099514646144 : i64> loc(#loc371)
      %154 = "tpu.Lut"(%152#0, %153) : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.091178946456692916>, 0 : i64>, tensor<1x1x1x256xsi8, 1099514646144 : i64>) -> tensor<1x64x28x40x!quant.uniform<i8:f32, 0.086384731496062988>, 71680 : i64> loc(#loc372)
      %155 = "top.Weight"() : () -> tensor<1x64x1x9xi8, 1099514646400 : i64> loc(#loc373)
      %156 = "top.Weight"() : () -> tensor<1x64x9x64xsi8, 1099514652160 : i64> loc(#loc374)
      %157:2 = "tpu.Group"(%152#1, %129#0, %152#2) ({
        %266 = "tpu.Load"(%152#1) {do_bcast = false, ginfo = #tpu.lg<out_addr = 4864, out_size = 1152, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 7], h_slice = [7, 7], w_idx = [0], w_slice = [20], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.070121227559055116>, 143360 : i64>) -> tensor<1x64x14x20x!quant.uniform<i8:f32, 0.070121227559055116>> loc(#loc377)
        %267 = "tpu.Load"(%129#0) {do_bcast = false, ginfo = #tpu.lg<out_addr = 28672, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 7], h_slice = [7, 7], w_idx = [0], w_slice = [20], id = 1, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.044711233070866142>, 373248 : i64>) -> tensor<1x128x14x20x!quant.uniform<i8:f32, 0.044711233070866142>> loc(#loc378)
        %268 = "tpu.Load"(%152#2) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 4864, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 13], h_slice = [15, 15], w_idx = [0], w_slice = [40], id = 2, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.053514148818897635>, 247808 : i64>) -> tensor<1x64x28x40x!quant.uniform<i8:f32, 0.053514148818897635>> loc(#loc379)
        %269 = "tpu.Load"(%156) {do_bcast = false, ginfo = #tpu.lg<out_addr = 8192, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [64], id = 3, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x9x64xsi8, 1099514652160 : i64>) -> tensor<1x64x9x64xsi8> loc(#loc380)
        %270 = "tpu.Load"(%155) {do_bcast = false, ginfo = #tpu.lg<out_addr = 6016, out_size = 72, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 4, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x9xi8, 1099514646400 : i64>) -> tensor<1x64x1x9xi8> loc(#loc381)
        %271 = "tpu.Concat"(%266, %267) {axis = 1 : si32, do_relu = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 3456, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [192], d_idx = [0], d_slice = [1], h_idx = [0, 7], h_slice = [7, 7], w_idx = [0], w_slice = [20], id = 5, stage = 1, slice_idx = 0, group_type = 0>, multipliers = [1, 81], only_merge = false, relu_limit = -1.000000e+00 : f64, rshifts = [0, 7]} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.070121227559055116>>, tensor<1x128x14x20x!quant.uniform<i8:f32, 0.044711233070866142>>) -> tensor<1x192x14x20x!quant.uniform<i8:f32, 0.070121227559055116>> loc(#loc375)
        %272 = "tpu.Store"(%271, %0) {ginfo = #tpu.lg<out_addr = 24576, out_size = 3456, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [192], d_idx = [0], d_slice = [1], h_idx = [0, 7], h_slice = [7, 7], w_idx = [0], w_slice = [20], id = 6, stage = 1, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x192x14x20x!quant.uniform<i8:f32, 0.070121227559055116>>, none) -> tensor<1x192x14x20x!quant.uniform<i8:f32, 0.070121227559055116>, 319488 : i64> loc(#loc375)
        %273 = "tpu.Conv2D"(%268, %269, %270) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 4480, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 14], h_slice = [14, 14], w_idx = [0], w_slice = [40], id = 7, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.053514148818897635>>, tensor<1x64x9x64xsi8>, tensor<1x64x1x9xi8>) -> tensor<1x64x28x40x!quant.uniform<i8:f32, 0.19547489527559056>> loc(#loc376)
        %274 = "tpu.Store"(%273, %0) {ginfo = #tpu.lg<out_addr = 16384, out_size = 4480, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 14], h_slice = [14, 14], w_idx = [0], w_slice = [40], id = 8, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.19547489527559056>>, none) -> tensor<1x64x28x40x!quant.uniform<i8:f32, 0.19547489527559056>, 176128 : i64> loc(#loc376)
        "tpu.Yield"(%272, %274) : (tensor<1x192x14x20x!quant.uniform<i8:f32, 0.070121227559055116>, 319488 : i64>, tensor<1x64x28x40x!quant.uniform<i8:f32, 0.19547489527559056>, 176128 : i64>) -> () loc(#loc613)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 5, 2, 3, 4, 8, -2, 7, 6, 0, 1], group_type = 0 : i64, hsecs = 2 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.070121227559055116>, 143360 : i64>, tensor<1x128x14x20x!quant.uniform<i8:f32, 0.044711233070866142>, 373248 : i64>, tensor<1x64x28x40x!quant.uniform<i8:f32, 0.053514148818897635>, 247808 : i64>) -> (tensor<1x192x14x20x!quant.uniform<i8:f32, 0.070121227559055116>, 319488 : i64>, tensor<1x64x28x40x!quant.uniform<i8:f32, 0.19547489527559056>, 176128 : i64>) loc(#loc613)
      %158 = "top.Weight"() : () -> tensor<1x64x1x9xi8, 1099513106656 : i64> loc(#loc382)
      %159 = "top.Weight"() : () -> tensor<1x64x9x64xsi8, 1099513147008 : i64> loc(#loc383)
      %160 = "top.Weight"() : () -> tensor<1x128x1x9xi8, 1099514689024 : i64> loc(#loc384)
      %161 = "top.Weight"() : () -> tensor<1x128x1x192xsi8, 1099513031696 : i64> loc(#loc385)
      %162:2 = "tpu.Group"(%154, %157#0) ({
        %266 = "tpu.Load"(%154) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 4864, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 13], h_slice = [15, 15], w_idx = [0], w_slice = [40], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.086384731496062988>, 71680 : i64>) -> tensor<1x64x28x40x!quant.uniform<i8:f32, 0.086384731496062988>> loc(#loc388)
        %267 = "tpu.Load"(%159) {do_bcast = false, ginfo = #tpu.lg<out_addr = 8192, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [64], id = 1, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x9x64xsi8, 1099513147008 : i64>) -> tensor<1x64x9x64xsi8> loc(#loc389)
        %268 = "tpu.Load"(%158) {do_bcast = false, ginfo = #tpu.lg<out_addr = 7312, out_size = 72, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 2, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x9xi8, 1099513106656 : i64>) -> tensor<1x64x1x9xi8> loc(#loc390)
        %269 = "tpu.Load"(%157#0) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 3456, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [192], d_idx = [0], d_slice = [1], h_idx = [0, 7], h_slice = [7, 7], w_idx = [0], w_slice = [20], id = 3, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x192x14x20x!quant.uniform<i8:f32, 0.070121227559055116>, 319488 : i64>) -> tensor<1x192x14x20x!quant.uniform<i8:f32, 0.070121227559055116>> loc(#loc391)
        %270 = "tpu.Load"(%161) {do_bcast = false, ginfo = #tpu.lg<out_addr = 28672, out_size = 3072, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [192], id = 4, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x192xsi8, 1099513031696 : i64>) -> tensor<1x128x1x192xsi8> loc(#loc392)
        %271 = "tpu.Load"(%160) {do_bcast = false, ginfo = #tpu.lg<out_addr = 7168, out_size = 144, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 5, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x9xi8, 1099514689024 : i64>) -> tensor<1x128x1x9xi8> loc(#loc393)
        %272 = "tpu.Conv2D"(%266, %267, %268) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 4480, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 14], h_slice = [14, 14], w_idx = [0], w_slice = [40], id = 6, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.086384731496062988>>, tensor<1x64x9x64xsi8>, tensor<1x64x1x9xi8>) -> tensor<1x64x28x40x!quant.uniform<i8:f32, 0.20691319212598425>> loc(#loc386)
        %273 = "tpu.Store"(%272, %0) {ginfo = #tpu.lg<out_addr = 16384, out_size = 4480, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 14], h_slice = [14, 14], w_idx = [0], w_slice = [40], id = 7, stage = 1, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.20691319212598425>>, none) -> tensor<1x64x28x40x!quant.uniform<i8:f32, 0.20691319212598425>, 0 : i64> loc(#loc386)
        %274 = "tpu.Conv2D"(%269, %270, %271) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 4864, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 7], h_slice = [7, 7], w_idx = [0], w_slice = [20], id = 8, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x192x14x20x!quant.uniform<i8:f32, 0.070121227559055116>>, tensor<1x128x1x192xsi8>, tensor<1x128x1x9xi8>) -> tensor<1x128x14x20x!quant.uniform<i8:f32, 0.10717361968503937>> loc(#loc387)
        %275 = "tpu.Store"(%274, %0) {ginfo = #tpu.lg<out_addr = 4864, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 7], h_slice = [7, 7], w_idx = [0], w_slice = [20], id = 9, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.10717361968503937>>, none) -> tensor<1x128x14x20x!quant.uniform<i8:f32, 0.10717361968503937>, 283648 : i64> loc(#loc387)
        "tpu.Yield"(%273, %275) : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.20691319212598425>, 0 : i64>, tensor<1x128x14x20x!quant.uniform<i8:f32, 0.10717361968503937>, 283648 : i64>) -> () loc(#loc614)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 6, 3, 4, 5, 9, -2, 8, 7, 0, 1, 2], group_type = 0 : i64, hsecs = 2 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [9], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.086384731496062988>, 71680 : i64>, tensor<1x192x14x20x!quant.uniform<i8:f32, 0.070121227559055116>, 319488 : i64>) -> (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.20691319212598425>, 0 : i64>, tensor<1x128x14x20x!quant.uniform<i8:f32, 0.10717361968503937>, 283648 : i64>) loc(#loc614)
      %163 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099514691328 : i64> loc(#loc394)
      %164 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099514697728 : i64> loc(#loc395)
      %165 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099513223088 : i64> loc(#loc396)
      %166 = "top.Weight"() : () -> tensor<1x64x1x9xi8, 1099512588912 : i64> loc(#loc397)
      %167 = "top.Weight"() : () -> tensor<1x64x1x64xsi8, 1099512584816 : i64> loc(#loc398)
      %168 = "top.Weight"() : () -> tensor<1x4x1x9xi8, 1099513223040 : i64> loc(#loc399)
      %169 = "top.Weight"() : () -> tensor<1x4x1x64xsi8, 1099512584560 : i64> loc(#loc400)
      %170:3 = "tpu.Group"(%157#1, %162#0, %162#1) ({
        %266 = "tpu.Load"(%157#1) {do_bcast = false, ginfo = #tpu.lg<out_addr = 20480, out_size = 4480, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 14], h_slice = [14, 14], w_idx = [0], w_slice = [40], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.19547489527559056>, 176128 : i64>) -> tensor<1x64x28x40x!quant.uniform<i8:f32, 0.19547489527559056>> loc(#loc404)
        %267 = "tpu.Load"(%163) {do_bcast = true, ginfo = #tpu.lg<out_addr = 11520, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 1, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099514691328 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc405)
        %268 = "tpu.Load"(%162#0) {do_bcast = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 4480, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 14], h_slice = [14, 14], w_idx = [0], w_slice = [40], id = 2, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.20691319212598425>, 0 : i64>) -> tensor<1x64x28x40x!quant.uniform<i8:f32, 0.20691319212598425>> loc(#loc406)
        %269 = "tpu.Load"(%164) {do_bcast = true, ginfo = #tpu.lg<out_addr = 28672, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 3, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099514697728 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc407)
        %270 = "tpu.Lut"(%266, %267) {ginfo = #tpu.lg<out_addr = 0, out_size = 4480, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 14], h_slice = [14, 14], w_idx = [0], w_slice = [40], id = 4, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.19547489527559056>>, tensor<1x1x1x256xsi8>) -> tensor<1x64x28x40x!quant.uniform<i8:f32, 0.15002548661417323>> loc(#loc408)
        %271 = "tpu.Load"(%162#1) {do_bcast = false, ginfo = #tpu.lg<out_addr = 20480, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 7], h_slice = [7, 7], w_idx = [0], w_slice = [20], id = 5, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.10717361968503937>, 283648 : i64>) -> tensor<1x128x14x20x!quant.uniform<i8:f32, 0.10717361968503937>> loc(#loc409)
        %272 = "tpu.Load"(%165) {do_bcast = true, ginfo = #tpu.lg<out_addr = 11264, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 6, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099513223088 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc410)
        %273 = "tpu.Lut"(%268, %269) {ginfo = #tpu.lg<out_addr = 4480, out_size = 4480, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 14], h_slice = [14, 14], w_idx = [0], w_slice = [40], id = 7, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.20691319212598425>>, tensor<1x1x1x256xsi8>) -> tensor<1x64x28x40x!quant.uniform<i8:f32, 0.12737548976377952>> loc(#loc411)
        %274 = "tpu.Load"(%167) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24960, out_size = 512, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [64], id = 8, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x64xsi8, 1099512584816 : i64>) -> tensor<1x64x1x64xsi8> loc(#loc412)
        %275 = "tpu.Load"(%166) {do_bcast = false, ginfo = #tpu.lg<out_addr = 29488, out_size = 72, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 9, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x9xi8, 1099512588912 : i64>) -> tensor<1x64x1x9xi8> loc(#loc413)
        %276 = "tpu.Lut"(%271, %272) {ginfo = #tpu.lg<out_addr = 8960, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 7], h_slice = [7, 7], w_idx = [0], w_slice = [20], id = 10, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.10717361968503937>>, tensor<1x1x1x256xsi8>) -> tensor<1x128x14x20x!quant.uniform<i8:f32, 0.086351832283464561>> loc(#loc401)
        %277 = "tpu.Load"(%169) {do_bcast = false, ginfo = #tpu.lg<out_addr = 11776, out_size = 64, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [4], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [64], id = 11, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x4x1x64xsi8, 1099512584560 : i64>) -> tensor<1x4x1x64xsi8> loc(#loc414)
        %278 = "tpu.Load"(%168) {do_bcast = false, ginfo = #tpu.lg<out_addr = 11840, out_size = 9, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [4], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 12, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x4x1x9xi8, 1099513223040 : i64>) -> tensor<1x4x1x9xi8> loc(#loc415)
        %279 = "tpu.Store"(%276, %0) {ginfo = #tpu.lg<out_addr = 8960, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 7], h_slice = [7, 7], w_idx = [0], w_slice = [20], id = 13, stage = 1, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.086351832283464561>>, none) -> tensor<1x128x14x20x!quant.uniform<i8:f32, 0.086351832283464561>, 247808 : i64> loc(#loc401)
        %280 = "tpu.Conv2D"(%270, %274, %275) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 4480, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 14], h_slice = [14, 14], w_idx = [0], w_slice = [40], id = 14, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.15002548661417323>>, tensor<1x64x1x64xsi8>, tensor<1x64x1x9xi8>) -> tensor<1x64x28x40x!quant.uniform<i8:f32, 0.14529467007874017>> loc(#loc402)
        %281 = "tpu.Store"(%280, %0) {ginfo = #tpu.lg<out_addr = 12288, out_size = 4480, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 14], h_slice = [14, 14], w_idx = [0], w_slice = [40], id = 15, stage = 1, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.14529467007874017>>, none) -> tensor<1x64x28x40x!quant.uniform<i8:f32, 0.14529467007874017>, 99968 : i64> loc(#loc402)
        %282 = "tpu.Conv2D"(%273, %277, %278) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 28928, out_size = 560, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [4], d_idx = [0], d_slice = [1], h_idx = [0, 14], h_slice = [14, 14], w_idx = [0], w_slice = [40], id = 16, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.12737548976377952>>, tensor<1x4x1x64xsi8>, tensor<1x4x1x9xi8>) -> tensor<1x4x28x40x!quant.uniform<i8:f32, 0.14529467007874017>> loc(#loc403)
        %283 = "tpu.Store"(%282, %0) {ginfo = #tpu.lg<out_addr = 28928, out_size = 560, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [4], d_idx = [0], d_slice = [1], h_idx = [0, 14], h_slice = [14, 14], w_idx = [0], w_slice = [40], id = 17, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x4x28x40x!quant.uniform<i8:f32, 0.14529467007874017>>, none) -> tensor<1x4x28x40x!quant.uniform<i8:f32, 0.14529467007874017>, 171648 : i64> loc(#loc403)
        "tpu.Yield"(%279, %281, %283) : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.086351832283464561>, 247808 : i64>, tensor<1x64x28x40x!quant.uniform<i8:f32, 0.14529467007874017>, 99968 : i64>, tensor<1x4x28x40x!quant.uniform<i8:f32, 0.14529467007874017>, 171648 : i64>) -> () loc(#loc615)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 4, 2, 3, 17, -2, 7, 5, 6, -3, 10, 8, 9, -4, 14, 11, 12, 13, -5, 16, 15, 0, 1], group_type = 0 : i64, hsecs = 2 : i64, nsecs = 1 : i64, other_down_overlap_op = [-1, 9], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.19547489527559056>, 176128 : i64>, tensor<1x64x28x40x!quant.uniform<i8:f32, 0.20691319212598425>, 0 : i64>, tensor<1x128x14x20x!quant.uniform<i8:f32, 0.10717361968503937>, 283648 : i64>) -> (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.086351832283464561>, 247808 : i64>, tensor<1x64x28x40x!quant.uniform<i8:f32, 0.14529467007874017>, 99968 : i64>, tensor<1x4x28x40x!quant.uniform<i8:f32, 0.14529467007874017>, 171648 : i64>) loc(#loc615)
      %171 = "tpu.Slice"(%170#0, %0, %0, %0, %0) {axes = [], ends = [9223372036854775807, 64, 9223372036854775807, 9223372036854775807], hasparamConvert_axes = [1], offset = [0, 0, 0, 0], steps = [1, 1, 1, 1]} : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.086351832283464561>, 247808 : i64>, none, none, none, none) -> tensor<1x64x14x20x!quant.uniform<i8:f32, 0.086351832283464561>, 247808 : i64> loc(#loc416)
      %172 = "tpu.Slice"(%170#0, %0, %0, %0, %0) {axes = [], ends = [9223372036854775807, 128, 9223372036854775807, 9223372036854775807], hasparamConvert_axes = [1], offset = [0, 64, 0, 0], steps = [1, 1, 1, 1]} : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.086351832283464561>, 247808 : i64>, none, none, none, none) -> tensor<1x64x14x20x!quant.uniform<i8:f32, 0.086351832283464561>, 265728 : i64> loc(#loc417)
      %173 = "tpu.Concat"(%170#1, %170#2) {axis = 1 : si32, do_relu = false, multipliers = [1, 1], only_merge = true, relu_limit = -1.000000e+00 : f64, rshifts = [0, 0]} : (tensor<1x64x28x40x!quant.uniform<i8:f32, 0.14529467007874017>, 99968 : i64>, tensor<1x4x28x40x!quant.uniform<i8:f32, 0.14529467007874017>, 171648 : i64>) -> tensor<1x68x28x40x!quant.uniform<i8:f32, 0.14529467007874017>, 99968 : i64> loc(#loc418)
      %174 = "top.Weight"() {do_compress = true} : () -> tensor<1x64x1x9xi8, 1099512583984 : i64> loc(#loc419)
      %175 = "top.Weight"() {do_compress = true} : () -> tensor<1x64x9x64xsi8, 1099513108320 : i64> loc(#loc420)
      %176 = "tpu.Conv2D"(%172, %175, %174) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.086351832283464561>, 265728 : i64>, tensor<1x64x9x64xsi8, 1099513108320 : i64>, tensor<1x64x1x9xi8, 1099512583984 : i64>) -> tensor<1x64x14x20x!quant.uniform<i8:f32, 0.083914718897637797>, 71680 : i64> loc(#loc421)
      %177 = "tpu.Reshape"(%173) {flatten_start_dim = -1 : i64, shape = [1, 68, -1]} : (tensor<1x68x28x40x!quant.uniform<i8:f32, 0.14529467007874017>, 99968 : i64>) -> tensor<1x68x1120x!quant.uniform<i8:f32, 0.14529467007874017>, 99968 : i64> loc(#loc422)
      %178 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099513065632 : i64> loc(#loc423)
      %179 = "top.Weight"() : () -> tensor<1x64x1x9xi8, 1099512435440 : i64> loc(#loc424)
      %180 = "top.Weight"() : () -> tensor<1x64x9x64xsi8, 1099512398576 : i64> loc(#loc425)
      %181 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099514398144 : i64> loc(#loc426)
      %182 = "top.Weight"() : () -> tensor<1x128x1x9xi8, 1099512397424 : i64> loc(#loc427)
      %183 = "top.Weight"() : () -> tensor<1x128x1x192xsi8, 1099512998112 : i64> loc(#loc428)
      %184 = "tpu.Group"(%176, %171, %172) ({
        %266 = "tpu.Load"(%176) {do_bcast = false, ginfo = #tpu.lg<out_addr = 20480, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [14], w_idx = [0], w_slice = [20], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.083914718897637797>, 71680 : i64>) -> tensor<1x64x14x20x!quant.uniform<i8:f32, 0.083914718897637797>> loc(#loc430)
        %267 = "tpu.Load"(%178) {do_bcast = true, ginfo = #tpu.lg<out_addr = 24576, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 1, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099513065632 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc431)
        %268 = "tpu.Load"(%180) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [64], id = 2, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x9x64xsi8, 1099512398576 : i64>) -> tensor<1x64x9x64xsi8> loc(#loc432)
        %269 = "tpu.Load"(%179) {do_bcast = false, ginfo = #tpu.lg<out_addr = 4608, out_size = 72, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 3, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x9xi8, 1099512435440 : i64>) -> tensor<1x64x1x9xi8> loc(#loc433)
        %270 = "tpu.Lut"(%266, %267) {ginfo = #tpu.lg<out_addr = 16384, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [14], w_idx = [0], w_slice = [20], id = 4, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.083914718897637797>>, tensor<1x1x1x256xsi8>) -> tensor<1x64x14x20x!quant.uniform<i8:f32, 0.038318277165354325>> loc(#loc434)
        %271 = "tpu.Load"(%181) {do_bcast = true, ginfo = #tpu.lg<out_addr = 28672, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 5, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099514398144 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc435)
        %272 = "tpu.Conv2D"(%270, %268, %269) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 20480, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [14], w_idx = [0], w_slice = [20], id = 6, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.038318277165354325>>, tensor<1x64x9x64xsi8>, tensor<1x64x1x9xi8>) -> tensor<1x64x14x20x!quant.uniform<i8:f32, 0.091387576377952751>> loc(#loc436)
        %273 = "tpu.Load"(%171) {do_bcast = false, ginfo = #tpu.lg<out_addr = 8192, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [14], w_idx = [0], w_slice = [20], id = 7, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.086351832283464561>, 247808 : i64>) -> tensor<1x64x14x20x!quant.uniform<i8:f32, 0.086351832283464561>> loc(#loc437)
        %274 = "tpu.Load"(%172) {do_bcast = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [14], w_idx = [0], w_slice = [20], id = 8, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.086351832283464561>, 265728 : i64>) -> tensor<1x64x14x20x!quant.uniform<i8:f32, 0.086351832283464561>> loc(#loc438)
        %275 = "tpu.Lut"(%272, %271) {ginfo = #tpu.lg<out_addr = 24576, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [14], w_idx = [0], w_slice = [20], id = 9, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.091387576377952751>>, tensor<1x1x1x256xsi8>) -> tensor<1x64x14x20x!quant.uniform<i8:f32, 0.087254338582677157>> loc(#loc439)
        %276 = "tpu.Load"(%183) {do_bcast = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 3072, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [192], id = 10, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x192xsi8, 1099512998112 : i64>) -> tensor<1x128x1x192xsi8> loc(#loc440)
        %277 = "tpu.Load"(%182) {do_bcast = false, ginfo = #tpu.lg<out_addr = 28672, out_size = 144, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 11, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x9xi8, 1099512397424 : i64>) -> tensor<1x128x1x9xi8> loc(#loc441)
        %278 = "tpu.Concat"(%273, %274, %275) {axis = 1 : si32, do_relu = false, ginfo = #tpu.lg<out_addr = 0, out_size = 6912, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [192], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [14], w_idx = [0], w_slice = [20], id = 12, stage = 1, slice_idx = 0, group_type = 0>, multipliers = [1, 1, 64], only_merge = false, relu_limit = -1.000000e+00 : f64, rshifts = [0, 0, 6]} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.086351832283464561>>, tensor<1x64x14x20x!quant.uniform<i8:f32, 0.086351832283464561>>, tensor<1x64x14x20x!quant.uniform<i8:f32, 0.087254338582677157>>) -> tensor<1x192x14x20x!quant.uniform<i8:f32, 0.086351832283464561>> loc(#loc442)
        %279 = "tpu.Conv2D"(%278, %276, %277) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 8192, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [14], w_idx = [0], w_slice = [20], id = 13, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x192x14x20x!quant.uniform<i8:f32, 0.086351832283464561>>, tensor<1x128x1x192xsi8>, tensor<1x128x1x9xi8>) -> tensor<1x128x14x20x!quant.uniform<i8:f32, 0.092404636220472444>> loc(#loc429)
        %280 = "tpu.Store"(%279, %0) {ginfo = #tpu.lg<out_addr = 8192, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [14], w_idx = [0], w_slice = [20], id = 14, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.092404636220472444>>, none) -> tensor<1x128x14x20x!quant.uniform<i8:f32, 0.092404636220472444>, 35840 : i64> loc(#loc429)
        "tpu.Yield"(%280) : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.092404636220472444>, 35840 : i64>) -> () loc(#loc429)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 4, 2, 3, -2, 6, 5, 14, -3, 9, 7, 8, -4, 12, 10, 11, -5, 13, 0, 1], group_type = 0 : i64, hsecs = 1 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.083914718897637797>, 71680 : i64>, tensor<1x64x14x20x!quant.uniform<i8:f32, 0.086351832283464561>, 247808 : i64>, tensor<1x64x14x20x!quant.uniform<i8:f32, 0.086351832283464561>, 265728 : i64>) -> tensor<1x128x14x20x!quant.uniform<i8:f32, 0.092404636220472444>, 35840 : i64> loc(#loc429)
      %185 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099513146752 : i64> loc(#loc443)
      %186 = "top.Weight"() : () -> tensor<1x128x1x9xi8, 1099512830320 : i64> loc(#loc444)
      %187 = "top.Weight"() : () -> tensor<1x128x9x128xsi8, 1099513313168 : i64> loc(#loc445)
      %188:2 = "tpu.Group"(%184) ({
        %266 = "tpu.Load"(%184) {do_bcast = false, ginfo = #tpu.lg<out_addr = 25088, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [14], w_idx = [0], w_slice = [20], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.092404636220472444>, 35840 : i64>) -> tensor<1x128x14x20x!quant.uniform<i8:f32, 0.092404636220472444>> loc(#loc448)
        %267 = "tpu.Load"(%185) {do_bcast = true, ginfo = #tpu.lg<out_addr = 19712, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 1, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099513146752 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc449)
        %268 = "tpu.Load"(%187) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 18432, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [128], id = 2, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x9x128xsi8, 1099513313168 : i64>) -> tensor<1x128x9x128xsi8> loc(#loc450)
        %269 = "tpu.Load"(%186) {do_bcast = false, ginfo = #tpu.lg<out_addr = 19968, out_size = 144, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 3, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x9xi8, 1099512830320 : i64>) -> tensor<1x128x1x9xi8> loc(#loc451)
        %270 = "tpu.Lut"(%266, %267) {ginfo = #tpu.lg<out_addr = 20480, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [14], w_idx = [0], w_slice = [20], id = 4, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.092404636220472444>>, tensor<1x1x1x256xsi8>) -> tensor<1x128x14x20x!quant.uniform<i8:f32, 0.054264250393700787>> loc(#loc446)
        %271 = "tpu.Store"(%270, %0) {ginfo = #tpu.lg<out_addr = 20480, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [14], w_idx = [0], w_slice = [20], id = 5, stage = 1, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.054264250393700787>>, none) -> tensor<1x128x14x20x!quant.uniform<i8:f32, 0.054264250393700787>, 0 : i64> loc(#loc446)
        %272 = "tpu.Conv2D"(%270, %268, %269) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 18432, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 6, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [2, 2], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.054264250393700787>>, tensor<1x128x9x128xsi8>, tensor<1x128x1x9xi8>) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.067744059842519688>> loc(#loc447)
        %273 = "tpu.Store"(%272, %0) {ginfo = #tpu.lg<out_addr = 18432, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 7, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.067744059842519688>>, none) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.067744059842519688>, 80640 : i64> loc(#loc447)
        "tpu.Yield"(%271, %273) : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.054264250393700787>, 0 : i64>, tensor<1x128x7x10x!quant.uniform<i8:f32, 0.067744059842519688>, 80640 : i64>) -> () loc(#loc616)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 4, 2, 3, 7, -2, 6, 5, 0, 1], group_type = 0 : i64, hsecs = 1 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.092404636220472444>, 35840 : i64>) -> (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.054264250393700787>, 0 : i64>, tensor<1x128x7x10x!quant.uniform<i8:f32, 0.067744059842519688>, 80640 : i64>) loc(#loc616)
      %189 = "top.Weight"() : () -> tensor<1x64x1x9xi8, 1099512396272 : i64> loc(#loc452)
      %190 = "top.Weight"() : () -> tensor<1x64x9x128xsi8, 1099512746608 : i64> loc(#loc453)
      %191 = "top.Weight"() : () -> tensor<1x64x1x9xi8, 1099512396848 : i64> loc(#loc454)
      %192 = "top.Weight"() : () -> tensor<1x64x9x128xsi8, 1099512322544 : i64> loc(#loc455)
      %193 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099513875088 : i64> loc(#loc456)
      %194:3 = "tpu.Group"(%188#0, %188#1) ({
        %266 = "tpu.Load"(%190) {do_bcast = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 9216, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [128], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x9x128xsi8, 1099512746608 : i64>) -> tensor<1x64x9x128xsi8> loc(#loc460)
        %267 = "tpu.Load"(%188#0) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 6], h_slice = [8, 8], w_idx = [0], w_slice = [20], id = 1, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.054264250393700787>, 0 : i64>) -> tensor<1x128x14x20x!quant.uniform<i8:f32, 0.054264250393700787>> loc(#loc461)
        %268 = "tpu.Load"(%189) {do_bcast = false, ginfo = #tpu.lg<out_addr = 11216, out_size = 72, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 2, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x9xi8, 1099512396272 : i64>) -> tensor<1x64x1x9xi8> loc(#loc462)
        %269 = "tpu.Load"(%192) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 9216, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [128], id = 3, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x9x128xsi8, 1099512322544 : i64>) -> tensor<1x64x9x128xsi8> loc(#loc463)
        %270 = "tpu.Load"(%191) {do_bcast = false, ginfo = #tpu.lg<out_addr = 11136, out_size = 72, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 4, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x9xi8, 1099512396848 : i64>) -> tensor<1x64x1x9xi8> loc(#loc464)
        %271 = "tpu.Conv2D"(%267, %266, %268) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 28672, out_size = 1152, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 7], h_slice = [7, 7], w_idx = [0], w_slice = [20], id = 5, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.054264250393700787>>, tensor<1x64x9x128xsi8>, tensor<1x64x1x9xi8>) -> tensor<1x64x14x20x!quant.uniform<i8:f32, 0.16238281653543307>> loc(#loc457)
        %272 = "tpu.Load"(%188#1) {do_bcast = false, ginfo = #tpu.lg<out_addr = 21504, out_size = 768, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 4], h_slice = [4, 3], w_idx = [0], w_slice = [10], id = 6, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.067744059842519688>, 80640 : i64>) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.067744059842519688>> loc(#loc465)
        %273 = "tpu.Load"(%193) {do_bcast = true, ginfo = #tpu.lg<out_addr = 29824, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 7, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099513875088 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc466)
        %274 = "tpu.Store"(%271, %0) {ginfo = #tpu.lg<out_addr = 28672, out_size = 1152, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 7], h_slice = [7, 7], w_idx = [0], w_slice = [20], id = 8, stage = 1, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.16238281653543307>>, none) -> tensor<1x64x14x20x!quant.uniform<i8:f32, 0.16238281653543307>, 35840 : i64> loc(#loc457)
        %275 = "tpu.Conv2D"(%267, %269, %270) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 9984, out_size = 1152, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 7], h_slice = [7, 7], w_idx = [0], w_slice = [20], id = 9, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.054264250393700787>>, tensor<1x64x9x128xsi8>, tensor<1x64x1x9xi8>) -> tensor<1x64x14x20x!quant.uniform<i8:f32, 0.078670426771653548>> loc(#loc458)
        %276 = "tpu.Store"(%275, %0) {ginfo = #tpu.lg<out_addr = 9984, out_size = 1152, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 7], h_slice = [7, 7], w_idx = [0], w_slice = [20], id = 10, stage = 1, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.078670426771653548>>, none) -> tensor<1x64x14x20x!quant.uniform<i8:f32, 0.078670426771653548>, 53760 : i64> loc(#loc458)
        %277 = "tpu.Lut"(%272, %273) {ginfo = #tpu.lg<out_addr = 9216, out_size = 768, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 4], h_slice = [4, 3], w_idx = [0], w_slice = [10], id = 11, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.067744059842519688>>, tensor<1x1x1x256xsi8>) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.094307200787401568>> loc(#loc459)
        %278 = "tpu.Store"(%277, %0) {ginfo = #tpu.lg<out_addr = 9216, out_size = 768, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 4], h_slice = [4, 3], w_idx = [0], w_slice = [10], id = 12, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.094307200787401568>>, none) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.094307200787401568>, 71680 : i64> loc(#loc459)
        "tpu.Yield"(%274, %276, %278) : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.16238281653543307>, 35840 : i64>, tensor<1x64x14x20x!quant.uniform<i8:f32, 0.078670426771653548>, 53760 : i64>, tensor<1x128x7x10x!quant.uniform<i8:f32, 0.094307200787401568>, 71680 : i64>) -> () loc(#loc617)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 5, 3, 4, 12, -2, 9, 6, 7, 8, 0, -3, 11, 10, 1, 2], group_type = 0 : i64, hsecs = 2 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x128x14x20x!quant.uniform<i8:f32, 0.054264250393700787>, 0 : i64>, tensor<1x128x7x10x!quant.uniform<i8:f32, 0.067744059842519688>, 80640 : i64>) -> (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.16238281653543307>, 35840 : i64>, tensor<1x64x14x20x!quant.uniform<i8:f32, 0.078670426771653548>, 53760 : i64>, tensor<1x128x7x10x!quant.uniform<i8:f32, 0.094307200787401568>, 71680 : i64>) loc(#loc617)
      %195 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099512322288 : i64> loc(#loc467)
      %196 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099514208768 : i64> loc(#loc468)
      %197 = "top.Weight"() : () -> tensor<1x64x1x9xi8, 1099512321712 : i64> loc(#loc469)
      %198 = "top.Weight"() : () -> tensor<1x64x9x64xsi8, 1099514424640 : i64> loc(#loc470)
      %199 = "top.Weight"() : () -> tensor<1x64x1x9xi8, 1099512321136 : i64> loc(#loc471)
      %200 = "top.Weight"() : () -> tensor<1x64x9x64xsi8, 1099513186176 : i64> loc(#loc472)
      %201 = "top.Weight"() : () -> tensor<1x256x1x9xi8, 1099512318832 : i64> loc(#loc473)
      %202 = "top.Weight"() : () -> tensor<1x256x1x384xsi8, 1099512220528 : i64> loc(#loc474)
      %203 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099512220272 : i64> loc(#loc475)
      %204 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099512219440 : i64> loc(#loc476)
      %205 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099513105568 : i64> loc(#loc477)
      %206 = "top.Weight"() : () -> tensor<1x64x1x9xi8, 1099512218864 : i64> loc(#loc478)
      %207 = "top.Weight"() : () -> tensor<1x64x1x64xsi8, 1099512214768 : i64> loc(#loc479)
      %208 = "top.Weight"() : () -> tensor<1x4x1x9xi8, 1099512214704 : i64> loc(#loc480)
      %209 = "top.Weight"() : () -> tensor<1x4x1x64xsi8, 1099512214448 : i64> loc(#loc481)
      %210:3 = "tpu.Group"(%194#0, %194#1, %194#2, %105) ({
        %266 = "tpu.Load"(%194#0) {do_bcast = false, ginfo = #tpu.lg<out_addr = 4608, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [14], w_idx = [0], w_slice = [20], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.16238281653543307>, 35840 : i64>) -> tensor<1x64x14x20x!quant.uniform<i8:f32, 0.16238281653543307>> loc(#loc485)
        %267 = "tpu.Load"(%195) {do_bcast = true, ginfo = #tpu.lg<out_addr = 20480, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 1, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099512322288 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc486)
        %268 = "tpu.Load"(%194#1) {do_bcast = false, ginfo = #tpu.lg<out_addr = 8192, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [14], w_idx = [0], w_slice = [20], id = 2, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.078670426771653548>, 53760 : i64>) -> tensor<1x64x14x20x!quant.uniform<i8:f32, 0.078670426771653548>> loc(#loc487)
        %269 = "tpu.Load"(%196) {do_bcast = true, ginfo = #tpu.lg<out_addr = 12288, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 3, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099514208768 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc488)
        %270 = "tpu.Lut"(%266, %267) {ginfo = #tpu.lg<out_addr = 0, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [14], w_idx = [0], w_slice = [20], id = 4, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.16238281653543307>>, tensor<1x1x1x256xsi8>) -> tensor<1x64x14x20x!quant.uniform<i8:f32, 0.0689938125984252>> loc(#loc489)
        %271 = "tpu.Load"(%194#2) {do_bcast = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 5, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.094307200787401568>, 71680 : i64>) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.094307200787401568>> loc(#loc490)
        %272 = "tpu.Load"(%105) {do_bcast = false, ginfo = #tpu.lg<out_addr = 2304, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 6, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x7x10x!quant.uniform<i8:f32, 0.018032498425196851>, 409088 : i64>) -> tensor<1x256x7x10x!quant.uniform<i8:f32, 0.018032498425196851>> loc(#loc263)
        %273 = "tpu.Lut"(%268, %269) {ginfo = #tpu.lg<out_addr = 24320, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [14], w_idx = [0], w_slice = [20], id = 7, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.078670426771653548>>, tensor<1x1x1x256xsi8>) -> tensor<1x64x14x20x!quant.uniform<i8:f32, 0.06674247086614174>> loc(#loc491)
        %274 = "tpu.Load"(%198) {do_bcast = false, ginfo = #tpu.lg<out_addr = 8192, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [64], id = 8, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x9x64xsi8, 1099514424640 : i64>) -> tensor<1x64x9x64xsi8> loc(#loc492)
        %275 = "tpu.Load"(%197) {do_bcast = false, ginfo = #tpu.lg<out_addr = 4864, out_size = 72, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 9, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x9xi8, 1099512321712 : i64>) -> tensor<1x64x1x9xi8> loc(#loc493)
        %276 = "tpu.Concat"(%271, %272) {axis = 1 : si32, do_relu = false, ginfo = #tpu.lg<out_addr = 20480, out_size = 3840, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [384], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 10, stage = 1, slice_idx = 0, group_type = 0>, multipliers = [114, 87], only_merge = false, relu_limit = -1.000000e+00 : f64, rshifts = [6, 8]} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.094307200787401568>>, tensor<1x256x7x10x!quant.uniform<i8:f32, 0.018032498425196851>>) -> tensor<1x384x7x10x!quant.uniform<i8:f32, 0.052567028346456691>> loc(#loc494)
        %277 = "tpu.Load"(%200) {do_bcast = false, ginfo = #tpu.lg<out_addr = 26624, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [64], id = 11, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x9x64xsi8, 1099513186176 : i64>) -> tensor<1x64x9x64xsi8> loc(#loc495)
        %278 = "tpu.Load"(%199) {do_bcast = false, ginfo = #tpu.lg<out_addr = 12800, out_size = 72, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 12, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x9xi8, 1099512321136 : i64>) -> tensor<1x64x1x9xi8> loc(#loc496)
        %279 = "tpu.Conv2D"(%270, %274, %275) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 17152, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [14], w_idx = [0], w_slice = [20], id = 13, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.0689938125984252>>, tensor<1x64x9x64xsi8>, tensor<1x64x1x9xi8>) -> tensor<1x64x14x20x!quant.uniform<i8:f32, 0.14142692755905512>> loc(#loc497)
        %280 = "tpu.Load"(%202) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 12288, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [384], id = 14, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x1x384xsi8, 1099512220528 : i64>) -> tensor<1x256x1x384xsi8> loc(#loc498)
        %281 = "tpu.Load"(%201) {do_bcast = false, ginfo = #tpu.lg<out_addr = 19456, out_size = 288, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 15, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x1x9xi8, 1099512318832 : i64>) -> tensor<1x256x1x9xi8> loc(#loc499)
        %282 = "tpu.Conv2D"(%273, %277, %278) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 14848, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [14], w_idx = [0], w_slice = [20], id = 16, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.06674247086614174>>, tensor<1x64x9x64xsi8>, tensor<1x64x1x9xi8>) -> tensor<1x64x14x20x!quant.uniform<i8:f32, 0.11012609763779528>> loc(#loc500)
        %283 = "tpu.Load"(%203) {do_bcast = true, ginfo = #tpu.lg<out_addr = 24576, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 17, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099512220272 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc501)
        %284 = "tpu.Conv2D"(%276, %280, %281) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 18, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x384x7x10x!quant.uniform<i8:f32, 0.052567028346456691>>, tensor<1x256x1x384xsi8>, tensor<1x256x1x9xi8>) -> tensor<1x256x7x10x!quant.uniform<i8:f32, 0.11355767322834645>> loc(#loc502)
        %285 = "tpu.Load"(%204) {do_bcast = true, ginfo = #tpu.lg<out_addr = 8192, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 19, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099512219440 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc503)
        %286 = "tpu.Lut"(%279, %283) {ginfo = #tpu.lg<out_addr = 0, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [14], w_idx = [0], w_slice = [20], id = 20, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.14142692755905512>>, tensor<1x1x1x256xsi8>) -> tensor<1x64x14x20x!quant.uniform<i8:f32, 0.10723368031496062>> loc(#loc504)
        %287 = "tpu.Load"(%205) {do_bcast = true, ginfo = #tpu.lg<out_addr = 20480, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 21, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099513105568 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc505)
        %288 = "tpu.Lut"(%282, %285) {ginfo = #tpu.lg<out_addr = 2304, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [14], w_idx = [0], w_slice = [20], id = 22, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.11012609763779528>>, tensor<1x1x1x256xsi8>) -> tensor<1x64x14x20x!quant.uniform<i8:f32, 0.089135332283464568>> loc(#loc506)
        %289 = "tpu.Load"(%207) {do_bcast = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 512, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [64], id = 23, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x64xsi8, 1099512214768 : i64>) -> tensor<1x64x1x64xsi8> loc(#loc507)
        %290 = "tpu.Load"(%206) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 72, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 24, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x9xi8, 1099512218864 : i64>) -> tensor<1x64x1x9xi8> loc(#loc508)
        %291 = "tpu.Lut"(%284, %287) {ginfo = #tpu.lg<out_addr = 6912, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 25, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x256x7x10x!quant.uniform<i8:f32, 0.11355767322834645>>, tensor<1x1x1x256xsi8>) -> tensor<1x256x7x10x!quant.uniform<i8:f32, 0.068181061417322844>> loc(#loc482)
        %292 = "tpu.Load"(%209) {do_bcast = false, ginfo = #tpu.lg<out_addr = 9472, out_size = 64, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [4], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [64], id = 26, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x4x1x64xsi8, 1099512214448 : i64>) -> tensor<1x4x1x64xsi8> loc(#loc509)
        %293 = "tpu.Load"(%208) {do_bcast = false, ginfo = #tpu.lg<out_addr = 20736, out_size = 9, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [4], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 27, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x4x1x9xi8, 1099512214704 : i64>) -> tensor<1x4x1x9xi8> loc(#loc510)
        %294 = "tpu.Store"(%291, %0) {ginfo = #tpu.lg<out_addr = 6912, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 28, stage = 1, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x256x7x10x!quant.uniform<i8:f32, 0.068181061417322844>>, none) -> tensor<1x256x7x10x!quant.uniform<i8:f32, 0.068181061417322844>, 0 : i64> loc(#loc482)
        %295 = "tpu.Conv2D"(%286, %289, %290) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [14], w_idx = [0], w_slice = [20], id = 29, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.10723368031496062>>, tensor<1x64x1x64xsi8>, tensor<1x64x1x9xi8>) -> tensor<1x64x14x20x!quant.uniform<i8:f32, 0.14529467007874017>> loc(#loc483)
        %296 = "tpu.Store"(%295, %0) {ginfo = #tpu.lg<out_addr = 12288, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [14], w_idx = [0], w_slice = [20], id = 30, stage = 1, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.14529467007874017>>, none) -> tensor<1x64x14x20x!quant.uniform<i8:f32, 0.14529467007874017>, 176128 : i64> loc(#loc483)
        %297 = "tpu.Conv2D"(%288, %292, %293) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 288, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [4], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [14], w_idx = [0], w_slice = [20], id = 31, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.089135332283464568>>, tensor<1x4x1x64xsi8>, tensor<1x4x1x9xi8>) -> tensor<1x4x14x20x!quant.uniform<i8:f32, 0.14529467007874017>> loc(#loc484)
        %298 = "tpu.Store"(%297, %0) {ginfo = #tpu.lg<out_addr = 16384, out_size = 288, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [4], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [14], w_idx = [0], w_slice = [20], id = 32, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x4x14x20x!quant.uniform<i8:f32, 0.14529467007874017>>, none) -> tensor<1x4x14x20x!quant.uniform<i8:f32, 0.14529467007874017>, 194048 : i64> loc(#loc484)
        "tpu.Yield"(%294, %296, %298) : (tensor<1x256x7x10x!quant.uniform<i8:f32, 0.068181061417322844>, 0 : i64>, tensor<1x64x14x20x!quant.uniform<i8:f32, 0.14529467007874017>, 176128 : i64>, tensor<1x4x14x20x!quant.uniform<i8:f32, 0.14529467007874017>, 194048 : i64>) -> () loc(#loc618)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 4, 2, 3, 32, -2, 7, 5, 6, -3, 10, 8, 9, -4, 13, 11, 12, -5, 16, 14, 15, -6, 18, 17, -7, 20, 19, -8, 22, 21, -9, 25, 23, 24, -10, 29, 26, 27, 28, 0, 1, -11, 31, 30], group_type = 0 : i64, hsecs = 1 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.16238281653543307>, 35840 : i64>, tensor<1x64x14x20x!quant.uniform<i8:f32, 0.078670426771653548>, 53760 : i64>, tensor<1x128x7x10x!quant.uniform<i8:f32, 0.094307200787401568>, 71680 : i64>, tensor<1x256x7x10x!quant.uniform<i8:f32, 0.018032498425196851>, 409088 : i64>) -> (tensor<1x256x7x10x!quant.uniform<i8:f32, 0.068181061417322844>, 0 : i64>, tensor<1x64x14x20x!quant.uniform<i8:f32, 0.14529467007874017>, 176128 : i64>, tensor<1x4x14x20x!quant.uniform<i8:f32, 0.14529467007874017>, 194048 : i64>) loc(#loc618)
      %211 = "tpu.Slice"(%210#0, %0, %0, %0, %0) {axes = [], ends = [9223372036854775807, 128, 9223372036854775807, 9223372036854775807], hasparamConvert_axes = [1], offset = [0, 0, 0, 0], steps = [1, 1, 1, 1]} : (tensor<1x256x7x10x!quant.uniform<i8:f32, 0.068181061417322844>, 0 : i64>, none, none, none, none) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.068181061417322844>, 0 : i64> loc(#loc511)
      %212 = "tpu.Slice"(%210#0, %0, %0, %0, %0) {axes = [], ends = [9223372036854775807, 256, 9223372036854775807, 9223372036854775807], hasparamConvert_axes = [1], offset = [0, 128, 0, 0], steps = [1, 1, 1, 1]} : (tensor<1x256x7x10x!quant.uniform<i8:f32, 0.068181061417322844>, 0 : i64>, none, none, none, none) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.068181061417322844>, 8960 : i64> loc(#loc512)
      %213 = "tpu.Concat"(%210#1, %210#2) {axis = 1 : si32, do_relu = false, multipliers = [1, 1], only_merge = true, relu_limit = -1.000000e+00 : f64, rshifts = [0, 0]} : (tensor<1x64x14x20x!quant.uniform<i8:f32, 0.14529467007874017>, 176128 : i64>, tensor<1x4x14x20x!quant.uniform<i8:f32, 0.14529467007874017>, 194048 : i64>) -> tensor<1x68x14x20x!quant.uniform<i8:f32, 0.14529467007874017>, 176128 : i64> loc(#loc513)
      %214 = "top.Weight"() {do_compress = true} : () -> tensor<1x128x1x9xi8, 1099512213040 : i64> loc(#loc514)
      %215 = "top.Weight"() {do_compress = true} : () -> tensor<1x128x9x128xsi8, 1099512436528 : i64> loc(#loc515)
      %216 = "tpu.Conv2D"(%212, %215, %214) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.068181061417322844>, 8960 : i64>, tensor<1x128x9x128xsi8, 1099512436528 : i64>, tensor<1x128x1x9xi8, 1099512213040 : i64>) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.089609363779527551>, 17920 : i64> loc(#loc516)
      %217 = "tpu.Reshape"(%213) {flatten_start_dim = -1 : i64, shape = [1, 68, -1]} : (tensor<1x68x14x20x!quant.uniform<i8:f32, 0.14529467007874017>, 176128 : i64>) -> tensor<1x68x280x!quant.uniform<i8:f32, 0.14529467007874017>, 176128 : i64> loc(#loc517)
      %218 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099513472912 : i64> loc(#loc518)
      %219 = "top.Weight"() : () -> tensor<1x128x1x9xi8, 1099512211888 : i64> loc(#loc519)
      %220 = "top.Weight"() : () -> tensor<1x128x9x128xsi8, 1099512064432 : i64> loc(#loc520)
      %221 = "tpu.Group"(%216) ({
        %266 = "tpu.Load"(%216) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.089609363779527551>, 17920 : i64>) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.089609363779527551>> loc(#loc522)
        %267 = "tpu.Load"(%218) {do_bcast = true, ginfo = #tpu.lg<out_addr = 18432, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 1, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099513472912 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc523)
        %268 = "tpu.Load"(%220) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 18432, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [128], id = 2, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x9x128xsi8, 1099512064432 : i64>) -> tensor<1x128x9x128xsi8> loc(#loc524)
        %269 = "tpu.Load"(%219) {do_bcast = false, ginfo = #tpu.lg<out_addr = 18688, out_size = 144, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 3, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x9xi8, 1099512211888 : i64>) -> tensor<1x128x1x9xi8> loc(#loc525)
        %270 = "tpu.Lut"(%266, %267) {ginfo = #tpu.lg<out_addr = 20480, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 4, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.089609363779527551>>, tensor<1x1x1x256xsi8>) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.030805681102362203>> loc(#loc526)
        %271 = "tpu.Conv2D"(%270, %268, %269) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 28672, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 5, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.030805681102362203>>, tensor<1x128x9x128xsi8>, tensor<1x128x1x9xi8>) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.070711894488188987>> loc(#loc521)
        %272 = "tpu.Store"(%271, %0) {ginfo = #tpu.lg<out_addr = 28672, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 6, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.070711894488188987>>, none) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.070711894488188987>, 35840 : i64> loc(#loc521)
        "tpu.Yield"(%272) : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.070711894488188987>, 35840 : i64>) -> () loc(#loc521)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 4, 2, 3, 6, -2, 5, 0, 1], group_type = 0 : i64, hsecs = 1 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.089609363779527551>, 17920 : i64>) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.070711894488188987>, 35840 : i64> loc(#loc521)
      %222 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099512628592 : i64> loc(#loc527)
      %223 = "top.Weight"() : () -> tensor<1x256x1x9xi8, 1099513183872 : i64> loc(#loc528)
      %224 = "top.Weight"() : () -> tensor<1x256x1x384xsi8, 1099511966128 : i64> loc(#loc529)
      %225 = "tpu.Group"(%221, %211, %212) ({
        %266 = "tpu.Load"(%221) {do_bcast = false, ginfo = #tpu.lg<out_addr = 28672, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.070711894488188987>, 35840 : i64>) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.070711894488188987>> loc(#loc531)
        %267 = "tpu.Load"(%222) {do_bcast = true, ginfo = #tpu.lg<out_addr = 16128, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 1, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099512628592 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc532)
        %268 = "tpu.Load"(%211) {do_bcast = false, ginfo = #tpu.lg<out_addr = 18944, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 2, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.068181061417322844>, 0 : i64>) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.068181061417322844>> loc(#loc533)
        %269 = "tpu.Load"(%212) {do_bcast = false, ginfo = #tpu.lg<out_addr = 20480, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 3, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.068181061417322844>, 8960 : i64>) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.068181061417322844>> loc(#loc534)
        %270 = "tpu.Lut"(%266, %267) {ginfo = #tpu.lg<out_addr = 24576, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 4, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.070711894488188987>>, tensor<1x1x1x256xsi8>) -> tensor<1x128x7x10x!quant.uniform<i8:f32, 0.066060206299212595>> loc(#loc535)
        %271 = "tpu.Load"(%224) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 12288, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [384], id = 5, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x1x384xsi8, 1099511966128 : i64>) -> tensor<1x256x1x384xsi8> loc(#loc536)
        %272 = "tpu.Load"(%223) {do_bcast = false, ginfo = #tpu.lg<out_addr = 21760, out_size = 288, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 6, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x1x9xi8, 1099513183872 : i64>) -> tensor<1x256x1x9xi8> loc(#loc537)
        %273 = "tpu.Concat"(%268, %269, %270) {axis = 1 : si32, do_relu = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 3840, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [384], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 7, stage = 1, slice_idx = 0, group_type = 0>, multipliers = [1, 1, 124], only_merge = false, relu_limit = -1.000000e+00 : f64, rshifts = [0, 0, 7]} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.068181061417322844>>, tensor<1x128x7x10x!quant.uniform<i8:f32, 0.068181061417322844>>, tensor<1x128x7x10x!quant.uniform<i8:f32, 0.066060206299212595>>) -> tensor<1x384x7x10x!quant.uniform<i8:f32, 0.068181061417322844>> loc(#loc538)
        %274 = "tpu.Conv2D"(%273, %271, %272) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 8, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x384x7x10x!quant.uniform<i8:f32, 0.068181061417322844>>, tensor<1x256x1x384xsi8>, tensor<1x256x1x9xi8>) -> tensor<1x256x7x10x!quant.uniform<i8:f32, 0.13541340236220473>> loc(#loc530)
        %275 = "tpu.Store"(%274, %0) {ginfo = #tpu.lg<out_addr = 16384, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 9, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x256x7x10x!quant.uniform<i8:f32, 0.13541340236220473>>, none) -> tensor<1x256x7x10x!quant.uniform<i8:f32, 0.13541340236220473>, 17920 : i64> loc(#loc530)
        "tpu.Yield"(%275) : (tensor<1x256x7x10x!quant.uniform<i8:f32, 0.13541340236220473>, 17920 : i64>) -> () loc(#loc530)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 4, 2, 3, 9, -2, 7, 5, 6, -3, 8, 0, 1], group_type = 0 : i64, hsecs = 1 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x128x7x10x!quant.uniform<i8:f32, 0.070711894488188987>, 35840 : i64>, tensor<1x128x7x10x!quant.uniform<i8:f32, 0.068181061417322844>, 0 : i64>, tensor<1x128x7x10x!quant.uniform<i8:f32, 0.068181061417322844>, 8960 : i64>) -> tensor<1x256x7x10x!quant.uniform<i8:f32, 0.13541340236220473>, 17920 : i64> loc(#loc530)
      %226 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099513108064 : i64> loc(#loc539)
      %227 = "top.Weight"() : () -> tensor<1x64x1x9xi8, 1099514060736 : i64> loc(#loc540)
      %228 = "top.Weight"() : () -> tensor<1x64x9x256xsi8, 1099511671216 : i64> loc(#loc541)
      %229:2 = "tpu.Group"(%225) ({
        %266 = "tpu.Load"(%225) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x7x10x!quant.uniform<i8:f32, 0.13541340236220473>, 17920 : i64>) -> tensor<1x256x7x10x!quant.uniform<i8:f32, 0.13541340236220473>> loc(#loc544)
        %267 = "tpu.Load"(%226) {do_bcast = true, ginfo = #tpu.lg<out_addr = 18432, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 1, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099513108064 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc545)
        %268 = "tpu.Load"(%228) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 18432, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [256], id = 2, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x9x256xsi8, 1099511671216 : i64>) -> tensor<1x64x9x256xsi8> loc(#loc546)
        %269 = "tpu.Load"(%227) {do_bcast = false, ginfo = #tpu.lg<out_addr = 18688, out_size = 72, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 3, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x9xi8, 1099514060736 : i64>) -> tensor<1x64x1x9xi8> loc(#loc547)
        %270 = "tpu.Lut"(%266, %267) {ginfo = #tpu.lg<out_addr = 20480, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 4, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x256x7x10x!quant.uniform<i8:f32, 0.13541340236220473>>, tensor<1x1x1x256xsi8>) -> tensor<1x256x7x10x!quant.uniform<i8:f32, 0.045780149606299218>> loc(#loc542)
        %271 = "tpu.Store"(%270, %0) {ginfo = #tpu.lg<out_addr = 20480, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 5, stage = 1, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x256x7x10x!quant.uniform<i8:f32, 0.045780149606299218>>, none) -> tensor<1x256x7x10x!quant.uniform<i8:f32, 0.045780149606299218>, 0 : i64> loc(#loc542)
        %272 = "tpu.Conv2D"(%270, %268, %269) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 28672, out_size = 640, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 6, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x256x7x10x!quant.uniform<i8:f32, 0.045780149606299218>>, tensor<1x64x9x256xsi8>, tensor<1x64x1x9xi8>) -> tensor<1x64x7x10x!quant.uniform<i8:f32, 0.098445216535433075>> loc(#loc543)
        %273 = "tpu.Store"(%272, %0) {ginfo = #tpu.lg<out_addr = 28672, out_size = 640, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 7, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x64x7x10x!quant.uniform<i8:f32, 0.098445216535433075>>, none) -> tensor<1x64x7x10x!quant.uniform<i8:f32, 0.098445216535433075>, 35840 : i64> loc(#loc543)
        "tpu.Yield"(%271, %273) : (tensor<1x256x7x10x!quant.uniform<i8:f32, 0.045780149606299218>, 0 : i64>, tensor<1x64x7x10x!quant.uniform<i8:f32, 0.098445216535433075>, 35840 : i64>) -> () loc(#loc619)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 4, 2, 3, 7, -2, 6, 5, 0, 1], group_type = 0 : i64, hsecs = 1 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x256x7x10x!quant.uniform<i8:f32, 0.13541340236220473>, 17920 : i64>) -> (tensor<1x256x7x10x!quant.uniform<i8:f32, 0.045780149606299218>, 0 : i64>, tensor<1x64x7x10x!quant.uniform<i8:f32, 0.098445216535433075>, 35840 : i64>) loc(#loc619)
      %230 = "top.Weight"() : () -> tensor<1x64x1x9xi8, 1099512219696 : i64> loc(#loc548)
      %231 = "top.Weight"() : () -> tensor<1x64x9x256xsi8, 1099511818672 : i64> loc(#loc549)
      %232 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099512997856 : i64> loc(#loc550)
      %233 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099512849088 : i64> loc(#loc551)
      %234 = "top.Weight"() : () -> tensor<1x64x1x9xi8, 1099514399488 : i64> loc(#loc552)
      %235 = "top.Weight"() : () -> tensor<1x64x9x64xsi8, 1099511634352 : i64> loc(#loc553)
      %236 = "top.Weight"() : () -> tensor<1x64x1x9xi8, 1099511633776 : i64> loc(#loc554)
      %237 = "top.Weight"() : () -> tensor<1x64x9x64xsi8, 1099514023872 : i64> loc(#loc555)
      %238 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099511633520 : i64> loc(#loc556)
      %239 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099511633264 : i64> loc(#loc557)
      %240 = "top.Weight"() : () -> tensor<1x64x1x9xi8, 1099511632688 : i64> loc(#loc558)
      %241 = "top.Weight"() : () -> tensor<1x64x1x64xsi8, 1099511628592 : i64> loc(#loc559)
      %242 = "top.Weight"() : () -> tensor<1x4x1x9xi8, 1099511628544 : i64> loc(#loc560)
      %243 = "top.Weight"() : () -> tensor<1x4x1x64xsi8, 1099511628288 : i64> loc(#loc561)
      %244:2 = "tpu.Group"(%229#0, %229#1) ({
        %266 = "tpu.Load"(%231) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 18432, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [256], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x9x256xsi8, 1099511818672 : i64>) -> tensor<1x64x9x256xsi8> loc(#loc564)
        %267 = "tpu.Load"(%229#0) {do_bcast = false, ginfo = #tpu.lg<out_addr = 28672, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 1, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x7x10x!quant.uniform<i8:f32, 0.045780149606299218>, 0 : i64>) -> tensor<1x256x7x10x!quant.uniform<i8:f32, 0.045780149606299218>> loc(#loc565)
        %268 = "tpu.Load"(%230) {do_bcast = false, ginfo = #tpu.lg<out_addr = 19584, out_size = 72, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 2, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x9xi8, 1099512219696 : i64>) -> tensor<1x64x1x9xi8> loc(#loc566)
        %269 = "tpu.Load"(%229#1) {do_bcast = false, ginfo = #tpu.lg<out_addr = 25088, out_size = 640, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 3, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x7x10x!quant.uniform<i8:f32, 0.098445216535433075>, 35840 : i64>) -> tensor<1x64x7x10x!quant.uniform<i8:f32, 0.098445216535433075>> loc(#loc567)
        %270 = "tpu.Load"(%232) {do_bcast = true, ginfo = #tpu.lg<out_addr = 19072, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 4, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099512997856 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc568)
        %271 = "tpu.Load"(%235) {do_bcast = false, ginfo = #tpu.lg<out_addr = 20480, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [64], id = 5, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x9x64xsi8, 1099511634352 : i64>) -> tensor<1x64x9x64xsi8> loc(#loc569)
        %272 = "tpu.Conv2D"(%267, %266, %268) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 18432, out_size = 640, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 6, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x256x7x10x!quant.uniform<i8:f32, 0.045780149606299218>>, tensor<1x64x9x256xsi8>, tensor<1x64x1x9xi8>) -> tensor<1x64x7x10x!quant.uniform<i8:f32, 0.034995533070866142>> loc(#loc570)
        %273 = "tpu.Load"(%233) {do_bcast = true, ginfo = #tpu.lg<out_addr = 4096, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 7, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099512849088 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc571)
        %274 = "tpu.Lut"(%269, %270) {ginfo = #tpu.lg<out_addr = 0, out_size = 640, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 8, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x64x7x10x!quant.uniform<i8:f32, 0.098445216535433075>>, tensor<1x1x1x256xsi8>) -> tensor<1x64x7x10x!quant.uniform<i8:f32, 0.056820584251968502>> loc(#loc572)
        %275 = "tpu.Load"(%234) {do_bcast = false, ginfo = #tpu.lg<out_addr = 8192, out_size = 72, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 9, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x9xi8, 1099514399488 : i64>) -> tensor<1x64x1x9xi8> loc(#loc573)
        %276 = "tpu.Lut"(%272, %273) {ginfo = #tpu.lg<out_addr = 19072, out_size = 640, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 10, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x64x7x10x!quant.uniform<i8:f32, 0.034995533070866142>>, tensor<1x1x1x256xsi8>) -> tensor<1x64x7x10x!quant.uniform<i8:f32, 0.031334092125984252>> loc(#loc574)
        %277 = "tpu.Load"(%237) {do_bcast = false, ginfo = #tpu.lg<out_addr = 25088, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [64], id = 11, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x9x64xsi8, 1099514023872 : i64>) -> tensor<1x64x9x64xsi8> loc(#loc575)
        %278 = "tpu.Load"(%236) {do_bcast = false, ginfo = #tpu.lg<out_addr = 19968, out_size = 72, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 12, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x9xi8, 1099511633776 : i64>) -> tensor<1x64x1x9xi8> loc(#loc576)
        %279 = "tpu.Conv2D"(%274, %271, %275) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 18432, out_size = 640, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 13, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x7x10x!quant.uniform<i8:f32, 0.056820584251968502>>, tensor<1x64x9x64xsi8>, tensor<1x64x1x9xi8>) -> tensor<1x64x7x10x!quant.uniform<i8:f32, 0.099249006299212599>> loc(#loc577)
        %280 = "tpu.Load"(%238) {do_bcast = true, ginfo = #tpu.lg<out_addr = 19712, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 14, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099511633520 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc578)
        %281 = "tpu.Conv2D"(%276, %277, %278) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 20480, out_size = 640, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 15, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x7x10x!quant.uniform<i8:f32, 0.031334092125984252>>, tensor<1x64x9x64xsi8>, tensor<1x64x1x9xi8>) -> tensor<1x64x7x10x!quant.uniform<i8:f32, 0.04219518503937008>> loc(#loc579)
        %282 = "tpu.Load"(%239) {do_bcast = true, ginfo = #tpu.lg<out_addr = 28672, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 16, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099511633264 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc580)
        %283 = "tpu.Lut"(%279, %280) {ginfo = #tpu.lg<out_addr = 24576, out_size = 640, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 17, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x64x7x10x!quant.uniform<i8:f32, 0.099249006299212599>>, tensor<1x1x1x256xsi8>) -> tensor<1x64x7x10x!quant.uniform<i8:f32, 0.075080522834645683>> loc(#loc581)
        %284 = "tpu.Load"(%241) {do_bcast = false, ginfo = #tpu.lg<out_addr = 19072, out_size = 512, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [64], id = 18, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x64xsi8, 1099511628592 : i64>) -> tensor<1x64x1x64xsi8> loc(#loc582)
        %285 = "tpu.Load"(%240) {do_bcast = false, ginfo = #tpu.lg<out_addr = 19664, out_size = 72, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 19, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x9xi8, 1099511632688 : i64>) -> tensor<1x64x1x9xi8> loc(#loc583)
        %286 = "tpu.Lut"(%281, %282) {ginfo = #tpu.lg<out_addr = 18432, out_size = 640, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 20, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x64x7x10x!quant.uniform<i8:f32, 0.04219518503937008>>, tensor<1x1x1x256xsi8>) -> tensor<1x64x7x10x!quant.uniform<i8:f32, 0.032116844881889768>> loc(#loc584)
        %287 = "tpu.Load"(%243) {do_bcast = false, ginfo = #tpu.lg<out_addr = 31232, out_size = 64, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [4], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [64], id = 21, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x4x1x64xsi8, 1099511628288 : i64>) -> tensor<1x4x1x64xsi8> loc(#loc585)
        %288 = "tpu.Load"(%242) {do_bcast = false, ginfo = #tpu.lg<out_addr = 19744, out_size = 9, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [4], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 22, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x4x1x9xi8, 1099511628544 : i64>) -> tensor<1x4x1x9xi8> loc(#loc586)
        %289 = "tpu.Conv2D"(%283, %284, %285) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 20480, out_size = 640, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 23, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x7x10x!quant.uniform<i8:f32, 0.075080522834645683>>, tensor<1x64x1x64xsi8>, tensor<1x64x1x9xi8>) -> tensor<1x64x7x10x!quant.uniform<i8:f32, 0.14529467007874017>> loc(#loc562)
        %290 = "tpu.Store"(%289, %0) {ginfo = #tpu.lg<out_addr = 20480, out_size = 640, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 24, stage = 1, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x64x7x10x!quant.uniform<i8:f32, 0.14529467007874017>>, none) -> tensor<1x64x7x10x!quant.uniform<i8:f32, 0.14529467007874017>, 195168 : i64> loc(#loc562)
        %291 = "tpu.Conv2D"(%286, %287, %288) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 25728, out_size = 80, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [4], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 25, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x7x10x!quant.uniform<i8:f32, 0.032116844881889768>>, tensor<1x4x1x64xsi8>, tensor<1x4x1x9xi8>) -> tensor<1x4x7x10x!quant.uniform<i8:f32, 0.14529467007874017>> loc(#loc563)
        %292 = "tpu.Store"(%291, %0) {ginfo = #tpu.lg<out_addr = 25728, out_size = 80, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [4], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [7], w_idx = [0], w_slice = [10], id = 26, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x4x7x10x!quant.uniform<i8:f32, 0.14529467007874017>>, none) -> tensor<1x4x7x10x!quant.uniform<i8:f32, 0.14529467007874017>, 199648 : i64> loc(#loc563)
        "tpu.Yield"(%290, %292) : (tensor<1x64x7x10x!quant.uniform<i8:f32, 0.14529467007874017>, 195168 : i64>, tensor<1x4x7x10x!quant.uniform<i8:f32, 0.14529467007874017>, 199648 : i64>) -> () loc(#loc620)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 6, 3, 4, 26, 5, -2, 8, 7, -3, 10, 9, -4, 13, 11, 12, -5, 15, 14, 0, -6, 17, 16, -7, 20, 18, 19, -8, 23, 21, 22, 1, 2, -9, 25, 24], group_type = 0 : i64, hsecs = 1 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x256x7x10x!quant.uniform<i8:f32, 0.045780149606299218>, 0 : i64>, tensor<1x64x7x10x!quant.uniform<i8:f32, 0.098445216535433075>, 35840 : i64>) -> (tensor<1x64x7x10x!quant.uniform<i8:f32, 0.14529467007874017>, 195168 : i64>, tensor<1x4x7x10x!quant.uniform<i8:f32, 0.14529467007874017>, 199648 : i64>) loc(#loc620)
      %245 = "tpu.Concat"(%244#0, %244#1) {axis = 1 : si32, do_relu = false, multipliers = [1, 1], only_merge = true, relu_limit = -1.000000e+00 : f64, rshifts = [0, 0]} : (tensor<1x64x7x10x!quant.uniform<i8:f32, 0.14529467007874017>, 195168 : i64>, tensor<1x4x7x10x!quant.uniform<i8:f32, 0.14529467007874017>, 199648 : i64>) -> tensor<1x68x7x10x!quant.uniform<i8:f32, 0.14529467007874017>, 195168 : i64> loc(#loc587)
      %246 = "tpu.Reshape"(%245) {flatten_start_dim = -1 : i64, shape = [1, 68, -1]} : (tensor<1x68x7x10x!quant.uniform<i8:f32, 0.14529467007874017>, 195168 : i64>) -> tensor<1x68x70x!quant.uniform<i8:f32, 0.14529467007874017>, 195168 : i64> loc(#loc588)
      %247 = "tpu.Concat"(%177, %217, %246) {axis = 2 : si32, do_relu = false, multipliers = [1, 1, 1], only_merge = false, relu_limit = -1.000000e+00 : f64, rshifts = [0, 0, 0]} : (tensor<1x68x1120x!quant.uniform<i8:f32, 0.14529467007874017>, 99968 : i64>, tensor<1x68x280x!quant.uniform<i8:f32, 0.14529467007874017>, 176128 : i64>, tensor<1x68x70x!quant.uniform<i8:f32, 0.14529467007874017>, 195168 : i64>) -> tensor<1x68x1470x!quant.uniform<i8:f32, 0.14529467007874017>, 0 : i64> loc(#loc589)
      %248 = "tpu.Slice"(%247, %0, %0, %0, %0) {axes = [], ends = [9223372036854775807, 64, 9223372036854775807], hasparamConvert_axes = [1], offset = [0, 0, 0], steps = [1, 1, 1]} : (tensor<1x68x1470x!quant.uniform<i8:f32, 0.14529467007874017>, 0 : i64>, none, none, none, none) -> tensor<1x64x1470x!quant.uniform<i8:f32, 0.14529467007874017>, 0 : i64> loc(#loc590)
      %249 = "tpu.Slice"(%247, %0, %0, %0, %0) {axes = [], ends = [9223372036854775807, 68, 9223372036854775807], hasparamConvert_axes = [1], offset = [0, 64, 0], steps = [1, 1, 1]} : (tensor<1x68x1470x!quant.uniform<i8:f32, 0.14529467007874017>, 0 : i64>, none, none, none, none) -> tensor<1x4x1470x!quant.uniform<i8:f32, 0.14529467007874017>, 94080 : i64> loc(#loc591)
      %250 = "tpu.Reshape"(%248) {flatten_start_dim = -1 : i64, shape = [1, 4, 16, 1470]} : (tensor<1x64x1470x!quant.uniform<i8:f32, 0.14529467007874017>, 0 : i64>) -> tensor<1x4x16x1470x!quant.uniform<i8:f32, 0.14529467007874017>, 0 : i64> loc(#loc592)
      %251 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099512214192 : i64> loc(#loc593)
      %252 = "tpu.Lut"(%249, %251) : (tensor<1x4x1470x!quant.uniform<i8:f32, 0.14529467007874017>, 94080 : i64>, tensor<1x1x1x256xsi8, 1099512214192 : i64>) -> tensor<1x4x1470x!quant.uniform<i8:f32, 0.0078740157480314959>, 376320 : i64> loc(#loc594)
      %253 = "tpu.Cast"(%250) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x4x16x1470x!quant.uniform<i8:f32, 0.14529467007874017>, 0 : i64>) -> tensor<1x4x16x1470xbf16, 188160 : i64> loc(#loc595)
      %254 = "top.Weight"() : () -> tensor<1x1x32x8xbf16, 1099511627776 : i64> loc(#loc596)
      %255 = "top.Weight"() : () -> tensor<1x1x32x8xbf16, 1099512707952 : i64> loc(#loc597)
      %256 = "top.Weight"() : () -> tensor<1x1x32x8xbf16, 1099512436016 : i64> loc(#loc598)
      %257 = "top.Weight"() : () -> tensor<1x1x32x8xbf16, 1099513068192 : i64> loc(#loc599)
      %258 = "tpu.Softmax"(%253, %254, %255, %256, %257, %0) {axis = 2 : si32, beta = 1.000000e+00 : f64, log = false, round_mode = #tpu<round_mode HalfAwayFromZero>} : (tensor<1x4x16x1470xbf16, 188160 : i64>, tensor<1x1x32x8xbf16, 1099511627776 : i64>, tensor<1x1x32x8xbf16, 1099512707952 : i64>, tensor<1x1x32x8xbf16, 1099512436016 : i64>, tensor<1x1x32x8xbf16, 1099513068192 : i64>, none) -> tensor<1x4x16x1470x!quant.calibrated<bf16<-1.000000e+00:1.000000e+00>>, 0 : i64> loc(#loc600)
      %259 = "tpu.Cast"(%258) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x4x16x1470x!quant.calibrated<bf16<-1.000000e+00:1.000000e+00>>, 0 : i64>) -> tensor<1x4x16x1470x!quant.uniform<i8:bf16, 0.0078740157480314959>, 188160 : i64> loc(#loc601)
      %260 = "tpu.Permute"(%259, %0) {order = [0, 2, 1, 3]} : (tensor<1x4x16x1470x!quant.uniform<i8:bf16, 0.0078740157480314959>, 188160 : i64>, none) -> tensor<1x16x4x1470x!quant.uniform<i8:bf16, 0.0078740157480314959>, 0 : i64> loc(#loc602)
      %261 = "top.Weight"() {do_compress = true} : () -> tensor<1x1x1x5xi8, 1099514023344 : i64> loc(#loc603)
      %262 = "top.Weight"() {do_compress = true} : () -> tensor<1x1x1x16xsi8, 1099512214752 : i64> loc(#loc604)
      %263 = "tpu.Conv2D"(%260, %262, %261) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = false} : (tensor<1x16x4x1470x!quant.uniform<i8:bf16, 0.0078740157480314959>, 0 : i64>, tensor<1x1x1x16xsi8, 1099512214752 : i64>, tensor<1x1x1x5xi8, 1099514023344 : i64>) -> tensor<1x1x4x1470x!quant.uniform<i8:f32, 0.081157151181102366>, 94080 : i64> loc(#loc605)
      %264 = "tpu.Cast"(%252) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x4x1470x!quant.uniform<i8:f32, 0.0078740157480314959>, 376320 : i64>) -> tensor<1x4x1470xf32, 4398046511104 : i64> loc(#loc606)
      %265 = "tpu.Cast"(%263) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x1x4x1470x!quant.uniform<i8:f32, 0.081157151181102366>, 94080 : i64>) -> tensor<1x1x4x1470xf32, 5497558138880 : i64> loc(#loc607)
      return %265, %264 : tensor<1x1x4x1470xf32, 5497558138880 : i64>, tensor<1x4x1470xf32, 4398046511104 : i64> loc(#loc)
    } loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc2 = loc("/model.0/conv/Conv_output_0_Conv_bias_packed")
#loc3 = loc("/model.0/conv/Conv_output_0_Conv_filter_reordered")
#loc4 = loc("/model.0/act/Mul_output_0_Mul_table")
#loc5 = loc("/model.1/conv/Conv_output_0_Conv_bias_packed")
#loc6 = loc("/model.1/conv/Conv_output_0_Conv_filter_reordered")
#loc7 = loc("/model.1/conv/Conv_output_0_Conv")
#loc8 = loc("load_0")
#loc9 = loc("load_/model.0/conv/Conv_output_0_Conv_filter_reordered")
#loc10 = loc("load_/model.0/conv/Conv_output_0_Conv_bias_packed")
#loc11 = loc("load_/model.0/act/Mul_output_0_Mul_table")
#loc12 = loc("/model.0/conv/Conv_output_0_Conv")
#loc13 = loc("load_/model.1/conv/Conv_output_0_Conv_filter_reordered")
#loc14 = loc("load_/model.1/conv/Conv_output_0_Conv_bias_packed")
#loc15 = loc("/model.0/act/Mul_output_0_Mul")
#loc16 = loc("/model.1/act/Mul_output_0_Mul_table")
#loc17 = loc("/model.2/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc18 = loc("/model.2/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc19 = loc("/model.2/cv1/act/Mul_output_0_Mul_table")
#loc20 = loc("/model.2/cv1/act/Mul_output_0_Mul")
#loc21 = loc("load_/model.1/conv/Conv_output_0_Conv")
#loc22 = loc("load_/model.1/act/Mul_output_0_Mul_table")
#loc23 = loc("load_/model.2/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc24 = loc("load_/model.2/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc25 = loc("/model.1/act/Mul_output_0_Mul")
#loc26 = loc("load_/model.2/cv1/act/Mul_output_0_Mul_table")
#loc27 = loc("/model.2/cv1/conv/Conv_output_0_Conv")
#loc28 = loc("/model.2/Split_output_0_Split")
#loc29 = loc("/model.2/Split_output_1_Split")
#loc30 = loc("/model.2/m.0/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc31 = loc("/model.2/m.0/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc32 = loc("/model.2/m.0/cv1/act/Mul_output_0_Mul_table")
#loc33 = loc("/model.2/m.0/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc34 = loc("/model.2/m.0/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc35 = loc("/model.2/m.0/cv2/conv/Conv_output_0_Conv")
#loc36 = loc("load_/model.2/Split_output_1_Split")
#loc37 = loc("load_/model.2/m.0/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc38 = loc("load_/model.2/m.0/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc39 = loc("load_/model.2/m.0/cv1/act/Mul_output_0_Mul_table")
#loc40 = loc("/model.2/m.0/cv1/conv/Conv_output_0_Conv")
#loc41 = loc("load_/model.2/m.0/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc42 = loc("load_/model.2/m.0/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc43 = loc("/model.2/m.0/cv1/act/Mul_output_0_Mul")
#loc44 = loc("/model.2/m.0/cv2/act/Mul_output_0_Mul_table")
#loc45 = loc("/model.2/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc46 = loc("/model.2/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc47 = loc("/model.2/cv2/conv/Conv_output_0_Conv")
#loc48 = loc("load_/model.2/m.0/cv2/conv/Conv_output_0_Conv")
#loc49 = loc("load_/model.2/m.0/cv2/act/Mul_output_0_Mul_table")
#loc50 = loc("/model.2/m.0/cv2/act/Mul_output_0_Mul")
#loc51 = loc("load_/model.2/Split_output_0_Split")
#loc52 = loc("/model.2/m.0/Add_output_0_Add")
#loc53 = loc("load_/model.2/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc54 = loc("load_/model.2/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc55 = loc("/model.2/Concat_output_0_Concat")
#loc56 = loc("/model.2/cv2/act/Mul_output_0_Mul_table")
#loc57 = loc("/model.3/conv/Conv_output_0_Conv_bias_packed")
#loc58 = loc("/model.3/conv/Conv_output_0_Conv_filter_reordered")
#loc59 = loc("/model.3/conv/Conv_output_0_Conv")
#loc60 = loc("load_/model.2/cv2/conv/Conv_output_0_Conv")
#loc61 = loc("load_/model.2/cv2/act/Mul_output_0_Mul_table")
#loc62 = loc("load_/model.3/conv/Conv_output_0_Conv_filter_reordered")
#loc63 = loc("load_/model.3/conv/Conv_output_0_Conv_bias_packed")
#loc64 = loc("/model.2/cv2/act/Mul_output_0_Mul")
#loc65 = loc("/model.3/act/Mul_output_0_Mul_table")
#loc66 = loc("/model.4/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc67 = loc("/model.4/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc68 = loc("/model.4/cv1/act/Mul_output_0_Mul_table")
#loc69 = loc("/model.4/cv1/act/Mul_output_0_Mul")
#loc70 = loc("load_/model.3/conv/Conv_output_0_Conv")
#loc71 = loc("load_/model.3/act/Mul_output_0_Mul_table")
#loc72 = loc("load_/model.4/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc73 = loc("load_/model.4/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc74 = loc("/model.3/act/Mul_output_0_Mul")
#loc75 = loc("load_/model.4/cv1/act/Mul_output_0_Mul_table")
#loc76 = loc("/model.4/cv1/conv/Conv_output_0_Conv")
#loc77 = loc("/model.4/Split_output_0_Split")
#loc78 = loc("/model.4/Split_output_1_Split")
#loc79 = loc("/model.4/m.0/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc80 = loc("/model.4/m.0/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc81 = loc("/model.4/m.0/cv1/act/Mul_output_0_Mul_table")
#loc82 = loc("/model.4/m.0/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc83 = loc("/model.4/m.0/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc84 = loc("/model.4/m.0/cv2/act/Mul_output_0_Mul_table")
#loc85 = loc("/model.4/m.1/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc86 = loc("/model.4/m.1/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc87 = loc("/model.4/m.1/cv1/act/Mul_output_0_Mul_table")
#loc88 = loc("/model.4/m.1/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc89 = loc("/model.4/m.1/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc90 = loc("/model.4/m.0/Add_output_0_Add")
#loc91 = loc("/model.4/m.1/cv2/conv/Conv_output_0_Conv")
#loc92 = loc("load_/model.4/Split_output_1_Split")
#loc93 = loc("load_/model.4/m.0/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc94 = loc("load_/model.4/m.0/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc95 = loc("load_/model.4/m.0/cv1/act/Mul_output_0_Mul_table")
#loc96 = loc("/model.4/m.0/cv1/conv/Conv_output_0_Conv")
#loc97 = loc("load_/model.4/m.0/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc98 = loc("load_/model.4/m.0/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc99 = loc("/model.4/m.0/cv1/act/Mul_output_0_Mul")
#loc100 = loc("load_/model.4/m.0/cv2/act/Mul_output_0_Mul_table")
#loc101 = loc("/model.4/m.0/cv2/conv/Conv_output_0_Conv")
#loc102 = loc("/model.4/m.0/cv2/act/Mul_output_0_Mul")
#loc103 = loc("load_/model.4/m.1/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc104 = loc("load_/model.4/m.1/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc105 = loc("load_/model.4/m.1/cv1/act/Mul_output_0_Mul_table")
#loc106 = loc("/model.4/m.1/cv1/conv/Conv_output_0_Conv")
#loc107 = loc("load_/model.4/m.1/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc108 = loc("load_/model.4/m.1/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc109 = loc("/model.4/m.1/cv1/act/Mul_output_0_Mul")
#loc110 = loc("/model.4/m.1/cv2/act/Mul_output_0_Mul_table")
#loc111 = loc("/model.4/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc112 = loc("/model.4/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc113 = loc("/model.4/cv2/act/Mul_output_0_Mul_table")
#loc114 = loc("/model.4/cv2/act/Mul_output_0_Mul")
#loc115 = loc("load_/model.4/m.1/cv2/conv/Conv_output_0_Conv")
#loc116 = loc("load_/model.4/m.1/cv2/act/Mul_output_0_Mul_table")
#loc117 = loc("load_/model.4/m.0/Add_output_0_Add")
#loc118 = loc("/model.4/m.1/cv2/act/Mul_output_0_Mul")
#loc119 = loc("load_/model.4/Split_output_0_Split")
#loc120 = loc("/model.4/m.1/Add_output_0_Add")
#loc121 = loc("load_/model.4/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc122 = loc("load_/model.4/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc123 = loc("/model.4/Concat_output_0_Concat")
#loc124 = loc("load_/model.4/cv2/act/Mul_output_0_Mul_table")
#loc125 = loc("/model.4/cv2/conv/Conv_output_0_Conv")
#loc126 = loc("/model.5/conv/Conv_output_0_Conv_bias_packed")
#loc127 = loc("/model.5/conv/Conv_output_0_Conv_filter_reordered")
#loc128 = loc("/model.5/act/Mul_output_0_Mul_table")
#loc129 = loc("/model.6/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc130 = loc("/model.6/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc131 = loc("/model.6/cv1/act/Mul_output_0_Mul_table")
#loc132 = loc("/model.6/cv1/act/Mul_output_0_Mul")
#loc133 = loc("load_/model.5/conv/Conv_output_0_Conv_filter_reordered")
#loc134 = loc("load_/model.4/cv2/act/Mul_output_0_Mul")
#loc135 = loc("load_/model.5/conv/Conv_output_0_Conv_bias_packed")
#loc136 = loc("load_/model.5/act/Mul_output_0_Mul_table")
#loc137 = loc("/model.5/conv/Conv_output_0_Conv")
#loc138 = loc("load_/model.6/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc139 = loc("load_/model.6/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc140 = loc("/model.5/act/Mul_output_0_Mul")
#loc141 = loc("load_/model.6/cv1/act/Mul_output_0_Mul_table")
#loc142 = loc("/model.6/cv1/conv/Conv_output_0_Conv")
#loc143 = loc("/model.6/Split_output_0_Split")
#loc144 = loc("/model.6/Split_output_1_Split")
#loc145 = loc("/model.6/m.0/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc146 = loc("/model.6/m.0/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc147 = loc("/model.6/m.0/cv1/act/Mul_output_0_Mul_table")
#loc148 = loc("/model.6/m.0/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc149 = loc("/model.6/m.0/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc150 = loc("/model.6/m.0/cv2/act/Mul_output_0_Mul_table")
#loc151 = loc("/model.6/m.1/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc152 = loc("/model.6/m.1/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc153 = loc("/model.6/m.1/cv1/act/Mul_output_0_Mul_table")
#loc154 = loc("/model.6/m.1/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc155 = loc("/model.6/m.1/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc156 = loc("/model.6/m.1/cv2/act/Mul_output_0_Mul_table")
#loc157 = loc("/model.6/m.0/Add_output_0_Add")
#loc158 = loc("/model.6/m.1/Add_output_0_Add")
#loc159 = loc("load_/model.6/Split_output_1_Split")
#loc160 = loc("load_/model.6/m.0/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc161 = loc("load_/model.6/m.0/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc162 = loc("load_/model.6/m.0/cv1/act/Mul_output_0_Mul_table")
#loc163 = loc("/model.6/m.0/cv1/conv/Conv_output_0_Conv")
#loc164 = loc("load_/model.6/m.0/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc165 = loc("load_/model.6/m.0/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc166 = loc("/model.6/m.0/cv1/act/Mul_output_0_Mul")
#loc167 = loc("load_/model.6/m.0/cv2/act/Mul_output_0_Mul_table")
#loc168 = loc("/model.6/m.0/cv2/conv/Conv_output_0_Conv")
#loc169 = loc("/model.6/m.0/cv2/act/Mul_output_0_Mul")
#loc170 = loc("load_/model.6/m.1/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc171 = loc("load_/model.6/m.1/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc172 = loc("load_/model.6/m.1/cv1/act/Mul_output_0_Mul_table")
#loc173 = loc("/model.6/m.1/cv1/conv/Conv_output_0_Conv")
#loc174 = loc("load_/model.6/m.1/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc175 = loc("load_/model.6/m.1/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc176 = loc("/model.6/m.1/cv1/act/Mul_output_0_Mul")
#loc177 = loc("load_/model.6/m.1/cv2/act/Mul_output_0_Mul_table")
#loc178 = loc("/model.6/m.1/cv2/conv/Conv_output_0_Conv")
#loc179 = loc("/model.6/m.1/cv2/act/Mul_output_0_Mul")
#loc180 = loc("/model.6/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc181 = loc("/model.6/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc182 = loc("/model.6/cv2/act/Mul_output_0_Mul_table")
#loc183 = loc("/model.6/cv2/act/Mul_output_0_Mul")
#loc184 = loc("load_/model.6/Split_output_0_Split")
#loc185 = loc("load_/model.6/m.0/Add_output_0_Add")
#loc186 = loc("load_/model.6/m.1/Add_output_0_Add")
#loc187 = loc("load_/model.6/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc188 = loc("load_/model.6/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc189 = loc("/model.6/Concat_output_0_Concat")
#loc190 = loc("load_/model.6/cv2/act/Mul_output_0_Mul_table")
#loc191 = loc("/model.6/cv2/conv/Conv_output_0_Conv")
#loc192 = loc("/model.7/conv/Conv_output_0_Conv_bias_packed")
#loc193 = loc("/model.7/conv/Conv_output_0_Conv_filter_reordered")
#loc194 = loc("/model.7/conv/Conv_output_0_Conv")
#loc195 = loc("/model.7/act/Mul_output_0_Mul_table")
#loc196 = loc("/model.8/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc197 = loc("/model.8/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc198 = loc("/model.8/cv1/act/Mul_output_0_Mul_table")
#loc199 = loc("/model.8/cv1/act/Mul_output_0_Mul")
#loc200 = loc("load_/model.7/conv/Conv_output_0_Conv")
#loc201 = loc("load_/model.7/act/Mul_output_0_Mul_table")
#loc202 = loc("load_/model.8/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc203 = loc("load_/model.8/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc204 = loc("/model.7/act/Mul_output_0_Mul")
#loc205 = loc("load_/model.8/cv1/act/Mul_output_0_Mul_table")
#loc206 = loc("/model.8/cv1/conv/Conv_output_0_Conv")
#loc207 = loc("/model.8/Split_output_0_Split")
#loc208 = loc("/model.8/Split_output_1_Split")
#loc209 = loc("/model.8/m.0/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc210 = loc("/model.8/m.0/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc211 = loc("/model.8/m.0/cv1/conv/Conv_output_0_Conv")
#loc212 = loc("/model.8/m.0/cv1/act/Mul_output_0_Mul_table")
#loc213 = loc("/model.8/m.0/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc214 = loc("/model.8/m.0/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc215 = loc("/model.8/m.0/cv2/conv/Conv_output_0_Conv")
#loc216 = loc("load_/model.8/m.0/cv1/conv/Conv_output_0_Conv")
#loc217 = loc("load_/model.8/m.0/cv1/act/Mul_output_0_Mul_table")
#loc218 = loc("load_/model.8/m.0/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc219 = loc("load_/model.8/m.0/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc220 = loc("/model.8/m.0/cv1/act/Mul_output_0_Mul")
#loc221 = loc("/model.8/m.0/cv2/act/Mul_output_0_Mul_table")
#loc222 = loc("/model.8/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc223 = loc("/model.8/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc224 = loc("/model.8/cv2/act/Mul_output_0_Mul_table")
#loc225 = loc("/model.9/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc226 = loc("/model.9/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc227 = loc("/model.9/cv1/act/Mul_output_0_Mul_table")
#loc228 = loc("/model.9/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc229 = loc("/model.9/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc230 = loc("/model.9/cv2/act/Mul_output_0_Mul_table")
#loc231 = loc("/model.9/cv2/act/Mul_output_0_Mul")
#loc232 = loc("load_/model.8/m.0/cv2/conv/Conv_output_0_Conv")
#loc233 = loc("load_/model.8/m.0/cv2/act/Mul_output_0_Mul_table")
#loc234 = loc("load_/model.8/Split_output_1_Split")
#loc235 = loc("load_/model.8/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc236 = loc("/model.8/m.0/cv2/act/Mul_output_0_Mul")
#loc237 = loc("load_/model.8/Split_output_0_Split")
#loc238 = loc("/model.8/m.0/Add_output_0_Add")
#loc239 = loc("load_/model.8/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc240 = loc("/model.8/Concat_output_0_Concat")
#loc241 = loc("load_/model.8/cv2/act/Mul_output_0_Mul_table")
#loc242 = loc("/model.8/cv2/conv/Conv_output_0_Conv")
#loc243 = loc("load_/model.9/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc244 = loc("load_/model.9/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc245 = loc("/model.8/cv2/act/Mul_output_0_Mul")
#loc246 = loc("load_/model.9/cv1/act/Mul_output_0_Mul_table")
#loc247 = loc("load_/model.9/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc248 = loc("/model.9/cv1/conv/Conv_output_0_Conv")
#loc249 = loc("/model.9/cv1/act/Mul_output_0_Mul")
#loc250 = loc("/model.9/m/MaxPool_output_0_MaxPool")
#loc251 = loc("/model.9/m_1/MaxPool_output_0_MaxPool")
#loc252 = loc("/model.9/m_2/MaxPool_output_0_MaxPool")
#loc253 = loc("load_/model.9/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc254 = loc("/model.9/Concat_output_0_Concat")
#loc255 = loc("load_/model.9/cv2/act/Mul_output_0_Mul_table")
#loc256 = loc("/model.9/cv2/conv/Conv_output_0_Conv")
#loc257 = loc("/model.10/Resize_output_0_Resize_filter_reordered")
#loc258 = loc("/model.10/Resize_output_0_Resize_bias_packed")
#loc259 = loc("/model.12/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc260 = loc("/model.12/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc261 = loc("/model.12/cv1/act/Mul_output_0_Mul_table")
#loc262 = loc("/model.12/cv1/act/Mul_output_0_Mul")
#loc263 = loc("load_/model.9/cv2/act/Mul_output_0_Mul")
#loc264 = loc("load_/model.10/Resize_output_0_Resize_filter_reordered")
#loc265 = loc("load_/model.10/Resize_output_0_Resize_bias_packed")
#loc266 = loc("load_/model.6/cv2/act/Mul_output_0_Mul")
#loc267 = loc("/model.10/Resize_output_0_Resize")
#loc268 = loc("load_/model.12/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc269 = loc("load_/model.12/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc270 = loc("/model.11/Concat_output_0_Concat")
#loc271 = loc("load_/model.12/cv1/act/Mul_output_0_Mul_table")
#loc272 = loc("/model.12/cv1/conv/Conv_output_0_Conv")
#loc273 = loc("/model.12/Split_output_0_Split")
#loc274 = loc("/model.12/Split_output_1_Split")
#loc275 = loc("/model.12/m.0/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc276 = loc("/model.12/m.0/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc277 = loc("/model.12/m.0/cv1/act/Mul_output_0_Mul_table")
#loc278 = loc("/model.12/m.0/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc279 = loc("/model.12/m.0/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc280 = loc("/model.12/m.0/cv2/act/Mul_output_0_Mul_table")
#loc281 = loc("/model.12/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc282 = loc("/model.12/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc283 = loc("/model.12/cv2/conv/Conv_output_0_Conv")
#loc284 = loc("load_/model.12/Split_output_1_Split")
#loc285 = loc("load_/model.12/m.0/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc286 = loc("load_/model.12/m.0/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc287 = loc("load_/model.12/m.0/cv1/act/Mul_output_0_Mul_table")
#loc288 = loc("/model.12/m.0/cv1/conv/Conv_output_0_Conv")
#loc289 = loc("load_/model.12/m.0/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc290 = loc("load_/model.12/m.0/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc291 = loc("/model.12/m.0/cv1/act/Mul_output_0_Mul")
#loc292 = loc("load_/model.12/m.0/cv2/act/Mul_output_0_Mul_table")
#loc293 = loc("/model.12/m.0/cv2/conv/Conv_output_0_Conv")
#loc294 = loc("load_/model.12/Split_output_0_Split")
#loc295 = loc("/model.12/m.0/cv2/act/Mul_output_0_Mul")
#loc296 = loc("load_/model.12/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc297 = loc("load_/model.12/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc298 = loc("/model.12/Concat_output_0_Concat")
#loc299 = loc("/model.12/cv2/act/Mul_output_0_Mul_table")
#loc300 = loc("/model.13/Resize_output_0_Resize_filter_reordered")
#loc301 = loc("/model.13/Resize_output_0_Resize_bias_packed")
#loc302 = loc("/model.15/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc303 = loc("/model.15/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc304 = loc("/model.15/cv1/act/Mul_output_0_Mul_table")
#loc305 = loc("/model.12/cv2/act/Mul_output_0_Mul")
#loc306 = loc("/model.15/cv1/act/Mul_output_0_Mul")
#loc307 = loc("load_/model.12/cv2/conv/Conv_output_0_Conv")
#loc308 = loc("load_/model.12/cv2/act/Mul_output_0_Mul_table")
#loc309 = loc("load_/model.13/Resize_output_0_Resize_filter_reordered")
#loc310 = loc("load_/model.13/Resize_output_0_Resize_bias_packed")
#loc311 = loc("/model.13/Resize_output_0_Resize")
#loc312 = loc("load_/model.15/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc313 = loc("load_/model.15/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc314 = loc("/model.14/Concat_output_0_Concat")
#loc315 = loc("load_/model.15/cv1/act/Mul_output_0_Mul_table")
#loc316 = loc("/model.15/cv1/conv/Conv_output_0_Conv")
#loc317 = loc("/model.15/Split_output_0_Split")
#loc318 = loc("/model.15/Split_output_1_Split")
#loc319 = loc("/model.15/m.0/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc320 = loc("/model.15/m.0/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc321 = loc("/model.15/m.0/cv1/act/Mul_output_0_Mul_table")
#loc322 = loc("/model.15/m.0/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc323 = loc("/model.15/m.0/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc324 = loc("/model.15/m.0/cv2/conv/Conv_output_0_Conv")
#loc325 = loc("load_/model.15/Split_output_1_Split")
#loc326 = loc("load_/model.15/m.0/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc327 = loc("load_/model.15/m.0/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc328 = loc("load_/model.15/m.0/cv1/act/Mul_output_0_Mul_table")
#loc329 = loc("/model.15/m.0/cv1/conv/Conv_output_0_Conv")
#loc330 = loc("load_/model.15/m.0/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc331 = loc("load_/model.15/m.0/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc332 = loc("/model.15/m.0/cv1/act/Mul_output_0_Mul")
#loc333 = loc("/model.15/m.0/cv2/act/Mul_output_0_Mul_table")
#loc334 = loc("/model.15/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc335 = loc("/model.15/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc336 = loc("/model.15/cv2/conv/Conv_output_0_Conv")
#loc337 = loc("load_/model.15/m.0/cv2/conv/Conv_output_0_Conv")
#loc338 = loc("load_/model.15/m.0/cv2/act/Mul_output_0_Mul_table")
#loc339 = loc("load_/model.15/Split_output_0_Split")
#loc340 = loc("/model.15/m.0/cv2/act/Mul_output_0_Mul")
#loc341 = loc("load_/model.15/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc342 = loc("load_/model.15/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc343 = loc("/model.15/Concat_output_0_Concat")
#loc344 = loc("/model.15/cv2/act/Mul_output_0_Mul_table")
#loc345 = loc("/model.16/conv/Conv_output_0_Conv_bias_packed")
#loc346 = loc("/model.16/conv/Conv_output_0_Conv_filter_reordered")
#loc347 = loc("/model.15/cv2/act/Mul_output_0_Mul")
#loc348 = loc("/model.16/conv/Conv_output_0_Conv")
#loc349 = loc("load_/model.15/cv2/conv/Conv_output_0_Conv")
#loc350 = loc("load_/model.15/cv2/act/Mul_output_0_Mul_table")
#loc351 = loc("load_/model.16/conv/Conv_output_0_Conv_filter_reordered")
#loc352 = loc("load_/model.16/conv/Conv_output_0_Conv_bias_packed")
#loc353 = loc("/model.22/cv2.0/cv2.0.0/conv/Conv_output_0_Conv_bias_packed")
#loc354 = loc("/model.22/cv2.0/cv2.0.0/conv/Conv_output_0_Conv_filter_reordered")
#loc355 = loc("/model.22/cv3.0/cv3.0.0/conv/Conv_output_0_Conv_bias_packed")
#loc356 = loc("/model.22/cv3.0/cv3.0.0/conv/Conv_output_0_Conv_filter_reordered")
#loc357 = loc("/model.16/act/Mul_output_0_Mul_table")
#loc358 = loc("/model.22/cv2.0/cv2.0.0/act/Mul_output_0_Mul_table")
#loc359 = loc("/model.22/cv3.0/cv3.0.0/conv/Conv_output_0_Conv")
#loc360 = loc("/model.16/act/Mul_output_0_Mul")
#loc361 = loc("/model.22/cv2.0/cv2.0.0/act/Mul_output_0_Mul")
#loc362 = loc("load_/model.15/cv2/act/Mul_output_0_Mul")
#loc363 = loc("load_/model.22/cv2.0/cv2.0.0/conv/Conv_output_0_Conv_filter_reordered")
#loc364 = loc("load_/model.22/cv2.0/cv2.0.0/conv/Conv_output_0_Conv_bias_packed")
#loc365 = loc("load_/model.22/cv3.0/cv3.0.0/conv/Conv_output_0_Conv_filter_reordered")
#loc366 = loc("load_/model.22/cv3.0/cv3.0.0/conv/Conv_output_0_Conv_bias_packed")
#loc367 = loc("/model.22/cv2.0/cv2.0.0/conv/Conv_output_0_Conv")
#loc368 = loc("load_/model.16/conv/Conv_output_0_Conv")
#loc369 = loc("load_/model.16/act/Mul_output_0_Mul_table")
#loc370 = loc("load_/model.22/cv2.0/cv2.0.0/act/Mul_output_0_Mul_table")
#loc371 = loc("/model.22/cv3.0/cv3.0.0/act/Mul_output_0_Mul_table")
#loc372 = loc("/model.22/cv3.0/cv3.0.0/act/Mul_output_0_Mul")
#loc373 = loc("/model.22/cv2.0/cv2.0.1/conv/Conv_output_0_Conv_bias_packed")
#loc374 = loc("/model.22/cv2.0/cv2.0.1/conv/Conv_output_0_Conv_filter_reordered")
#loc375 = loc("/model.17/Concat_output_0_Concat")
#loc376 = loc("/model.22/cv2.0/cv2.0.1/conv/Conv_output_0_Conv")
#loc377 = loc("load_/model.16/act/Mul_output_0_Mul")
#loc378 = loc("load_/model.12/cv2/act/Mul_output_0_Mul")
#loc379 = loc("load_/model.22/cv2.0/cv2.0.0/act/Mul_output_0_Mul")
#loc380 = loc("load_/model.22/cv2.0/cv2.0.1/conv/Conv_output_0_Conv_filter_reordered")
#loc381 = loc("load_/model.22/cv2.0/cv2.0.1/conv/Conv_output_0_Conv_bias_packed")
#loc382 = loc("/model.22/cv3.0/cv3.0.1/conv/Conv_output_0_Conv_bias_packed")
#loc383 = loc("/model.22/cv3.0/cv3.0.1/conv/Conv_output_0_Conv_filter_reordered")
#loc384 = loc("/model.18/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc385 = loc("/model.18/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc386 = loc("/model.22/cv3.0/cv3.0.1/conv/Conv_output_0_Conv")
#loc387 = loc("/model.18/cv1/conv/Conv_output_0_Conv")
#loc388 = loc("load_/model.22/cv3.0/cv3.0.0/act/Mul_output_0_Mul")
#loc389 = loc("load_/model.22/cv3.0/cv3.0.1/conv/Conv_output_0_Conv_filter_reordered")
#loc390 = loc("load_/model.22/cv3.0/cv3.0.1/conv/Conv_output_0_Conv_bias_packed")
#loc391 = loc("load_/model.17/Concat_output_0_Concat")
#loc392 = loc("load_/model.18/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc393 = loc("load_/model.18/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc394 = loc("/model.22/cv2.0/cv2.0.1/act/Mul_output_0_Mul_table")
#loc395 = loc("/model.22/cv3.0/cv3.0.1/act/Mul_output_0_Mul_table")
#loc396 = loc("/model.18/cv1/act/Mul_output_0_Mul_table")
#loc397 = loc("/model.22/cv2.0/cv2.0.2/Conv_output_0_Conv_bias_packed")
#loc398 = loc("/model.22/cv2.0/cv2.0.2/Conv_output_0_Conv_filter_reordered")
#loc399 = loc("/model.22/cv3.0/cv3.0.2/Conv_output_0_Conv_bias_packed")
#loc400 = loc("/model.22/cv3.0/cv3.0.2/Conv_output_0_Conv_filter_reordered")
#loc401 = loc("/model.18/cv1/act/Mul_output_0_Mul")
#loc402 = loc("/model.22/cv2.0/cv2.0.2/Conv_output_0_Conv")
#loc403 = loc("/model.22/cv3.0/cv3.0.2/Conv_output_0_Conv")
#loc404 = loc("load_/model.22/cv2.0/cv2.0.1/conv/Conv_output_0_Conv")
#loc405 = loc("load_/model.22/cv2.0/cv2.0.1/act/Mul_output_0_Mul_table")
#loc406 = loc("load_/model.22/cv3.0/cv3.0.1/conv/Conv_output_0_Conv")
#loc407 = loc("load_/model.22/cv3.0/cv3.0.1/act/Mul_output_0_Mul_table")
#loc408 = loc("/model.22/cv2.0/cv2.0.1/act/Mul_output_0_Mul")
#loc409 = loc("load_/model.18/cv1/conv/Conv_output_0_Conv")
#loc410 = loc("load_/model.18/cv1/act/Mul_output_0_Mul_table")
#loc411 = loc("/model.22/cv3.0/cv3.0.1/act/Mul_output_0_Mul")
#loc412 = loc("load_/model.22/cv2.0/cv2.0.2/Conv_output_0_Conv_filter_reordered")
#loc413 = loc("load_/model.22/cv2.0/cv2.0.2/Conv_output_0_Conv_bias_packed")
#loc414 = loc("load_/model.22/cv3.0/cv3.0.2/Conv_output_0_Conv_filter_reordered")
#loc415 = loc("load_/model.22/cv3.0/cv3.0.2/Conv_output_0_Conv_bias_packed")
#loc416 = loc("/model.18/Split_output_0_Split")
#loc417 = loc("/model.18/Split_output_1_Split")
#loc418 = loc("/model.22/Concat_output_0_Concat")
#loc419 = loc("/model.18/m.0/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc420 = loc("/model.18/m.0/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc421 = loc("/model.18/m.0/cv1/conv/Conv_output_0_Conv")
#loc422 = loc("/model.22/Reshape_output_0_Reshape")
#loc423 = loc("/model.18/m.0/cv1/act/Mul_output_0_Mul_table")
#loc424 = loc("/model.18/m.0/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc425 = loc("/model.18/m.0/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc426 = loc("/model.18/m.0/cv2/act/Mul_output_0_Mul_table")
#loc427 = loc("/model.18/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc428 = loc("/model.18/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc429 = loc("/model.18/cv2/conv/Conv_output_0_Conv")
#loc430 = loc("load_/model.18/m.0/cv1/conv/Conv_output_0_Conv")
#loc431 = loc("load_/model.18/m.0/cv1/act/Mul_output_0_Mul_table")
#loc432 = loc("load_/model.18/m.0/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc433 = loc("load_/model.18/m.0/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc434 = loc("/model.18/m.0/cv1/act/Mul_output_0_Mul")
#loc435 = loc("load_/model.18/m.0/cv2/act/Mul_output_0_Mul_table")
#loc436 = loc("/model.18/m.0/cv2/conv/Conv_output_0_Conv")
#loc437 = loc("load_/model.18/Split_output_0_Split")
#loc438 = loc("load_/model.18/Split_output_1_Split")
#loc439 = loc("/model.18/m.0/cv2/act/Mul_output_0_Mul")
#loc440 = loc("load_/model.18/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc441 = loc("load_/model.18/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc442 = loc("/model.18/Concat_output_0_Concat")
#loc443 = loc("/model.18/cv2/act/Mul_output_0_Mul_table")
#loc444 = loc("/model.19/conv/Conv_output_0_Conv_bias_packed")
#loc445 = loc("/model.19/conv/Conv_output_0_Conv_filter_reordered")
#loc446 = loc("/model.18/cv2/act/Mul_output_0_Mul")
#loc447 = loc("/model.19/conv/Conv_output_0_Conv")
#loc448 = loc("load_/model.18/cv2/conv/Conv_output_0_Conv")
#loc449 = loc("load_/model.18/cv2/act/Mul_output_0_Mul_table")
#loc450 = loc("load_/model.19/conv/Conv_output_0_Conv_filter_reordered")
#loc451 = loc("load_/model.19/conv/Conv_output_0_Conv_bias_packed")
#loc452 = loc("/model.22/cv2.1/cv2.1.0/conv/Conv_output_0_Conv_bias_packed")
#loc453 = loc("/model.22/cv2.1/cv2.1.0/conv/Conv_output_0_Conv_filter_reordered")
#loc454 = loc("/model.22/cv3.1/cv3.1.0/conv/Conv_output_0_Conv_bias_packed")
#loc455 = loc("/model.22/cv3.1/cv3.1.0/conv/Conv_output_0_Conv_filter_reordered")
#loc456 = loc("/model.19/act/Mul_output_0_Mul_table")
#loc457 = loc("/model.22/cv2.1/cv2.1.0/conv/Conv_output_0_Conv")
#loc458 = loc("/model.22/cv3.1/cv3.1.0/conv/Conv_output_0_Conv")
#loc459 = loc("/model.19/act/Mul_output_0_Mul")
#loc460 = loc("load_/model.22/cv2.1/cv2.1.0/conv/Conv_output_0_Conv_filter_reordered")
#loc461 = loc("load_/model.18/cv2/act/Mul_output_0_Mul")
#loc462 = loc("load_/model.22/cv2.1/cv2.1.0/conv/Conv_output_0_Conv_bias_packed")
#loc463 = loc("load_/model.22/cv3.1/cv3.1.0/conv/Conv_output_0_Conv_filter_reordered")
#loc464 = loc("load_/model.22/cv3.1/cv3.1.0/conv/Conv_output_0_Conv_bias_packed")
#loc465 = loc("load_/model.19/conv/Conv_output_0_Conv")
#loc466 = loc("load_/model.19/act/Mul_output_0_Mul_table")
#loc467 = loc("/model.22/cv2.1/cv2.1.0/act/Mul_output_0_Mul_table")
#loc468 = loc("/model.22/cv3.1/cv3.1.0/act/Mul_output_0_Mul_table")
#loc469 = loc("/model.22/cv2.1/cv2.1.1/conv/Conv_output_0_Conv_bias_packed")
#loc470 = loc("/model.22/cv2.1/cv2.1.1/conv/Conv_output_0_Conv_filter_reordered")
#loc471 = loc("/model.22/cv3.1/cv3.1.1/conv/Conv_output_0_Conv_bias_packed")
#loc472 = loc("/model.22/cv3.1/cv3.1.1/conv/Conv_output_0_Conv_filter_reordered")
#loc473 = loc("/model.21/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc474 = loc("/model.21/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc475 = loc("/model.22/cv2.1/cv2.1.1/act/Mul_output_0_Mul_table")
#loc476 = loc("/model.22/cv3.1/cv3.1.1/act/Mul_output_0_Mul_table")
#loc477 = loc("/model.21/cv1/act/Mul_output_0_Mul_table")
#loc478 = loc("/model.22/cv2.1/cv2.1.2/Conv_output_0_Conv_bias_packed")
#loc479 = loc("/model.22/cv2.1/cv2.1.2/Conv_output_0_Conv_filter_reordered")
#loc480 = loc("/model.22/cv3.1/cv3.1.2/Conv_output_0_Conv_bias_packed")
#loc481 = loc("/model.22/cv3.1/cv3.1.2/Conv_output_0_Conv_filter_reordered")
#loc482 = loc("/model.21/cv1/act/Mul_output_0_Mul")
#loc483 = loc("/model.22/cv2.1/cv2.1.2/Conv_output_0_Conv")
#loc484 = loc("/model.22/cv3.1/cv3.1.2/Conv_output_0_Conv")
#loc485 = loc("load_/model.22/cv2.1/cv2.1.0/conv/Conv_output_0_Conv")
#loc486 = loc("load_/model.22/cv2.1/cv2.1.0/act/Mul_output_0_Mul_table")
#loc487 = loc("load_/model.22/cv3.1/cv3.1.0/conv/Conv_output_0_Conv")
#loc488 = loc("load_/model.22/cv3.1/cv3.1.0/act/Mul_output_0_Mul_table")
#loc489 = loc("/model.22/cv2.1/cv2.1.0/act/Mul_output_0_Mul")
#loc490 = loc("load_/model.19/act/Mul_output_0_Mul")
#loc491 = loc("/model.22/cv3.1/cv3.1.0/act/Mul_output_0_Mul")
#loc492 = loc("load_/model.22/cv2.1/cv2.1.1/conv/Conv_output_0_Conv_filter_reordered")
#loc493 = loc("load_/model.22/cv2.1/cv2.1.1/conv/Conv_output_0_Conv_bias_packed")
#loc494 = loc("/model.20/Concat_output_0_Concat")
#loc495 = loc("load_/model.22/cv3.1/cv3.1.1/conv/Conv_output_0_Conv_filter_reordered")
#loc496 = loc("load_/model.22/cv3.1/cv3.1.1/conv/Conv_output_0_Conv_bias_packed")
#loc497 = loc("/model.22/cv2.1/cv2.1.1/conv/Conv_output_0_Conv")
#loc498 = loc("load_/model.21/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc499 = loc("load_/model.21/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc500 = loc("/model.22/cv3.1/cv3.1.1/conv/Conv_output_0_Conv")
#loc501 = loc("load_/model.22/cv2.1/cv2.1.1/act/Mul_output_0_Mul_table")
#loc502 = loc("/model.21/cv1/conv/Conv_output_0_Conv")
#loc503 = loc("load_/model.22/cv3.1/cv3.1.1/act/Mul_output_0_Mul_table")
#loc504 = loc("/model.22/cv2.1/cv2.1.1/act/Mul_output_0_Mul")
#loc505 = loc("load_/model.21/cv1/act/Mul_output_0_Mul_table")
#loc506 = loc("/model.22/cv3.1/cv3.1.1/act/Mul_output_0_Mul")
#loc507 = loc("load_/model.22/cv2.1/cv2.1.2/Conv_output_0_Conv_filter_reordered")
#loc508 = loc("load_/model.22/cv2.1/cv2.1.2/Conv_output_0_Conv_bias_packed")
#loc509 = loc("load_/model.22/cv3.1/cv3.1.2/Conv_output_0_Conv_filter_reordered")
#loc510 = loc("load_/model.22/cv3.1/cv3.1.2/Conv_output_0_Conv_bias_packed")
#loc511 = loc("/model.21/Split_output_0_Split")
#loc512 = loc("/model.21/Split_output_1_Split")
#loc513 = loc("/model.22/Concat_1_output_0_Concat")
#loc514 = loc("/model.21/m.0/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc515 = loc("/model.21/m.0/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc516 = loc("/model.21/m.0/cv1/conv/Conv_output_0_Conv")
#loc517 = loc("/model.22/Reshape_1_output_0_Reshape")
#loc518 = loc("/model.21/m.0/cv1/act/Mul_output_0_Mul_table")
#loc519 = loc("/model.21/m.0/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc520 = loc("/model.21/m.0/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc521 = loc("/model.21/m.0/cv2/conv/Conv_output_0_Conv")
#loc522 = loc("load_/model.21/m.0/cv1/conv/Conv_output_0_Conv")
#loc523 = loc("load_/model.21/m.0/cv1/act/Mul_output_0_Mul_table")
#loc524 = loc("load_/model.21/m.0/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc525 = loc("load_/model.21/m.0/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc526 = loc("/model.21/m.0/cv1/act/Mul_output_0_Mul")
#loc527 = loc("/model.21/m.0/cv2/act/Mul_output_0_Mul_table")
#loc528 = loc("/model.21/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc529 = loc("/model.21/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc530 = loc("/model.21/cv2/conv/Conv_output_0_Conv")
#loc531 = loc("load_/model.21/m.0/cv2/conv/Conv_output_0_Conv")
#loc532 = loc("load_/model.21/m.0/cv2/act/Mul_output_0_Mul_table")
#loc533 = loc("load_/model.21/Split_output_0_Split")
#loc534 = loc("load_/model.21/Split_output_1_Split")
#loc535 = loc("/model.21/m.0/cv2/act/Mul_output_0_Mul")
#loc536 = loc("load_/model.21/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc537 = loc("load_/model.21/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc538 = loc("/model.21/Concat_output_0_Concat")
#loc539 = loc("/model.21/cv2/act/Mul_output_0_Mul_table")
#loc540 = loc("/model.22/cv2.2/cv2.2.0/conv/Conv_output_0_Conv_bias_packed")
#loc541 = loc("/model.22/cv2.2/cv2.2.0/conv/Conv_output_0_Conv_filter_reordered")
#loc542 = loc("/model.21/cv2/act/Mul_output_0_Mul")
#loc543 = loc("/model.22/cv2.2/cv2.2.0/conv/Conv_output_0_Conv")
#loc544 = loc("load_/model.21/cv2/conv/Conv_output_0_Conv")
#loc545 = loc("load_/model.21/cv2/act/Mul_output_0_Mul_table")
#loc546 = loc("load_/model.22/cv2.2/cv2.2.0/conv/Conv_output_0_Conv_filter_reordered")
#loc547 = loc("load_/model.22/cv2.2/cv2.2.0/conv/Conv_output_0_Conv_bias_packed")
#loc548 = loc("/model.22/cv3.2/cv3.2.0/conv/Conv_output_0_Conv_bias_packed")
#loc549 = loc("/model.22/cv3.2/cv3.2.0/conv/Conv_output_0_Conv_filter_reordered")
#loc550 = loc("/model.22/cv2.2/cv2.2.0/act/Mul_output_0_Mul_table")
#loc551 = loc("/model.22/cv3.2/cv3.2.0/act/Mul_output_0_Mul_table")
#loc552 = loc("/model.22/cv2.2/cv2.2.1/conv/Conv_output_0_Conv_bias_packed")
#loc553 = loc("/model.22/cv2.2/cv2.2.1/conv/Conv_output_0_Conv_filter_reordered")
#loc554 = loc("/model.22/cv3.2/cv3.2.1/conv/Conv_output_0_Conv_bias_packed")
#loc555 = loc("/model.22/cv3.2/cv3.2.1/conv/Conv_output_0_Conv_filter_reordered")
#loc556 = loc("/model.22/cv2.2/cv2.2.1/act/Mul_output_0_Mul_table")
#loc557 = loc("/model.22/cv3.2/cv3.2.1/act/Mul_output_0_Mul_table")
#loc558 = loc("/model.22/cv2.2/cv2.2.2/Conv_output_0_Conv_bias_packed")
#loc559 = loc("/model.22/cv2.2/cv2.2.2/Conv_output_0_Conv_filter_reordered")
#loc560 = loc("/model.22/cv3.2/cv3.2.2/Conv_output_0_Conv_bias_packed")
#loc561 = loc("/model.22/cv3.2/cv3.2.2/Conv_output_0_Conv_filter_reordered")
#loc562 = loc("/model.22/cv2.2/cv2.2.2/Conv_output_0_Conv")
#loc563 = loc("/model.22/cv3.2/cv3.2.2/Conv_output_0_Conv")
#loc564 = loc("load_/model.22/cv3.2/cv3.2.0/conv/Conv_output_0_Conv_filter_reordered")
#loc565 = loc("load_/model.21/cv2/act/Mul_output_0_Mul")
#loc566 = loc("load_/model.22/cv3.2/cv3.2.0/conv/Conv_output_0_Conv_bias_packed")
#loc567 = loc("load_/model.22/cv2.2/cv2.2.0/conv/Conv_output_0_Conv")
#loc568 = loc("load_/model.22/cv2.2/cv2.2.0/act/Mul_output_0_Mul_table")
#loc569 = loc("load_/model.22/cv2.2/cv2.2.1/conv/Conv_output_0_Conv_filter_reordered")
#loc570 = loc("/model.22/cv3.2/cv3.2.0/conv/Conv_output_0_Conv")
#loc571 = loc("load_/model.22/cv3.2/cv3.2.0/act/Mul_output_0_Mul_table")
#loc572 = loc("/model.22/cv2.2/cv2.2.0/act/Mul_output_0_Mul")
#loc573 = loc("load_/model.22/cv2.2/cv2.2.1/conv/Conv_output_0_Conv_bias_packed")
#loc574 = loc("/model.22/cv3.2/cv3.2.0/act/Mul_output_0_Mul")
#loc575 = loc("load_/model.22/cv3.2/cv3.2.1/conv/Conv_output_0_Conv_filter_reordered")
#loc576 = loc("load_/model.22/cv3.2/cv3.2.1/conv/Conv_output_0_Conv_bias_packed")
#loc577 = loc("/model.22/cv2.2/cv2.2.1/conv/Conv_output_0_Conv")
#loc578 = loc("load_/model.22/cv2.2/cv2.2.1/act/Mul_output_0_Mul_table")
#loc579 = loc("/model.22/cv3.2/cv3.2.1/conv/Conv_output_0_Conv")
#loc580 = loc("load_/model.22/cv3.2/cv3.2.1/act/Mul_output_0_Mul_table")
#loc581 = loc("/model.22/cv2.2/cv2.2.1/act/Mul_output_0_Mul")
#loc582 = loc("load_/model.22/cv2.2/cv2.2.2/Conv_output_0_Conv_filter_reordered")
#loc583 = loc("load_/model.22/cv2.2/cv2.2.2/Conv_output_0_Conv_bias_packed")
#loc584 = loc("/model.22/cv3.2/cv3.2.1/act/Mul_output_0_Mul")
#loc585 = loc("load_/model.22/cv3.2/cv3.2.2/Conv_output_0_Conv_filter_reordered")
#loc586 = loc("load_/model.22/cv3.2/cv3.2.2/Conv_output_0_Conv_bias_packed")
#loc587 = loc("/model.22/Concat_2_output_0_Concat")
#loc588 = loc("/model.22/Reshape_2_output_0_Reshape")
#loc589 = loc("/model.22/Concat_3_output_0_Concat")
#loc590 = loc("/model.22/Split_output_0_Split")
#loc591 = loc("/model.22/Split_output_1_Split")
#loc592 = loc("/model.22/dfl/Reshape_output_0_Reshape")
#loc593 = loc("/model.22/Sigmoid_output_0_Sigmoid_table")
#loc594 = loc("/model.22/Sigmoid_output_0_Sigmoid")
#loc595 = loc("/model.22/dfl/Transpose_output_0_Transpose/model.22/dfl/Softmax_output_0_Softmax_bf16_/model.22/dfl/Transpose_output_0_Transpose")
#loc596 = loc("/model.22/dfl/Softmax_output_0_Softmax/model.22/dfl/Softmax_output_0_Softmax_slope_table_bf16")
#loc597 = loc("/model.22/dfl/Softmax_output_0_Softmax/model.22/dfl/Softmax_output_0_Softmax_slope_slope_table_bf16")
#loc598 = loc("/model.22/dfl/Softmax_output_0_Softmax/model.22/dfl/Softmax_output_0_Softmax_pow_table_bf16")
#loc599 = loc("/model.22/dfl/Softmax_output_0_Softmax/model.22/dfl/Softmax_output_0_Softmax_pow_mantissa_table_bf16")
#loc600 = loc("/model.22/dfl/Softmax_output_0_Softmax_/model.22/dfl/Transpose_output_0_Transpose_/model.22/dfl/Transpose_output_0_Transpose/model.22/dfl/Softmax_output_0_Softmax_bf16")
#loc601 = loc("/model.22/dfl/Softmax_output_0_Softmax/model.22/dfl/conv/Conv_output_0_Conv_si8_/model.22/dfl/Transpose_output_0_Transpose_/model.22/dfl/Transpose_output_0_Transpose/model.22/dfl/Softmax_output_0_Softmax_bf16_/model.22/dfl/Softmax_output_0_Softmax")
#loc602 = loc("/model.22/dfl/Transpose_output_0_Transpose_/model.22/dfl/Transpose_output_0_Transpose/model.22/dfl/Softmax_output_0_Softmax_bf16_/model.22/dfl/Softmax_output_0_Softmax_/model.22/dfl/Softmax_output_0_Softmax/model.22/dfl/conv/Conv_output_0_Conv_si8")
#loc603 = loc("/model.22/dfl/conv/Conv_output_0_Conv_bias_packed")
#loc604 = loc("/model.22/dfl/conv/Conv_output_0_Conv_filter_reordered")
#loc605 = loc("/model.22/dfl/conv/Conv_output_0_Conv")
#loc606 = loc("/model.22/Sigmoid_output_0_Sigmoid_f32")
#loc607 = loc("/model.22/dfl/conv/Conv_output_0_Conv_f32")
#loc608 = loc(fused[#loc90, #loc91])
#loc609 = loc(fused[#loc157, #loc158])
#loc610 = loc(fused[#loc305, #loc306])
#loc611 = loc(fused[#loc347, #loc348])
#loc612 = loc(fused[#loc359, #loc360, #loc361])
#loc613 = loc(fused[#loc375, #loc376])
#loc614 = loc(fused[#loc386, #loc387])
#loc615 = loc(fused[#loc401, #loc402, #loc403])
#loc616 = loc(fused[#loc446, #loc447])
#loc617 = loc(fused[#loc457, #loc458, #loc459])
#loc618 = loc(fused[#loc482, #loc483, #loc484])
#loc619 = loc(fused[#loc542, #loc543])
#loc620 = loc(fused[#loc562, #loc563])

